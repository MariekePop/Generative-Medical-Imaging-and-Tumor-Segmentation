CUDA_VISIBLE_DEVICES=0
2.5.1 True
âœ… MedPy is available
/scratch/rth/jdekok/nnunetv2/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None
/home/rth/jdekok/my-scratch/.conda/envs/nnunet_env2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/scratch/rth/jdekok/nnunetv2/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:1184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(filename_or_checkpoint, map_location=self.device)

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-05-23 23:23:50.893570: Using torch.compile...
2025-05-23 23:23:54.604795: do_dummy_2d_data_aug: True
2025-05-23 23:23:54.605926: Using splits from existing split file: /home/rth/jdekok/my-scratch/nnunetv2/nnUNet/nnUNet_preprocessed/Dataset520_NeckTumour/splits_final.json
2025-05-23 23:23:54.613208: The split file contains 5 splits.
2025-05-23 23:23:54.613356: Desired fold for training: 0
2025-05-23 23:23:54.613393: This split has 99 training and 25 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [20, 256, 320], 'median_image_size_in_voxels': [38.0, 526.5, 560.0], 'spacing': [3.3379289627075197, 0.4296875, 0.4296875], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 528, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1878.0, 'mean': 280.69818115234375, 'median': 109.0, 'min': 0.310302734375, 'percentile_00_5': 0.36376953125, 'percentile_99_5': 1273.0, 'std': 325.38812255859375}, '1': {'max': 1496.849365234375, 'mean': 202.74009704589844, 'median': 248.0, 'min': 0.0, 'percentile_00_5': 0.279052734375, 'percentile_99_5': 898.8617553710938, 'std': 157.97946166992188}, '2': {'max': 1918.0, 'mean': 126.82990264892578, 'median': 76.19837951660156, 'min': 0.05133056640625, 'percentile_00_5': 0.230224609375, 'percentile_99_5': 1041.0, 'std': 199.923095703125}, '3': {'max': 2416.0, 'mean': 387.05078125, 'median': 273.39599609375, 'min': 0.18024106323719025, 'percentile_00_5': 62.25990295410156, 'percentile_99_5': 1538.0, 'std': 297.7813720703125}, '4': {'max': 1877.02587890625, 'mean': 76.21454620361328, 'median': 21.38671875, 'min': 0.11090087890625, 'percentile_00_5': 0.1507568359375, 'percentile_99_5': 644.9055786132812, 'std': 130.4279327392578}, '5': {'max': 4095.0, 'mean': 1058.8817138671875, 'median': 1061.5997314453125, 'min': 0.0, 'percentile_00_5': 0.2359619140625, 'percentile_99_5': 3554.832275390625, 'std': 945.0421142578125}}} 

2025-05-23 23:24:04.445165: unpacking dataset...
2025-05-23 23:24:22.696341: unpacking done...
2025-05-23 23:24:22.713865: Unable to plot network architecture: nnUNet_compile is enabled!
2025-05-23 23:24:22.776077: 
2025-05-23 23:24:22.776300: Epoch 950
2025-05-23 23:24:22.776515: Current learning rate: 0.00067
2025-05-23 23:26:48.310568: train_loss -0.8019
2025-05-23 23:26:48.310849: val_loss -0.5876
2025-05-23 23:26:48.310926: Pseudo dice [np.float32(0.7639)]
2025-05-23 23:26:48.311019: Epoch time: 145.54 s
2025-05-23 23:26:49.882230: 
2025-05-23 23:26:49.882466: Epoch 951
2025-05-23 23:26:49.882583: Current learning rate: 0.00066
2025-05-23 23:28:16.628985: train_loss -0.8026
2025-05-23 23:28:16.629484: val_loss -0.5481
2025-05-23 23:28:16.629584: Pseudo dice [np.float32(0.771)]
2025-05-23 23:28:16.629828: Epoch time: 86.75 s
2025-05-23 23:28:18.066759: 
2025-05-23 23:28:18.066994: Epoch 952
2025-05-23 23:28:18.067123: Current learning rate: 0.00065
2025-05-23 23:29:52.402698: train_loss -0.8198
2025-05-23 23:29:52.403489: val_loss -0.5223
2025-05-23 23:29:52.403574: Pseudo dice [np.float32(0.7324)]
2025-05-23 23:29:52.403676: Epoch time: 94.34 s
2025-05-23 23:29:53.927333: 
2025-05-23 23:29:53.927560: Epoch 953
2025-05-23 23:29:53.927671: Current learning rate: 0.00064
2025-05-23 23:31:28.471116: train_loss -0.7908
2025-05-23 23:31:28.471485: val_loss -0.6202
2025-05-23 23:31:28.471571: Pseudo dice [np.float32(0.7752)]
2025-05-23 23:31:28.471681: Epoch time: 94.55 s
2025-05-23 23:31:30.633455: 
2025-05-23 23:31:30.633724: Epoch 954
2025-05-23 23:31:30.633843: Current learning rate: 0.00063
2025-05-23 23:32:56.882020: train_loss -0.8179
2025-05-23 23:32:56.882438: val_loss -0.5872
2025-05-23 23:32:56.882518: Pseudo dice [np.float32(0.7676)]
2025-05-23 23:32:56.882625: Epoch time: 86.25 s
2025-05-23 23:32:58.357451: 
2025-05-23 23:32:58.357677: Epoch 955
2025-05-23 23:32:58.357818: Current learning rate: 0.00061
2025-05-23 23:34:23.223187: train_loss -0.8128
2025-05-23 23:34:23.223815: val_loss -0.554
2025-05-23 23:34:23.223918: Pseudo dice [np.float32(0.7341)]
2025-05-23 23:34:23.224089: Epoch time: 84.87 s
2025-05-23 23:34:24.664791: 
2025-05-23 23:34:24.665216: Epoch 956
2025-05-23 23:34:24.665359: Current learning rate: 0.0006
2025-05-23 23:35:55.890611: train_loss -0.8238
2025-05-23 23:35:55.891041: val_loss -0.5575
2025-05-23 23:35:55.891179: Pseudo dice [np.float32(0.7575)]
2025-05-23 23:35:55.891294: Epoch time: 91.23 s
2025-05-23 23:35:57.386025: 
2025-05-23 23:35:57.386292: Epoch 957
2025-05-23 23:35:57.386411: Current learning rate: 0.00059
2025-05-23 23:37:21.681736: train_loss -0.8079
2025-05-23 23:37:21.682108: val_loss -0.6301
2025-05-23 23:37:21.682193: Pseudo dice [np.float32(0.7771)]
2025-05-23 23:37:21.682292: Epoch time: 84.3 s
2025-05-23 23:37:23.125143: 
2025-05-23 23:37:23.125486: Epoch 958
2025-05-23 23:37:23.125633: Current learning rate: 0.00058
2025-05-23 23:38:49.781579: train_loss -0.8109
2025-05-23 23:38:49.782352: val_loss -0.5943
2025-05-23 23:38:49.782476: Pseudo dice [np.float32(0.763)]
2025-05-23 23:38:49.782713: Epoch time: 86.66 s
2025-05-23 23:38:51.269553: 
2025-05-23 23:38:51.269757: Epoch 959
2025-05-23 23:38:51.269873: Current learning rate: 0.00056
2025-05-23 23:40:28.557851: train_loss -0.8223
2025-05-23 23:40:28.581526: val_loss -0.5557
2025-05-23 23:40:28.581811: Pseudo dice [np.float32(0.7579)]
2025-05-23 23:40:28.581952: Epoch time: 97.29 s
2025-05-23 23:40:30.145289: 
2025-05-23 23:40:30.145622: Epoch 960
2025-05-23 23:40:30.145752: Current learning rate: 0.00055
2025-05-23 23:42:22.395245: train_loss -0.8184
2025-05-23 23:42:22.395860: val_loss -0.5192
2025-05-23 23:42:22.395941: Pseudo dice [np.float32(0.755)]
2025-05-23 23:42:22.396106: Epoch time: 112.25 s
2025-05-23 23:42:24.015866: 
2025-05-23 23:42:24.016082: Epoch 961
2025-05-23 23:42:24.016212: Current learning rate: 0.00054
2025-05-23 23:43:54.435734: train_loss -0.8254
2025-05-23 23:43:54.436144: val_loss -0.5483
2025-05-23 23:43:54.436245: Pseudo dice [np.float32(0.7547)]
2025-05-23 23:43:54.436362: Epoch time: 90.42 s
2025-05-23 23:43:56.503842: 
2025-05-23 23:43:56.504046: Epoch 962
2025-05-23 23:43:56.504166: Current learning rate: 0.00053
2025-05-23 23:45:26.987268: train_loss -0.8185
2025-05-23 23:45:26.987844: val_loss -0.5849
2025-05-23 23:45:26.987931: Pseudo dice [np.float32(0.747)]
2025-05-23 23:45:26.988074: Epoch time: 90.49 s
2025-05-23 23:45:28.563999: 
2025-05-23 23:45:28.564267: Epoch 963
2025-05-23 23:45:28.564389: Current learning rate: 0.00051
2025-05-23 23:46:52.817398: train_loss -0.8066
2025-05-23 23:46:52.817948: val_loss -0.5693
2025-05-23 23:46:52.818033: Pseudo dice [np.float32(0.7757)]
2025-05-23 23:46:52.818187: Epoch time: 84.25 s
2025-05-23 23:46:54.630070: 
2025-05-23 23:46:54.630317: Epoch 964
2025-05-23 23:46:54.630437: Current learning rate: 0.0005
2025-05-23 23:48:28.071473: train_loss -0.8124
2025-05-23 23:48:28.071954: val_loss -0.4827
2025-05-23 23:48:28.072073: Pseudo dice [np.float32(0.7211)]
2025-05-23 23:48:28.072192: Epoch time: 93.44 s
2025-05-23 23:48:29.882100: 
2025-05-23 23:48:29.882336: Epoch 965
2025-05-23 23:48:29.882448: Current learning rate: 0.00049
2025-05-23 23:50:09.783878: train_loss -0.8098
2025-05-23 23:50:09.784330: val_loss -0.5603
2025-05-23 23:50:09.784421: Pseudo dice [np.float32(0.7679)]
2025-05-23 23:50:09.784533: Epoch time: 99.9 s
2025-05-23 23:50:11.353030: 
2025-05-23 23:50:11.353243: Epoch 966
2025-05-23 23:50:11.353363: Current learning rate: 0.00048
2025-05-23 23:51:53.187332: train_loss -0.8252
2025-05-23 23:51:53.199367: val_loss -0.5786
2025-05-23 23:51:53.199607: Pseudo dice [np.float32(0.7758)]
2025-05-23 23:51:53.199744: Epoch time: 101.84 s
2025-05-23 23:51:54.786156: 
2025-05-23 23:51:54.786413: Epoch 967
2025-05-23 23:51:54.786531: Current learning rate: 0.00046
2025-05-23 23:53:24.417926: train_loss -0.799
2025-05-23 23:53:24.418295: val_loss -0.5612
2025-05-23 23:53:24.418366: Pseudo dice [np.float32(0.7567)]
2025-05-23 23:53:24.418453: Epoch time: 89.63 s
2025-05-23 23:53:25.989603: 
2025-05-23 23:53:25.989813: Epoch 968
2025-05-23 23:53:25.989937: Current learning rate: 0.00045
2025-05-23 23:55:00.221892: train_loss -0.8198
2025-05-23 23:55:00.222391: val_loss -0.538
2025-05-23 23:55:00.222479: Pseudo dice [np.float32(0.7566)]
2025-05-23 23:55:00.222604: Epoch time: 94.23 s
2025-05-23 23:55:01.742208: 
2025-05-23 23:55:01.742414: Epoch 969
2025-05-23 23:55:01.742532: Current learning rate: 0.00044
2025-05-23 23:56:50.882971: train_loss -0.8272
2025-05-23 23:56:50.883389: val_loss -0.6304
2025-05-23 23:56:50.883474: Pseudo dice [np.float32(0.7781)]
2025-05-23 23:56:50.883578: Epoch time: 109.14 s
2025-05-23 23:56:53.259125: 
2025-05-23 23:56:53.259390: Epoch 970
2025-05-23 23:56:53.259499: Current learning rate: 0.00043
2025-05-23 23:58:32.758404: train_loss -0.8243
2025-05-23 23:58:32.759039: val_loss -0.5987
2025-05-23 23:58:32.759139: Pseudo dice [np.float32(0.7661)]
2025-05-23 23:58:32.759235: Epoch time: 99.5 s
2025-05-23 23:58:34.203832: 
2025-05-23 23:58:34.204044: Epoch 971
2025-05-23 23:58:34.204163: Current learning rate: 0.00041
2025-05-24 00:00:09.662556: train_loss -0.8138
2025-05-24 00:00:09.662939: val_loss -0.5781
2025-05-24 00:00:09.663024: Pseudo dice [np.float32(0.7599)]
2025-05-24 00:00:09.663154: Epoch time: 95.46 s
2025-05-24 00:00:11.195876: 
2025-05-24 00:00:11.196083: Epoch 972
2025-05-24 00:00:11.196193: Current learning rate: 0.0004
2025-05-24 00:01:44.024639: train_loss -0.8226
2025-05-24 00:01:44.025486: val_loss -0.5353
2025-05-24 00:01:44.025572: Pseudo dice [np.float32(0.7467)]
2025-05-24 00:01:44.025681: Epoch time: 92.83 s
2025-05-24 00:01:45.379565: 
2025-05-24 00:01:45.379974: Epoch 973
2025-05-24 00:01:45.380148: Current learning rate: 0.00039
2025-05-24 00:03:08.850374: train_loss -0.8222
2025-05-24 00:03:08.851155: val_loss -0.5622
2025-05-24 00:03:08.851249: Pseudo dice [np.float32(0.7707)]
2025-05-24 00:03:08.851383: Epoch time: 83.47 s
2025-05-24 00:03:11.004263: 
2025-05-24 00:03:11.004531: Epoch 974
2025-05-24 00:03:11.004645: Current learning rate: 0.00037
2025-05-24 00:04:43.086388: train_loss -0.8147
2025-05-24 00:04:43.086802: val_loss -0.5767
2025-05-24 00:04:43.094989: Pseudo dice [np.float32(0.7671)]
2025-05-24 00:04:43.095277: Epoch time: 92.08 s
2025-05-24 00:04:44.511866: 
2025-05-24 00:04:44.512218: Epoch 975
2025-05-24 00:04:44.512353: Current learning rate: 0.00036
2025-05-24 00:06:17.895040: train_loss -0.8394
2025-05-24 00:06:17.895441: val_loss -0.5527
2025-05-24 00:06:17.895522: Pseudo dice [np.float32(0.7597)]
2025-05-24 00:06:17.895627: Epoch time: 93.38 s
2025-05-24 00:06:19.435316: 
2025-05-24 00:06:19.435544: Epoch 976
2025-05-24 00:06:19.435662: Current learning rate: 0.00035
2025-05-24 00:07:49.863526: train_loss -0.8136
2025-05-24 00:07:49.864049: val_loss -0.6313
2025-05-24 00:07:49.864160: Pseudo dice [np.float32(0.7547)]
2025-05-24 00:07:49.864271: Epoch time: 90.43 s
2025-05-24 00:07:51.290758: 
2025-05-24 00:07:51.291012: Epoch 977
2025-05-24 00:07:51.291147: Current learning rate: 0.00034
2025-05-24 00:09:21.741487: train_loss -0.8157
2025-05-24 00:09:21.742216: val_loss -0.5803
2025-05-24 00:09:21.742317: Pseudo dice [np.float32(0.7503)]
2025-05-24 00:09:21.742435: Epoch time: 90.45 s
2025-05-24 00:09:23.356933: 
2025-05-24 00:09:23.357157: Epoch 978
2025-05-24 00:09:23.357264: Current learning rate: 0.00032
2025-05-24 00:10:57.658306: train_loss -0.8237
2025-05-24 00:10:57.658796: val_loss -0.5741
2025-05-24 00:10:57.658891: Pseudo dice [np.float32(0.7625)]
2025-05-24 00:10:57.659020: Epoch time: 94.3 s
2025-05-24 00:10:59.019857: 
2025-05-24 00:10:59.020134: Epoch 979
2025-05-24 00:10:59.020255: Current learning rate: 0.00031
2025-05-24 00:12:47.502689: train_loss -0.8031
2025-05-24 00:12:47.503262: val_loss -0.5585
2025-05-24 00:12:47.503350: Pseudo dice [np.float32(0.7434)]
2025-05-24 00:12:47.503485: Epoch time: 108.48 s
2025-05-24 00:12:49.546411: 
2025-05-24 00:12:49.546736: Epoch 980
2025-05-24 00:12:49.546901: Current learning rate: 0.0003
2025-05-24 00:14:30.275198: train_loss -0.8088
2025-05-24 00:14:30.300387: val_loss -0.5805
2025-05-24 00:14:30.300746: Pseudo dice [np.float32(0.7645)]
2025-05-24 00:14:30.300923: Epoch time: 100.73 s
2025-05-24 00:14:32.011533: 
2025-05-24 00:14:32.011811: Epoch 981
2025-05-24 00:14:32.011945: Current learning rate: 0.00028
2025-05-24 00:16:17.879843: train_loss -0.8309
2025-05-24 00:16:17.880602: val_loss -0.6203
2025-05-24 00:16:17.880743: Pseudo dice [np.float32(0.772)]
2025-05-24 00:16:17.880919: Epoch time: 105.87 s
2025-05-24 00:16:19.623384: 
2025-05-24 00:16:19.623661: Epoch 982
2025-05-24 00:16:19.623807: Current learning rate: 0.00027
2025-05-24 00:17:49.939745: train_loss -0.8368
2025-05-24 00:17:49.940583: val_loss -0.5692
2025-05-24 00:17:49.940682: Pseudo dice [np.float32(0.7635)]
2025-05-24 00:17:49.940791: Epoch time: 90.32 s
2025-05-24 00:17:51.446759: 
2025-05-24 00:17:51.447039: Epoch 983
2025-05-24 00:17:51.447181: Current learning rate: 0.00026
2025-05-24 00:19:41.378919: train_loss -0.8153
2025-05-24 00:19:41.379599: val_loss -0.5446
2025-05-24 00:19:41.379738: Pseudo dice [np.float32(0.7564)]
2025-05-24 00:19:41.379912: Epoch time: 109.93 s
2025-05-24 00:19:42.912411: 
2025-05-24 00:19:42.912648: Epoch 984
2025-05-24 00:19:42.912756: Current learning rate: 0.00024
2025-05-24 00:21:24.464923: train_loss -0.8081
2025-05-24 00:21:24.465464: val_loss -0.5627
2025-05-24 00:21:24.465557: Pseudo dice [np.float32(0.744)]
2025-05-24 00:21:24.465684: Epoch time: 101.55 s
2025-05-24 00:21:25.915877: 
2025-05-24 00:21:25.916164: Epoch 985
2025-05-24 00:21:25.916303: Current learning rate: 0.00023
2025-05-24 00:23:01.856252: train_loss -0.8051
2025-05-24 00:23:01.856760: val_loss -0.575
2025-05-24 00:23:01.856921: Pseudo dice [np.float32(0.7423)]
2025-05-24 00:23:01.857020: Epoch time: 95.94 s
2025-05-24 00:23:04.037975: 
2025-05-24 00:23:04.038335: Epoch 986
2025-05-24 00:23:04.038482: Current learning rate: 0.00021
2025-05-24 00:24:46.457335: train_loss -0.8173
2025-05-24 00:24:46.457793: val_loss -0.5617
2025-05-24 00:24:46.457900: Pseudo dice [np.float32(0.7386)]
2025-05-24 00:24:46.458032: Epoch time: 102.42 s
2025-05-24 00:24:48.293753: 
2025-05-24 00:24:48.293983: Epoch 987
2025-05-24 00:24:48.294132: Current learning rate: 0.0002
2025-05-24 00:26:24.755338: train_loss -0.811
2025-05-24 00:26:24.767361: val_loss -0.6253
2025-05-24 00:26:24.767696: Pseudo dice [np.float32(0.7847)]
2025-05-24 00:26:24.767857: Epoch time: 96.46 s
2025-05-24 00:26:26.367607: 
2025-05-24 00:26:26.367871: Epoch 988
2025-05-24 00:26:26.367996: Current learning rate: 0.00019
2025-05-24 00:28:05.574983: train_loss -0.7994
2025-05-24 00:28:05.575373: val_loss -0.6127
2025-05-24 00:28:05.575466: Pseudo dice [np.float32(0.7749)]
2025-05-24 00:28:05.575584: Epoch time: 99.21 s
2025-05-24 00:28:07.114442: 
2025-05-24 00:28:07.114968: Epoch 989
2025-05-24 00:28:07.115145: Current learning rate: 0.00017
2025-05-24 00:29:51.168451: train_loss -0.8167
2025-05-24 00:29:51.169073: val_loss -0.5626
2025-05-24 00:29:51.169161: Pseudo dice [np.float32(0.7658)]
2025-05-24 00:29:51.169327: Epoch time: 104.06 s
2025-05-24 00:29:52.713692: 
2025-05-24 00:29:52.713970: Epoch 990
2025-05-24 00:29:52.714103: Current learning rate: 0.00016
2025-05-24 00:31:33.979886: train_loss -0.8288
2025-05-24 00:31:33.980625: val_loss -0.5777
2025-05-24 00:31:33.980732: Pseudo dice [np.float32(0.7738)]
2025-05-24 00:31:33.980861: Epoch time: 101.27 s
2025-05-24 00:31:35.425296: 
2025-05-24 00:31:35.425480: Epoch 991
2025-05-24 00:31:35.425585: Current learning rate: 0.00014
2025-05-24 00:33:33.508427: train_loss -0.8056
2025-05-24 00:33:33.509127: val_loss -0.5777
2025-05-24 00:33:33.509289: Pseudo dice [np.float32(0.7536)]
2025-05-24 00:33:33.509502: Epoch time: 118.08 s
2025-05-24 00:33:35.116968: 
2025-05-24 00:33:35.117419: Epoch 992
2025-05-24 00:33:35.117548: Current learning rate: 0.00013
2025-05-24 00:35:06.378671: train_loss -0.797
2025-05-24 00:35:06.379251: val_loss -0.5711
2025-05-24 00:35:06.379377: Pseudo dice [np.float32(0.7688)]
2025-05-24 00:35:06.379587: Epoch time: 91.26 s
2025-05-24 00:35:07.820769: 
2025-05-24 00:35:07.820962: Epoch 993
2025-05-24 00:35:07.821089: Current learning rate: 0.00011
2025-05-24 00:36:44.830425: train_loss -0.8233
2025-05-24 00:36:44.830862: val_loss -0.5783
2025-05-24 00:36:44.830941: Pseudo dice [np.float32(0.7659)]
2025-05-24 00:36:44.831047: Epoch time: 97.01 s
2025-05-24 00:36:46.270550: 
2025-05-24 00:36:46.270815: Epoch 994
2025-05-24 00:36:46.270924: Current learning rate: 0.0001
2025-05-24 00:38:24.162153: train_loss -0.8232
2025-05-24 00:38:24.163396: val_loss -0.565
2025-05-24 00:38:24.163568: Pseudo dice [np.float32(0.7623)]
2025-05-24 00:38:24.163791: Epoch time: 97.89 s
2025-05-24 00:38:25.871913: 
2025-05-24 00:38:25.872157: Epoch 995
2025-05-24 00:38:25.872302: Current learning rate: 8e-05
2025-05-24 00:40:04.078734: train_loss -0.8223
2025-05-24 00:40:04.079561: val_loss -0.5498
2025-05-24 00:40:04.079672: Pseudo dice [np.float32(0.7786)]
2025-05-24 00:40:04.079812: Epoch time: 98.21 s
2025-05-24 00:40:04.079905: Yayy! New best EMA pseudo Dice: 0.763700008392334
2025-05-24 00:40:06.391243: 
2025-05-24 00:40:06.391563: Epoch 996
2025-05-24 00:40:06.391728: Current learning rate: 7e-05
2025-05-24 00:41:39.243541: train_loss -0.8114
2025-05-24 00:41:39.292813: val_loss -0.6098
2025-05-24 00:41:39.293109: Pseudo dice [np.float32(0.7601)]
2025-05-24 00:41:39.293254: Epoch time: 92.85 s
2025-05-24 00:41:40.911046: 
2025-05-24 00:41:40.911306: Epoch 997
2025-05-24 00:41:40.911423: Current learning rate: 5e-05
2025-05-24 00:43:08.598166: train_loss -0.8163
2025-05-24 00:43:08.598696: val_loss -0.5327
2025-05-24 00:43:08.598902: Pseudo dice [np.float32(0.7462)]
2025-05-24 00:43:08.599162: Epoch time: 87.69 s
2025-05-24 00:43:09.966278: 
2025-05-24 00:43:09.966599: Epoch 998
2025-05-24 00:43:09.966727: Current learning rate: 4e-05
2025-05-24 00:44:49.077567: train_loss -0.8045
2025-05-24 00:44:49.077919: val_loss -0.5746
2025-05-24 00:44:49.078007: Pseudo dice [np.float32(0.7704)]
2025-05-24 00:44:49.078140: Epoch time: 99.11 s
2025-05-24 00:44:50.687953: 
2025-05-24 00:44:50.688358: Epoch 999
2025-05-24 00:44:50.688516: Current learning rate: 2e-05
2025-05-24 00:46:30.919573: train_loss -0.8131
2025-05-24 00:46:30.937210: val_loss -0.5438
2025-05-24 00:46:30.937500: Pseudo dice [np.float32(0.757)]
2025-05-24 00:46:30.937638: Epoch time: 100.23 s
2025-05-24 00:46:33.211758: Training done.
2025-05-24 00:46:33.356427: Using splits from existing split file: /home/rth/jdekok/my-scratch/nnunetv2/nnUNet/nnUNet_preprocessed/Dataset520_NeckTumour/splits_final.json
2025-05-24 00:46:33.379199: The split file contains 5 splits.
2025-05-24 00:46:33.379416: Desired fold for training: 0
2025-05-24 00:46:33.379487: This split has 99 training and 25 validation cases.
2025-05-24 00:46:33.380000: predicting 004
2025-05-24 00:46:33.398922: 004, shape torch.Size([6, 38, 511, 560]), rank 0
2025-05-24 00:47:06.389283: predicting 013
2025-05-24 00:47:06.413283: 013, shape torch.Size([6, 38, 510, 560]), rank 0
2025-05-24 00:47:11.425786: predicting 016
2025-05-24 00:47:11.448277: 016, shape torch.Size([6, 38, 528, 560]), rank 0
2025-05-24 00:47:17.951175: predicting 019
2025-05-24 00:47:17.980076: 019, shape torch.Size([6, 38, 526, 558]), rank 0
2025-05-24 00:47:24.516770: predicting 024
2025-05-24 00:47:24.543931: 024, shape torch.Size([6, 38, 533, 560]), rank 0
2025-05-24 00:47:31.226627: predicting 025
2025-05-24 00:47:31.250536: 025, shape torch.Size([6, 38, 507, 559]), rank 0
2025-05-24 00:47:36.379716: predicting 039
2025-05-24 00:47:36.402203: 039, shape torch.Size([6, 38, 520, 560]), rank 0
2025-05-24 00:47:42.816703: predicting 040
2025-05-24 00:47:42.836243: 040, shape torch.Size([6, 38, 510, 559]), rank 0
2025-05-24 00:47:47.699913: predicting 045
2025-05-24 00:47:47.724686: 045, shape torch.Size([6, 38, 520, 560]), rank 0
2025-05-24 00:47:54.139553: predicting 047
2025-05-24 00:47:54.163007: 047, shape torch.Size([6, 38, 532, 560]), rank 0
2025-05-24 00:48:00.590311: predicting 048
2025-05-24 00:48:00.609315: 048, shape torch.Size([6, 38, 518, 560]), rank 0
2025-05-24 00:48:07.097898: predicting 051
2025-05-24 00:48:07.125375: 051, shape torch.Size([6, 38, 550, 560]), rank 0
2025-05-24 00:48:13.906337: predicting 054
2025-05-24 00:48:13.930104: 054, shape torch.Size([6, 38, 532, 560]), rank 0
2025-05-24 00:48:20.365041: predicting 063
2025-05-24 00:48:20.384946: 063, shape torch.Size([6, 38, 536, 558]), rank 0
2025-05-24 00:48:26.812822: predicting 065
2025-05-24 00:48:26.836645: 065, shape torch.Size([6, 38, 536, 560]), rank 0
2025-05-24 00:48:33.346163: predicting 068
2025-05-24 00:48:33.371503: 068, shape torch.Size([6, 38, 516, 560]), rank 0
2025-05-24 00:48:39.816609: predicting 069
2025-05-24 00:48:39.839861: 069, shape torch.Size([6, 38, 514, 560]), rank 0
2025-05-24 00:48:46.330780: predicting 079
2025-05-24 00:48:46.354820: 079, shape torch.Size([6, 38, 533, 560]), rank 0
2025-05-24 00:48:52.874118: predicting 086
2025-05-24 00:48:52.899795: 086, shape torch.Size([6, 38, 525, 559]), rank 0
2025-05-24 00:48:59.562002: predicting 090
2025-05-24 00:48:59.590203: 090, shape torch.Size([6, 29, 447, 512]), rank 0
2025-05-24 00:49:02.817383: predicting 093
2025-05-24 00:49:02.859819: 093, shape torch.Size([6, 34, 557, 580]), rank 0
2025-05-24 00:49:09.788450: predicting 094
2025-05-24 00:49:09.813516: 094, shape torch.Size([6, 53, 570, 582]), rank 0
2025-05-24 00:49:20.879820: predicting 104
2025-05-24 00:49:20.920496: 104, shape torch.Size([6, 29, 561, 579]), rank 0
2025-05-24 00:49:25.210948: predicting 107
2025-05-24 00:49:25.260073: 107, shape torch.Size([6, 33, 570, 582]), rank 0
2025-05-24 00:49:31.737819: predicting 108
2025-05-24 00:49:31.760869: 108, shape torch.Size([6, 36, 576, 582]), rank 0
2025-05-24 00:50:57.467687: Validation complete
2025-05-24 00:50:57.468012: Mean Validation Dice:  0.6541960167496002
