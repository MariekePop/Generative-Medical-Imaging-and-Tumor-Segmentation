
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-23 16:59:06.857272: unpacking dataset... 
2025-04-23 16:59:19.271580: unpacking done... 
2025-04-23 16:59:19.285585: do_dummy_2d_data_aug: False 
2025-04-23 16:59:19.299582: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-23 16:59:19.319584: The split file contains 5 splits. 
2025-04-23 16:59:19.330584: Desired fold for training: 0 
2025-04-23 16:59:19.342582: This split has 4 training and 1 validation cases. 
2025-04-23 16:59:20.225760: Unable to plot network architecture: 
2025-04-23 16:59:20.236754: No module named 'hiddenlayer' 
2025-04-23 16:59:20.313749:  
2025-04-23 16:59:20.326750: Epoch 400 
2025-04-23 16:59:20.337750: Current learning rate: 0.0001333 
2025-04-23 17:07:22.416029: train_loss 2.4199 
2025-04-23 17:07:22.443033: val_loss 2.2023 
2025-04-23 17:07:22.465030: PSNR 19.5543 
2025-04-23 17:07:22.487033: Epoch time: 482.11 s 
2025-04-23 17:07:23.376092:  
2025-04-23 17:07:23.397094: Epoch 401 
2025-04-23 17:07:23.412094: Current learning rate: 0.000133 
2025-04-23 17:14:38.311834: train_loss 3.301 
2025-04-23 17:14:38.338834: val_loss 2.5733 
2025-04-23 17:14:38.363863: PSNR 19.4705 
2025-04-23 17:14:38.378860: Epoch time: 434.94 s 
2025-04-23 17:14:39.235421:  
2025-04-23 17:14:39.248422: Epoch 402 
2025-04-23 17:14:39.258419: Current learning rate: 0.0001327 
2025-04-23 17:30:42.213278: train_loss 3.6852 
2025-04-23 17:30:42.229278: val_loss 2.7314 
2025-04-23 17:30:42.243278: PSNR 19.2705 
2025-04-23 17:30:42.255278: Epoch time: 962.98 s 
2025-04-23 17:30:43.249311:  
2025-04-23 17:30:43.264312: Epoch 403 
2025-04-23 17:30:43.278321: Current learning rate: 0.0001324 
2025-04-23 17:46:44.958678: train_loss 3.5836 
2025-04-23 17:46:44.989710: val_loss 2.6017 
2025-04-23 17:46:45.000708: PSNR 19.7549 
2025-04-23 17:46:45.012710: Epoch time: 961.71 s 
2025-04-23 17:46:45.829809:  
2025-04-23 17:46:45.840810: Epoch 404 
2025-04-23 17:46:45.855813: Current learning rate: 0.0001321 
2025-04-23 18:02:51.485344: train_loss 3.6472 
2025-04-23 18:02:51.510348: val_loss 2.6148 
2025-04-23 18:02:51.522352: PSNR 19.8524 
2025-04-23 18:02:51.537349: Epoch time: 965.66 s 
2025-04-23 18:02:52.578165:  
2025-04-23 18:02:52.591166: Epoch 405 
2025-04-23 18:02:52.603167: Current learning rate: 0.0001318 
2025-04-23 18:18:58.599475: train_loss 3.7182 
2025-04-23 18:18:58.623476: val_loss 2.6028 
2025-04-23 18:18:58.641477: PSNR 19.7108 
2025-04-23 18:18:58.654477: Epoch time: 966.03 s 
2025-04-23 18:19:00.003141:  
2025-04-23 18:19:00.025173: Epoch 406 
2025-04-23 18:19:00.039181: Current learning rate: 0.0001315 
2025-04-23 18:35:05.031408: train_loss 3.718 
2025-04-23 18:35:05.063405: val_loss 2.6507 
2025-04-23 18:35:05.086405: PSNR 19.1765 
2025-04-23 18:35:05.105406: Epoch time: 965.03 s 
2025-04-23 18:35:06.475460:  
2025-04-23 18:35:06.498460: Epoch 407 
2025-04-23 18:35:06.520461: Current learning rate: 0.0001312 
2025-04-23 18:51:09.924848: train_loss 3.702 
2025-04-23 18:51:09.953849: val_loss 2.9129 
2025-04-23 18:51:09.970853: PSNR 18.6046 
2025-04-23 18:51:09.991851: Epoch time: 963.46 s 
2025-04-23 18:51:10.820899:  
2025-04-23 18:51:10.836893: Epoch 408 
2025-04-23 18:51:10.857893: Current learning rate: 0.0001309 
2025-04-23 19:07:14.217894: train_loss 3.672 
2025-04-23 19:07:14.232887: val_loss 2.8151 
2025-04-23 19:07:14.245885: PSNR 18.7437 
2025-04-23 19:07:14.265893: Epoch time: 963.4 s 
2025-04-23 19:07:15.309917:  
2025-04-23 19:07:15.329921: Epoch 409 
2025-04-23 19:07:15.342918: Current learning rate: 0.0001306 
2025-04-23 19:23:17.495557: train_loss 3.5551 
2025-04-23 19:23:17.529562: val_loss 2.6114 
2025-04-23 19:23:17.544557: PSNR 19.4059 
2025-04-23 19:23:17.564560: Epoch time: 962.19 s 
2025-04-23 19:23:18.804186:  
2025-04-23 19:23:18.816184: Epoch 410 
2025-04-23 19:23:18.829184: Current learning rate: 0.0001303 
2025-04-23 19:39:26.195772: train_loss 3.641 
2025-04-23 19:39:26.219773: val_loss 2.7614 
2025-04-23 19:39:26.234771: PSNR 19.2683 
2025-04-23 19:39:26.248773: Epoch time: 967.4 s 
2025-04-23 19:39:27.018289:  
2025-04-23 19:39:27.042294: Epoch 411 
2025-04-23 19:39:27.064296: Current learning rate: 0.00013 
2025-04-23 19:55:32.876786: train_loss 3.712 
2025-04-23 19:55:32.908826: val_loss 2.6037 
2025-04-23 19:55:32.919827: PSNR 19.6884 
2025-04-23 19:55:32.930827: Epoch time: 965.86 s 
2025-04-23 19:55:33.740857:  
2025-04-23 19:55:33.753858: Epoch 412 
2025-04-23 19:55:33.764857: Current learning rate: 0.0001297 
2025-04-23 20:11:35.205952: train_loss 3.5665 
2025-04-23 20:11:35.234952: val_loss 2.7261 
2025-04-23 20:11:35.253950: PSNR 19.174 
2025-04-23 20:11:35.264951: Epoch time: 961.47 s 
2025-04-23 20:11:36.289499:  
2025-04-23 20:11:36.305501: Epoch 413 
2025-04-23 20:11:36.319500: Current learning rate: 0.0001294 
2025-04-23 20:27:41.109443: train_loss 3.5955 
2025-04-23 20:27:41.136447: val_loss 2.6694 
2025-04-23 20:27:41.154444: PSNR 19.7276 
2025-04-23 20:27:41.168446: Epoch time: 964.82 s 
2025-04-23 20:27:42.165503:  
2025-04-23 20:27:42.179508: Epoch 414 
2025-04-23 20:27:42.195505: Current learning rate: 0.0001291 
2025-04-23 20:43:42.168440: train_loss 3.5561 
2025-04-23 20:43:42.198441: val_loss 2.8601 
2025-04-23 20:43:42.218441: PSNR 18.6223 
2025-04-23 20:43:42.234441: Epoch time: 960.01 s 
2025-04-23 20:43:43.483585:  
2025-04-23 20:43:43.501583: Epoch 415 
2025-04-23 20:43:43.514584: Current learning rate: 0.0001288 
2025-04-23 20:59:49.117192: train_loss 3.5519 
2025-04-23 20:59:49.132191: val_loss 2.9624 
2025-04-23 20:59:49.143192: PSNR 18.8165 
2025-04-23 20:59:49.156189: Epoch time: 965.64 s 
2025-04-23 20:59:50.099814:  
2025-04-23 20:59:50.115811: Epoch 416 
2025-04-23 20:59:50.129810: Current learning rate: 0.0001285 
2025-04-23 21:15:51.295683: train_loss 3.6831 
2025-04-23 21:15:51.315681: val_loss 2.7171 
2025-04-23 21:15:51.326683: PSNR 18.9457 
2025-04-23 21:15:51.339683: Epoch time: 961.2 s 
2025-04-23 21:15:52.262319:  
2025-04-23 21:15:52.274326: Epoch 417 
2025-04-23 21:15:52.286318: Current learning rate: 0.0001282 
2025-04-23 21:31:57.583807: train_loss 3.6082 
2025-04-23 21:31:57.599807: val_loss 2.5786 
2025-04-23 21:31:57.616806: PSNR 19.8562 
2025-04-23 21:31:57.627805: Epoch time: 965.33 s 
2025-04-23 21:31:58.570974:  
2025-04-23 21:31:58.585974: Epoch 418 
2025-04-23 21:31:58.596975: Current learning rate: 0.0001279 
2025-04-23 21:48:05.579228: train_loss 3.5837 
2025-04-23 21:48:05.594239: val_loss 2.8495 
2025-04-23 21:48:05.605230: PSNR 18.9959 
2025-04-23 21:48:05.617226: Epoch time: 967.01 s 
2025-04-23 21:48:06.405227:  
2025-04-23 21:48:06.427742: Epoch 419 
2025-04-23 21:48:06.441745: Current learning rate: 0.0001276 
2025-04-23 22:04:11.856739: train_loss 3.628 
2025-04-23 22:04:11.887743: val_loss 2.5856 
2025-04-23 22:04:11.899740: PSNR 19.888 
2025-04-23 22:04:11.911738: Epoch time: 965.46 s 
2025-04-23 22:04:12.946404:  
2025-04-23 22:04:12.971442: Epoch 420 
2025-04-23 22:04:12.985442: Current learning rate: 0.0001273 
2025-04-23 22:20:17.980692: train_loss 3.6146 
2025-04-23 22:20:18.013696: val_loss 2.6606 
2025-04-23 22:20:18.028692: PSNR 19.6602 
2025-04-23 22:20:18.038695: Epoch time: 965.04 s 
2025-04-23 22:20:19.010319:  
2025-04-23 22:20:19.033834: Epoch 421 
2025-04-23 22:20:19.048832: Current learning rate: 0.000127 
2025-04-23 22:36:23.602800: train_loss 3.6539 
2025-04-23 22:36:23.615798: val_loss 2.6625 
2025-04-23 22:36:23.625797: PSNR 19.4131 
2025-04-23 22:36:23.635800: Epoch time: 964.6 s 
2025-04-23 22:36:24.598836:  
2025-04-23 22:36:24.613825: Epoch 422 
2025-04-23 22:36:24.624825: Current learning rate: 0.0001267 
2025-04-23 22:52:27.836517: train_loss 3.6709 
2025-04-23 22:52:27.866517: val_loss 2.601 
2025-04-23 22:52:27.878519: PSNR 19.7658 
2025-04-23 22:52:27.892517: Epoch time: 963.24 s 
2025-04-23 22:52:29.116034:  
2025-04-23 22:52:29.130032: Epoch 423 
2025-04-23 22:52:29.143028: Current learning rate: 0.0001264 
2025-04-23 23:08:34.527437: train_loss 3.713 
2025-04-23 23:08:34.541439: val_loss 2.4342 
2025-04-23 23:08:34.555437: PSNR 19.9018 
2025-04-23 23:08:34.569446: Epoch time: 965.41 s 
2025-04-23 23:08:35.371098:  
2025-04-23 23:08:35.388098: Epoch 424 
2025-04-23 23:08:35.404093: Current learning rate: 0.0001261 
2025-04-23 23:24:41.181984: train_loss 3.6062 
2025-04-23 23:24:41.199984: val_loss 2.5762 
2025-04-23 23:24:41.210993: PSNR 19.9952 
2025-04-23 23:24:41.225988: Epoch time: 965.81 s 
2025-04-23 23:24:42.168206:  
2025-04-23 23:24:42.181208: Epoch 425 
2025-04-23 23:24:42.195209: Current learning rate: 0.0001258 
2025-04-23 23:40:44.994385: train_loss 3.6873 
2025-04-23 23:40:45.006384: val_loss 2.5998 
2025-04-23 23:40:45.019386: PSNR 19.664 
2025-04-23 23:40:45.031383: Epoch time: 962.83 s 
2025-04-23 23:40:45.779418:  
2025-04-23 23:40:45.794417: Epoch 426 
2025-04-23 23:40:45.808417: Current learning rate: 0.0001255 
2025-04-23 23:56:52.059194: train_loss 3.6936 
2025-04-23 23:56:52.077195: val_loss 2.7118 
2025-04-23 23:56:52.092196: PSNR 19.5149 
2025-04-23 23:56:52.104196: Epoch time: 966.28 s 
2025-04-23 23:56:53.063196:  
2025-04-23 23:56:53.076194: Epoch 427 
2025-04-23 23:56:53.087195: Current learning rate: 0.0001252 
2025-04-24 00:12:56.061263: train_loss 3.5563 
2025-04-24 00:12:56.095265: val_loss 2.8071 
2025-04-24 00:12:56.109265: PSNR 18.9705 
2025-04-24 00:12:56.120264: Epoch time: 963.0 s 
2025-04-24 00:12:57.363935:  
2025-04-24 00:12:57.376934: Epoch 428 
2025-04-24 00:12:57.390934: Current learning rate: 0.0001249 
2025-04-24 00:29:03.566201: train_loss 3.5145 
2025-04-24 00:29:03.579201: val_loss 2.8329 
2025-04-24 00:29:03.591200: PSNR 19.2058 
2025-04-24 00:29:03.602197: Epoch time: 966.21 s 
2025-04-24 00:29:04.492742:  
2025-04-24 00:29:04.505746: Epoch 429 
2025-04-24 00:29:04.517744: Current learning rate: 0.0001246 
2025-04-24 00:45:06.841093: train_loss 3.6154 
2025-04-24 00:45:06.867121: val_loss 2.7799 
2025-04-24 00:45:06.878127: PSNR 18.9694 
2025-04-24 00:45:06.891131: Epoch time: 962.35 s 
2025-04-24 00:45:07.652130:  
2025-04-24 00:45:07.677165: Epoch 430 
2025-04-24 00:45:07.692163: Current learning rate: 0.0001243 
2025-04-24 01:01:07.942764: train_loss 3.6114 
2025-04-24 01:01:07.967765: val_loss 2.6951 
2025-04-24 01:01:07.990767: PSNR 19.5613 
2025-04-24 01:01:08.013762: Epoch time: 960.29 s 
2025-04-24 01:01:09.070789:  
2025-04-24 01:01:09.085787: Epoch 431 
2025-04-24 01:01:09.096786: Current learning rate: 0.000124 
2025-04-24 01:17:11.316074: train_loss 3.6379 
2025-04-24 01:17:11.339071: val_loss 2.8857 
2025-04-24 01:17:11.360070: PSNR 18.6642 
2025-04-24 01:17:11.380075: Epoch time: 962.25 s 
2025-04-24 01:17:12.682128:  
2025-04-24 01:17:12.700132: Epoch 432 
2025-04-24 01:17:12.717127: Current learning rate: 0.0001237 
2025-04-24 01:33:18.150171: train_loss 3.4888 
2025-04-24 01:33:18.176172: val_loss 2.8002 
2025-04-24 01:33:18.194171: PSNR 19.1474 
2025-04-24 01:33:18.214177: Epoch time: 965.47 s 
2025-04-24 01:33:19.633759:  
2025-04-24 01:33:19.648766: Epoch 433 
2025-04-24 01:33:19.663760: Current learning rate: 0.0001233 
2025-04-24 01:49:25.138106: train_loss 3.6113 
2025-04-24 01:49:25.155107: val_loss 3.0037 
2025-04-24 01:49:25.171107: PSNR 18.6862 
2025-04-24 01:49:25.186106: Epoch time: 965.51 s 
2025-04-24 01:49:25.959107:  
2025-04-24 01:49:25.972106: Epoch 434 
2025-04-24 01:49:25.987110: Current learning rate: 0.000123 
2025-04-24 02:05:27.944086: train_loss 3.5853 
2025-04-24 02:05:27.962087: val_loss 2.6677 
2025-04-24 02:05:27.975092: PSNR 19.5186 
2025-04-24 02:05:27.989092: Epoch time: 961.99 s 
2025-04-24 02:05:28.756088:  
2025-04-24 02:05:28.771088: Epoch 435 
2025-04-24 02:05:28.781092: Current learning rate: 0.0001227 
2025-04-24 02:21:35.689292: train_loss 3.6243 
2025-04-24 02:21:35.702293: val_loss 2.6458 
2025-04-24 02:21:35.713292: PSNR 19.3061 
2025-04-24 02:21:35.729297: Epoch time: 966.94 s 
2025-04-24 02:21:36.693921:  
2025-04-24 02:21:36.710925: Epoch 436 
2025-04-24 02:21:36.721929: Current learning rate: 0.0001224 
2025-04-24 02:37:39.897024: train_loss 3.6193 
2025-04-24 02:37:39.926027: val_loss 2.7137 
2025-04-24 02:37:39.942029: PSNR 19.2323 
2025-04-24 02:37:39.953027: Epoch time: 963.21 s 
2025-04-24 02:37:41.104049:  
2025-04-24 02:37:41.116053: Epoch 437 
2025-04-24 02:37:41.129051: Current learning rate: 0.0001221 
2025-04-24 02:53:44.814916: train_loss 3.5796 
2025-04-24 02:53:44.829914: val_loss 2.6803 
2025-04-24 02:53:44.840916: PSNR 19.4585 
2025-04-24 02:53:44.852916: Epoch time: 963.72 s 
2025-04-24 02:53:45.624056:  
2025-04-24 02:53:45.639060: Epoch 438 
2025-04-24 02:53:45.650054: Current learning rate: 0.0001218 
2025-04-24 03:09:51.451609: train_loss 3.6196 
2025-04-24 03:09:51.471611: val_loss 2.7775 
2025-04-24 03:09:51.484610: PSNR 19.5049 
2025-04-24 03:09:51.495609: Epoch time: 965.83 s 
2025-04-24 03:09:52.299568:  
2025-04-24 03:09:52.312570: Epoch 439 
2025-04-24 03:09:52.326573: Current learning rate: 0.0001215 
2025-04-24 03:25:58.390684: train_loss 3.5694 
2025-04-24 03:25:58.405684: val_loss 2.72 
2025-04-24 03:25:58.416686: PSNR 19.5573 
2025-04-24 03:25:58.429686: Epoch time: 966.09 s 
2025-04-24 03:25:59.374462:  
2025-04-24 03:25:59.390460: Epoch 440 
2025-04-24 03:25:59.401464: Current learning rate: 0.0001212 
2025-04-24 03:42:00.689311: train_loss 3.5389 
2025-04-24 03:42:00.707311: val_loss 2.7676 
2025-04-24 03:42:00.721311: PSNR 19.0961 
2025-04-24 03:42:00.735309: Epoch time: 961.32 s 
2025-04-24 03:42:01.700313:  
2025-04-24 03:42:01.713312: Epoch 441 
2025-04-24 03:42:01.725312: Current learning rate: 0.0001209 
2025-04-24 03:58:00.945926: train_loss 3.6146 
2025-04-24 03:58:00.963925: val_loss 2.5688 
2025-04-24 03:58:00.975929: PSNR 19.7857 
2025-04-24 03:58:00.989928: Epoch time: 959.25 s 
2025-04-24 03:58:01.771969:  
2025-04-24 03:58:01.786972: Epoch 442 
2025-04-24 03:58:01.797968: Current learning rate: 0.0001206 
2025-04-24 04:14:07.723904: train_loss 3.6923 
2025-04-24 04:14:07.747906: val_loss 2.765 
2025-04-24 04:14:07.757903: PSNR 19.3183 
2025-04-24 04:14:07.780903: Epoch time: 965.96 s 
2025-04-24 04:14:08.569905:  
2025-04-24 04:14:08.586906: Epoch 443 
2025-04-24 04:14:08.608907: Current learning rate: 0.0001203 
2025-04-24 04:30:11.107025: train_loss 3.6188 
2025-04-24 04:30:11.136026: val_loss 2.9084 
2025-04-24 04:30:11.158026: PSNR 18.6109 
2025-04-24 04:30:11.179026: Epoch time: 962.54 s 
2025-04-24 04:30:12.302576:  
2025-04-24 04:30:12.315573: Epoch 444 
2025-04-24 04:30:12.329604: Current learning rate: 0.00012 
2025-04-24 04:46:13.054992: train_loss 3.5827 
2025-04-24 04:46:13.076993: val_loss 2.7429 
2025-04-24 04:46:13.095990: PSNR 19.03 
2025-04-24 04:46:13.105988: Epoch time: 960.76 s 
2025-04-24 04:46:13.893018:  
2025-04-24 04:46:13.909023: Epoch 445 
2025-04-24 04:46:13.923017: Current learning rate: 0.0001196 
2025-04-24 05:02:20.646730: train_loss 3.5158 
2025-04-24 05:02:20.661731: val_loss 2.8065 
2025-04-24 05:02:20.672732: PSNR 19.2905 
2025-04-24 05:02:20.683731: Epoch time: 966.76 s 
2025-04-24 05:02:21.445734:  
2025-04-24 05:02:21.457737: Epoch 446 
2025-04-24 05:02:21.471736: Current learning rate: 0.0001193 
2025-04-24 05:18:21.049171: train_loss 3.5908 
2025-04-24 05:18:21.075169: val_loss 2.7332 
2025-04-24 05:18:21.088170: PSNR 19.5806 
2025-04-24 05:18:21.108171: Epoch time: 959.61 s 
2025-04-24 05:18:22.370416:  
2025-04-24 05:18:22.384418: Epoch 447 
2025-04-24 05:18:22.398423: Current learning rate: 0.000119 
2025-04-24 05:34:28.148216: train_loss 3.5622 
2025-04-24 05:34:28.170217: val_loss 2.7528 
2025-04-24 05:34:28.184212: PSNR 19.0816 
2025-04-24 05:34:28.203214: Epoch time: 965.78 s 
2025-04-24 05:34:29.367158:  
2025-04-24 05:34:29.386159: Epoch 448 
2025-04-24 05:34:29.403160: Current learning rate: 0.0001187 
2025-04-24 05:50:31.471600: train_loss 3.5197 
2025-04-24 05:50:31.492599: val_loss 2.5993 
2025-04-24 05:50:31.506600: PSNR 20.0083 
2025-04-24 05:50:31.518601: Epoch time: 962.11 s 
2025-04-24 05:50:32.295603:  
2025-04-24 05:50:32.307143: Epoch 449 
2025-04-24 05:50:32.327662: Current learning rate: 0.0001184 
2025-04-24 06:06:38.003567: train_loss 3.5998 
2025-04-24 06:06:38.016567: val_loss 2.6234 
2025-04-24 06:06:38.031571: PSNR 19.6092 
2025-04-24 06:06:38.042568: Epoch time: 965.71 s 
2025-04-24 06:06:43.385311:  
2025-04-24 06:06:43.397546: Epoch 450 
2025-04-24 06:06:43.414349: Current learning rate: 0.0001181 
2025-04-24 06:22:42.176447: train_loss 3.5371 
2025-04-24 06:22:42.203446: val_loss 2.7872 
2025-04-24 06:22:42.217447: PSNR 19.0364 
2025-04-24 06:22:42.230450: Epoch time: 958.8 s 
2025-04-24 06:22:43.182445:  
2025-04-24 06:22:43.197449: Epoch 451 
2025-04-24 06:22:43.206450: Current learning rate: 0.0001178 
2025-04-24 06:38:45.497967: train_loss 3.599 
2025-04-24 06:38:45.512967: val_loss 2.6044 
2025-04-24 06:38:45.532967: PSNR 19.4247 
2025-04-24 06:38:45.545967: Epoch time: 962.32 s 
2025-04-24 06:38:46.649253:  
2025-04-24 06:38:46.664254: Epoch 452 
2025-04-24 06:38:46.679254: Current learning rate: 0.0001175 
2025-04-24 06:54:48.061462: train_loss 3.5527 
2025-04-24 06:54:48.075462: val_loss 2.4205 
2025-04-24 06:54:48.090467: PSNR 20.5589 
2025-04-24 06:54:48.100464: Epoch time: 961.42 s 
2025-04-24 06:54:48.879695:  
2025-04-24 06:54:48.895693: Epoch 453 
2025-04-24 06:54:48.906693: Current learning rate: 0.0001172 
2025-04-24 07:10:50.036815: train_loss 3.6256 
2025-04-24 07:10:50.057818: val_loss 2.6926 
2025-04-24 07:10:50.070817: PSNR 18.9451 
2025-04-24 07:10:50.082812: Epoch time: 961.16 s 
2025-04-24 07:10:50.880712:  
2025-04-24 07:10:50.898710: Epoch 454 
2025-04-24 07:10:50.909711: Current learning rate: 0.0001169 
2025-04-24 07:26:50.730445: train_loss 3.6064 
2025-04-24 07:26:50.751445: val_loss 2.7137 
2025-04-24 07:26:50.762446: PSNR 18.9672 
2025-04-24 07:26:50.779448: Epoch time: 959.85 s 
2025-04-24 07:26:52.034628:  
2025-04-24 07:26:52.049623: Epoch 455 
2025-04-24 07:26:52.060620: Current learning rate: 0.0001165 
2025-04-24 07:42:55.498827: train_loss 3.5652 
2025-04-24 07:42:55.519828: val_loss 2.6345 
2025-04-24 07:42:55.532825: PSNR 19.6371 
2025-04-24 07:42:55.542831: Epoch time: 963.47 s 
2025-04-24 07:42:56.402865:  
2025-04-24 07:42:56.420867: Epoch 456 
2025-04-24 07:42:56.435870: Current learning rate: 0.0001162 
2025-04-24 07:58:56.828622: train_loss 3.602 
2025-04-24 07:58:56.845622: val_loss 2.7075 
2025-04-24 07:58:56.858622: PSNR 19.7746 
2025-04-24 07:58:56.870626: Epoch time: 960.43 s 
2025-04-24 07:58:57.669623:  
2025-04-24 07:58:57.684621: Epoch 457 
2025-04-24 07:58:57.694621: Current learning rate: 0.0001159 
2025-04-24 08:15:00.267914: train_loss 3.547 
2025-04-24 08:15:00.294912: val_loss 2.5882 
2025-04-24 08:15:00.313915: PSNR 19.5535 
2025-04-24 08:15:00.334915: Epoch time: 962.6 s 
2025-04-24 08:15:01.513608:  
2025-04-24 08:15:01.528609: Epoch 458 
2025-04-24 08:15:01.539614: Current learning rate: 0.0001156 
2025-04-24 08:31:03.909145: train_loss 3.5981 
2025-04-24 08:31:03.927144: val_loss 2.5639 
2025-04-24 08:31:03.937145: PSNR 19.5946 
2025-04-24 08:31:03.953143: Epoch time: 962.4 s 
2025-04-24 08:31:04.756737:  
2025-04-24 08:31:04.772735: Epoch 459 
2025-04-24 08:31:04.788734: Current learning rate: 0.0001153 
2025-04-24 08:47:09.524536: train_loss 3.5164 
2025-04-24 08:47:09.537543: val_loss 2.6666 
2025-04-24 08:47:09.555536: PSNR 19.7748 
2025-04-24 08:47:09.566539: Epoch time: 964.77 s 
2025-04-24 08:47:10.735568:  
2025-04-24 08:47:10.747567: Epoch 460 
2025-04-24 08:47:10.760568: Current learning rate: 0.000115 
2025-04-24 09:03:14.867002: train_loss 3.6236 
2025-04-24 09:03:14.882003: val_loss 2.6821 
2025-04-24 09:03:14.896006: PSNR 19.4463 
2025-04-24 09:03:14.908002: Epoch time: 964.13 s 
2025-04-24 09:03:15.996059:  
2025-04-24 09:03:16.009063: Epoch 461 
2025-04-24 09:03:16.021064: Current learning rate: 0.0001147 
2025-04-24 09:19:19.893994: train_loss 3.6031 
2025-04-24 09:19:19.912993: val_loss 2.7907 
2025-04-24 09:19:19.926992: PSNR 19.0013 
2025-04-24 09:19:19.938996: Epoch time: 963.9 s 
2025-04-24 09:19:20.701040:  
2025-04-24 09:19:20.713037: Epoch 462 
2025-04-24 09:19:20.724037: Current learning rate: 0.0001144 
2025-04-24 09:35:19.786431: train_loss 3.5677 
2025-04-24 09:35:19.816427: val_loss 2.9339 
2025-04-24 09:35:19.838429: PSNR 18.9708 
2025-04-24 09:35:19.854428: Epoch time: 959.09 s 
2025-04-24 09:35:20.997040:  
2025-04-24 09:35:21.008039: Epoch 463 
2025-04-24 09:35:21.021039: Current learning rate: 0.000114 
2025-04-24 09:51:21.837314: train_loss 3.5226 
2025-04-24 09:51:21.866352: val_loss 2.6412 
2025-04-24 09:51:21.877358: PSNR 19.3325 
2025-04-24 09:51:21.890355: Epoch time: 960.84 s 
2025-04-24 09:51:22.662355:  
2025-04-24 09:51:22.684355: Epoch 464 
2025-04-24 09:51:22.704403: Current learning rate: 0.0001137 
2025-04-24 10:07:25.286211: train_loss 3.483 
2025-04-24 10:07:25.305211: val_loss 3.0813 
2025-04-24 10:07:25.323213: PSNR 18.0452 
2025-04-24 10:07:25.337212: Epoch time: 962.63 s 
2025-04-24 10:07:26.172012:  
2025-04-24 10:07:26.188015: Epoch 465 
2025-04-24 10:07:26.203015: Current learning rate: 0.0001134 
2025-04-24 10:23:27.415244: train_loss 3.6107 
2025-04-24 10:23:27.434244: val_loss 2.6524 
2025-04-24 10:23:27.451244: PSNR 19.7517 
2025-04-24 10:23:27.463241: Epoch time: 961.25 s 
2025-04-24 10:23:28.486475:  
2025-04-24 10:23:28.500476: Epoch 466 
2025-04-24 10:23:28.516477: Current learning rate: 0.0001131 
2025-04-24 10:39:29.371998: train_loss 3.5498 
2025-04-24 10:39:29.396002: val_loss 3.0388 
2025-04-24 10:39:29.416000: PSNR 18.7249 
2025-04-24 10:39:29.438001: Epoch time: 960.89 s 
2025-04-24 10:39:30.240693:  
2025-04-24 10:39:30.263695: Epoch 467 
2025-04-24 10:39:30.280691: Current learning rate: 0.0001128 
2025-04-24 10:55:33.759647: train_loss 3.4646 
2025-04-24 10:55:33.776649: val_loss 2.5602 
2025-04-24 10:55:33.793649: PSNR 19.8903 
2025-04-24 10:55:33.809648: Epoch time: 963.52 s 
2025-04-24 10:55:34.641646:  
2025-04-24 10:55:34.658649: Epoch 468 
2025-04-24 10:55:34.670650: Current learning rate: 0.0001125 
2025-04-24 11:11:35.730895: train_loss 3.5717 
2025-04-24 11:11:35.745895: val_loss 2.6058 
2025-04-24 11:11:35.758893: PSNR 19.7285 
2025-04-24 11:11:35.771895: Epoch time: 961.09 s 
2025-04-24 11:11:36.723925:  
2025-04-24 11:11:36.739923: Epoch 469 
2025-04-24 11:11:36.752924: Current learning rate: 0.0001122 
2025-04-24 11:27:35.998786: train_loss 3.4695 
2025-04-24 11:27:36.013793: val_loss 2.7382 
2025-04-24 11:27:36.033793: PSNR 19.4844 
2025-04-24 11:27:36.047787: Epoch time: 959.28 s 
2025-04-24 11:27:36.826444:  
2025-04-24 11:27:36.841442: Epoch 470 
2025-04-24 11:27:36.854440: Current learning rate: 0.0001119 
2025-04-24 11:43:39.016765: train_loss 3.6031 
2025-04-24 11:43:39.043769: val_loss 2.6773 
2025-04-24 11:43:39.060766: PSNR 19.3128 
2025-04-24 11:43:39.081766: Epoch time: 962.19 s 
2025-04-24 11:43:40.035799:  
2025-04-24 11:43:40.048412: Epoch 471 
2025-04-24 11:43:40.065310: Current learning rate: 0.0001115 
2025-04-24 11:59:42.588680: train_loss 3.4733 
2025-04-24 11:59:42.605682: val_loss 2.6743 
2025-04-24 11:59:42.617682: PSNR 19.7077 
2025-04-24 11:59:42.631682: Epoch time: 962.56 s 
2025-04-24 11:59:43.516683:  
2025-04-24 11:59:43.531682: Epoch 472 
2025-04-24 11:59:43.545683: Current learning rate: 0.0001112 
2025-04-24 12:15:42.130936: train_loss 3.5544 
2025-04-24 12:15:42.143937: val_loss 2.7287 
2025-04-24 12:15:42.154934: PSNR 18.9897 
2025-04-24 12:15:42.164935: Epoch time: 958.62 s 
2025-04-24 12:15:42.952965:  
2025-04-24 12:15:42.967966: Epoch 473 
2025-04-24 12:15:42.979965: Current learning rate: 0.0001109 
