
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-22 17:04:40.283151: unpacking dataset... 
2025-04-22 17:04:52.795766: unpacking done... 
2025-04-22 17:04:52.816774: do_dummy_2d_data_aug: False 
2025-04-22 17:04:52.836767: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-22 17:04:52.864765: The split file contains 5 splits. 
2025-04-22 17:04:52.880766: Desired fold for training: 0 
2025-04-22 17:04:52.895772: This split has 4 training and 1 validation cases. 
2025-04-22 17:04:53.886285: Unable to plot network architecture: 
2025-04-22 17:04:53.902288: No module named 'hiddenlayer' 
2025-04-22 17:04:53.983541:  
2025-04-22 17:04:54.001544: Epoch 350 
2025-04-22 17:04:54.018539: Current learning rate: 0.0001477 
2025-04-22 17:23:31.050245: train_loss 2.525 
2025-04-22 17:23:31.071241: val_loss 2.1694 
2025-04-22 17:23:31.085240: PSNR 19.4885 
2025-04-22 17:23:31.097241: Epoch time: 1117.07 s 
2025-04-22 17:23:32.211998:  
2025-04-22 17:23:32.226004: Epoch 351 
2025-04-22 17:23:32.244997: Current learning rate: 0.0001475 
2025-04-22 17:39:28.068609: train_loss 3.4429 
2025-04-22 17:39:28.097605: val_loss 2.5585 
2025-04-22 17:39:28.111607: PSNR 19.8299 
2025-04-22 17:39:28.127202: Epoch time: 955.86 s 
2025-04-22 17:39:29.286205:  
2025-04-22 17:39:29.303207: Epoch 352 
2025-04-22 17:39:29.318210: Current learning rate: 0.0001472 
2025-04-22 18:00:19.552590: train_loss 3.711 
2025-04-22 18:00:19.568585: val_loss 2.5436 
2025-04-22 18:00:19.580587: PSNR 19.8355 
2025-04-22 18:00:19.594587: Epoch time: 1250.27 s 
2025-04-22 18:00:20.391590:  
2025-04-22 18:00:20.408590: Epoch 353 
2025-04-22 18:00:20.423588: Current learning rate: 0.0001469 
2025-04-22 18:21:07.886018: train_loss 3.7275 
2025-04-22 18:21:07.902020: val_loss 2.5075 
2025-04-22 18:21:07.915020: PSNR 20.0008 
2025-04-22 18:21:07.930020: Epoch time: 1247.5 s 
2025-04-22 18:21:07.942022: Yayy! New best EMA pseudo PSNR: 19.453 
2025-04-22 18:21:13.849295:  
2025-04-22 18:21:13.868298: Epoch 354 
2025-04-22 18:21:13.892295: Current learning rate: 0.0001466 
2025-04-22 18:41:54.137407: train_loss 3.7466 
2025-04-22 18:41:54.151404: val_loss 2.626 
2025-04-22 18:41:54.163407: PSNR 19.6999 
2025-04-22 18:41:54.176405: Epoch time: 1240.29 s 
2025-04-22 18:41:54.189407: Yayy! New best EMA pseudo PSNR: 19.4777 
2025-04-22 18:41:59.702314:  
2025-04-22 18:41:59.717314: Epoch 355 
2025-04-22 18:41:59.731313: Current learning rate: 0.0001463 
2025-04-22 19:02:39.841432: train_loss 3.8073 
2025-04-22 19:02:39.858433: val_loss 2.6302 
2025-04-22 19:02:39.878433: PSNR 19.035 
2025-04-22 19:02:39.891434: Epoch time: 1240.14 s 
2025-04-22 19:02:41.080209:  
2025-04-22 19:02:41.094216: Epoch 356 
2025-04-22 19:02:41.109208: Current learning rate: 0.0001461 
2025-04-22 19:23:18.089239: train_loss 3.7411 
2025-04-22 19:23:18.104237: val_loss 2.7407 
2025-04-22 19:23:18.126274: PSNR 18.884 
2025-04-22 19:23:18.141268: Epoch time: 1237.02 s 
2025-04-22 19:23:19.614306:  
2025-04-22 19:23:19.632307: Epoch 357 
2025-04-22 19:23:19.657824: Current learning rate: 0.0001458 
2025-04-22 19:43:59.964450: train_loss 3.7669 
2025-04-22 19:43:59.986448: val_loss 2.5303 
2025-04-22 19:43:59.997448: PSNR 20.0265 
2025-04-22 19:44:00.008449: Epoch time: 1240.35 s 
2025-04-22 19:44:01.031476:  
2025-04-22 19:44:01.043480: Epoch 358 
2025-04-22 19:44:01.062479: Current learning rate: 0.0001455 
2025-04-22 20:04:37.445652: train_loss 3.7525 
2025-04-22 20:04:37.470170: val_loss 2.6274 
2025-04-22 20:04:37.493167: PSNR 19.3958 
2025-04-22 20:04:37.505165: Epoch time: 1236.42 s 
2025-04-22 20:04:38.745194:  
2025-04-22 20:04:38.763197: Epoch 359 
2025-04-22 20:04:38.775198: Current learning rate: 0.0001452 
2025-04-22 20:25:19.099962: train_loss 3.7817 
2025-04-22 20:25:19.118964: val_loss 2.8744 
2025-04-22 20:25:19.129963: PSNR 18.5326 
2025-04-22 20:25:19.143962: Epoch time: 1240.36 s 
2025-04-22 20:25:19.928993:  
2025-04-22 20:25:19.945999: Epoch 360 
2025-04-22 20:25:19.962007: Current learning rate: 0.0001449 
2025-04-22 20:45:56.207571: train_loss 3.7544 
2025-04-22 20:45:56.233283: val_loss 2.7271 
2025-04-22 20:45:56.253281: PSNR 19.2303 
2025-04-22 20:45:56.273282: Epoch time: 1236.28 s 
2025-04-22 20:45:57.250313:  
2025-04-22 20:45:57.264312: Epoch 361 
2025-04-22 20:45:57.284313: Current learning rate: 0.0001447 
2025-04-22 21:06:34.121498: train_loss 3.7017 
2025-04-22 21:06:34.146498: val_loss 2.6597 
2025-04-22 21:06:34.160498: PSNR 19.2672 
2025-04-22 21:06:34.171494: Epoch time: 1236.88 s 
2025-04-22 21:06:35.619038:  
2025-04-22 21:06:35.636038: Epoch 362 
2025-04-22 21:06:35.648040: Current learning rate: 0.0001444 
2025-04-22 21:27:16.203133: train_loss 3.7928 
2025-04-22 21:27:16.220131: val_loss 2.5691 
2025-04-22 21:27:16.237132: PSNR 19.7663 
2025-04-22 21:27:16.251132: Epoch time: 1240.59 s 
2025-04-22 21:27:17.104193:  
2025-04-22 21:27:17.119191: Epoch 363 
2025-04-22 21:27:17.130202: Current learning rate: 0.0001441 
2025-04-22 21:48:03.998436: train_loss 3.7862 
2025-04-22 21:48:04.013434: val_loss 2.629 
2025-04-22 21:48:04.028434: PSNR 19.5206 
2025-04-22 21:48:04.039436: Epoch time: 1246.9 s 
2025-04-22 21:48:04.828988:  
2025-04-22 21:48:04.840987: Epoch 364 
2025-04-22 21:48:04.854984: Current learning rate: 0.0001438 
2025-04-22 22:08:44.674145: train_loss 3.69 
2025-04-22 22:08:44.702144: val_loss 2.708 
2025-04-22 22:08:44.716146: PSNR 19.6159 
2025-04-22 22:08:44.737144: Epoch time: 1239.85 s 
2025-04-22 22:08:45.751181:  
2025-04-22 22:08:45.768185: Epoch 365 
2025-04-22 22:08:45.782181: Current learning rate: 0.0001435 
2025-04-22 22:29:31.617870: train_loss 3.7852 
2025-04-22 22:29:31.646392: val_loss 2.406 
2025-04-22 22:29:31.662391: PSNR 20.4543 
2025-04-22 22:29:31.673398: Epoch time: 1245.87 s 
2025-04-22 22:29:31.687394: Yayy! New best EMA pseudo PSNR: 19.5149 
2025-04-22 22:29:38.306925:  
2025-04-22 22:29:38.326924: Epoch 366 
2025-04-22 22:29:38.345928: Current learning rate: 0.0001432 
2025-04-22 22:50:15.768858: train_loss 3.735 
2025-04-22 22:50:15.796856: val_loss 2.8543 
2025-04-22 22:50:15.810861: PSNR 18.3833 
2025-04-22 22:50:15.824865: Epoch time: 1237.47 s 
2025-04-22 22:50:17.178960:  
2025-04-22 22:50:17.201964: Epoch 367 
2025-04-22 22:50:17.212958: Current learning rate: 0.0001429 
2025-04-22 23:11:05.644315: train_loss 3.7249 
2025-04-22 23:11:05.680318: val_loss 2.7322 
2025-04-22 23:11:05.692319: PSNR 19.2331 
2025-04-22 23:11:05.704314: Epoch time: 1248.47 s 
2025-04-22 23:11:07.145834:  
2025-04-22 23:11:07.168832: Epoch 368 
2025-04-22 23:11:07.190832: Current learning rate: 0.0001427 
2025-04-22 23:31:45.549268: train_loss 3.7021 
2025-04-22 23:31:45.564266: val_loss 2.3837 
2025-04-22 23:31:45.580264: PSNR 19.6537 
2025-04-22 23:31:45.593267: Epoch time: 1238.41 s 
2025-04-22 23:31:46.384848:  
2025-04-22 23:31:46.399845: Epoch 369 
2025-04-22 23:31:46.414846: Current learning rate: 0.0001424 
2025-04-22 23:52:28.420887: train_loss 3.6542 
2025-04-22 23:52:28.444890: val_loss 2.9416 
2025-04-22 23:52:28.458888: PSNR 18.3213 
2025-04-22 23:52:28.479889: Epoch time: 1242.04 s 
2025-04-22 23:52:29.306924:  
2025-04-22 23:52:29.329930: Epoch 370 
2025-04-22 23:52:29.349928: Current learning rate: 0.0001421 
2025-04-23 00:13:07.527847: train_loss 3.7461 
2025-04-23 00:13:07.545851: val_loss 2.7666 
2025-04-23 00:13:07.559850: PSNR 19.1398 
2025-04-23 00:13:07.572850: Epoch time: 1238.22 s 
2025-04-23 00:13:08.820905:  
2025-04-23 00:13:08.835907: Epoch 371 
2025-04-23 00:13:08.854906: Current learning rate: 0.0001418 
2025-04-23 00:33:57.139747: train_loss 3.7832 
2025-04-23 00:33:57.161747: val_loss 2.67 
2025-04-23 00:33:57.174749: PSNR 19.4651 
2025-04-23 00:33:57.190749: Epoch time: 1248.32 s 
2025-04-23 00:33:58.419603:  
2025-04-23 00:33:58.431600: Epoch 372 
2025-04-23 00:33:58.443602: Current learning rate: 0.0001415 
2025-04-23 00:54:48.200560: train_loss 3.6238 
2025-04-23 00:54:48.214561: val_loss 2.7669 
2025-04-23 00:54:48.229563: PSNR 18.7627 
2025-04-23 00:54:48.241560: Epoch time: 1249.78 s 
2025-04-23 00:54:49.094267:  
2025-04-23 00:54:49.106269: Epoch 373 
2025-04-23 00:54:49.116269: Current learning rate: 0.0001412 
2025-04-23 01:15:36.266693: train_loss 3.6591 
2025-04-23 01:15:36.289693: val_loss 2.5001 
2025-04-23 01:15:36.311695: PSNR 19.8089 
2025-04-23 01:15:36.331696: Epoch time: 1247.18 s 
2025-04-23 01:15:37.586327:  
2025-04-23 01:15:37.600333: Epoch 374 
2025-04-23 01:15:37.613326: Current learning rate: 0.0001409 
2025-04-23 01:36:25.114055: train_loss 3.6957 
2025-04-23 01:36:25.133055: val_loss 2.9034 
2025-04-23 01:36:25.148058: PSNR 18.8316 
2025-04-23 01:36:25.163058: Epoch time: 1247.53 s 
2025-04-23 01:36:26.217577:  
2025-04-23 01:36:26.230832: Epoch 375 
2025-04-23 01:36:26.242138: Current learning rate: 0.0001407 
2025-04-23 01:57:16.807350: train_loss 3.7298 
2025-04-23 01:57:16.836357: val_loss 2.5943 
2025-04-23 01:57:16.851353: PSNR 19.7023 
2025-04-23 01:57:16.867352: Epoch time: 1250.59 s 
2025-04-23 01:57:17.656383:  
2025-04-23 01:57:17.676382: Epoch 376 
2025-04-23 01:57:17.697384: Current learning rate: 0.0001404 
2025-04-23 02:18:08.098804: train_loss 3.6757 
2025-04-23 02:18:08.116805: val_loss 2.7401 
2025-04-23 02:18:08.130804: PSNR 19.1751 
2025-04-23 02:18:08.143804: Epoch time: 1250.45 s 
2025-04-23 02:18:09.297460:  
2025-04-23 02:18:09.311463: Epoch 377 
2025-04-23 02:18:09.323460: Current learning rate: 0.0001401 
2025-04-23 02:38:51.782620: train_loss 3.657 
2025-04-23 02:38:51.798624: val_loss 2.8159 
2025-04-23 02:38:51.811625: PSNR 18.9198 
2025-04-23 02:38:51.824625: Epoch time: 1242.49 s 
2025-04-23 02:38:52.863138:  
2025-04-23 02:38:52.887168: Epoch 378 
2025-04-23 02:38:52.900178: Current learning rate: 0.0001398 
2025-04-23 02:59:34.843596: train_loss 3.7547 
2025-04-23 02:59:34.858604: val_loss 2.6458 
2025-04-23 02:59:34.870597: PSNR 19.3773 
2025-04-23 02:59:34.885600: Epoch time: 1241.99 s 
2025-04-23 02:59:35.672600:  
2025-04-23 02:59:35.687599: Epoch 379 
2025-04-23 02:59:35.700600: Current learning rate: 0.0001395 
2025-04-23 03:20:25.889961: train_loss 3.7529 
2025-04-23 03:20:25.913962: val_loss 2.6658 
2025-04-23 03:20:25.937959: PSNR 19.4859 
2025-04-23 03:20:25.957963: Epoch time: 1250.22 s 
2025-04-23 03:20:26.973019:  
2025-04-23 03:20:26.994015: Epoch 380 
2025-04-23 03:20:27.008619: Current learning rate: 0.0001392 
2025-04-23 03:41:17.764164: train_loss 3.7037 
2025-04-23 03:41:17.781169: val_loss 2.7519 
2025-04-23 03:41:17.794164: PSNR 19.3174 
2025-04-23 03:41:17.814163: Epoch time: 1250.8 s 
2025-04-23 03:41:18.617712:  
2025-04-23 03:41:18.634707: Epoch 381 
2025-04-23 03:41:18.655706: Current learning rate: 0.0001389 
2025-04-23 04:02:08.197337: train_loss 3.7343 
2025-04-23 04:02:08.210336: val_loss 2.513 
2025-04-23 04:02:08.224335: PSNR 20.1081 
2025-04-23 04:02:08.237338: Epoch time: 1249.58 s 
2025-04-23 04:02:09.264334:  
2025-04-23 04:02:09.279335: Epoch 382 
2025-04-23 04:02:09.293333: Current learning rate: 0.0001386 
2025-04-23 04:22:53.032790: train_loss 3.7106 
2025-04-23 04:22:53.052787: val_loss 2.8287 
2025-04-23 04:22:53.065789: PSNR 18.6872 
2025-04-23 04:22:53.080787: Epoch time: 1243.77 s 
2025-04-23 04:22:53.957785:  
2025-04-23 04:22:53.974786: Epoch 383 
2025-04-23 04:22:53.990785: Current learning rate: 0.0001383 
2025-04-23 04:43:35.451313: train_loss 3.802 
2025-04-23 04:43:35.472318: val_loss 2.6106 
2025-04-23 04:43:35.485315: PSNR 18.7707 
2025-04-23 04:43:35.500315: Epoch time: 1241.5 s 
2025-04-23 04:43:36.447836:  
2025-04-23 04:43:36.465833: Epoch 384 
2025-04-23 04:43:36.480836: Current learning rate: 0.000138 
2025-04-23 05:04:24.053869: train_loss 3.661 
2025-04-23 05:04:24.066870: val_loss 2.724 
2025-04-23 05:04:24.077870: PSNR 19.1356 
2025-04-23 05:04:24.088871: Epoch time: 1247.61 s 
2025-04-23 05:04:24.920532:  
2025-04-23 05:04:24.936525: Epoch 385 
2025-04-23 05:04:24.947527: Current learning rate: 0.0001378 
2025-04-23 05:25:14.125252: train_loss 3.6608 
2025-04-23 05:25:14.159254: val_loss 2.5125 
2025-04-23 05:25:14.173252: PSNR 19.9796 
2025-04-23 05:25:14.194254: Epoch time: 1249.21 s 
2025-04-23 05:25:15.415804:  
2025-04-23 05:25:15.429804: Epoch 386 
2025-04-23 05:25:15.441803: Current learning rate: 0.0001375 
2025-04-23 05:45:58.194789: train_loss 3.6783 
2025-04-23 05:45:58.218790: val_loss 2.6346 
2025-04-23 05:45:58.231790: PSNR 19.6519 
2025-04-23 05:45:58.244790: Epoch time: 1242.78 s 
2025-04-23 05:45:59.085880:  
2025-04-23 05:45:59.103885: Epoch 387 
2025-04-23 05:45:59.124883: Current learning rate: 0.0001372 
2025-04-23 06:06:48.952790: train_loss 3.7642 
2025-04-23 06:06:48.972791: val_loss 2.6093 
2025-04-23 06:06:48.988788: PSNR 19.3443 
2025-04-23 06:06:48.999791: Epoch time: 1249.87 s 
2025-04-23 06:06:50.184325:  
2025-04-23 06:06:50.197330: Epoch 388 
2025-04-23 06:06:50.209325: Current learning rate: 0.0001369 
2025-04-23 06:27:32.015619: train_loss 3.7335 
2025-04-23 06:27:32.033621: val_loss 2.6044 
2025-04-23 06:27:32.051616: PSNR 19.3165 
2025-04-23 06:27:32.061619: Epoch time: 1241.84 s 
2025-04-23 06:27:33.055622:  
2025-04-23 06:27:33.067619: Epoch 389 
2025-04-23 06:27:33.082617: Current learning rate: 0.0001366 
2025-04-23 06:48:13.765013: train_loss 3.7055 
2025-04-23 06:48:13.780013: val_loss 2.7513 
2025-04-23 06:48:13.795014: PSNR 18.7066 
2025-04-23 06:48:13.808014: Epoch time: 1240.71 s 
2025-04-23 06:48:14.859568:  
2025-04-23 06:48:14.874564: Epoch 390 
2025-04-23 06:48:14.885565: Current learning rate: 0.0001363 
2025-04-23 07:08:52.081873: train_loss 3.672 
2025-04-23 07:08:52.114870: val_loss 2.5398 
2025-04-23 07:08:52.128872: PSNR 19.942 
2025-04-23 07:08:52.147871: Epoch time: 1237.23 s 
2025-04-23 07:08:53.375394:  
2025-04-23 07:08:53.391391: Epoch 391 
2025-04-23 07:08:53.408401: Current learning rate: 0.000136 
2025-04-23 07:29:31.389830: train_loss 3.7052 
2025-04-23 07:29:31.411153: val_loss 2.6018 
2025-04-23 07:29:31.427285: PSNR 19.3405 
2025-04-23 07:29:31.442109: Epoch time: 1238.02 s 
2025-04-23 07:29:32.251109:  
2025-04-23 07:29:32.264112: Epoch 392 
2025-04-23 07:29:32.274112: Current learning rate: 0.0001357 
2025-04-23 07:50:15.165961: train_loss 3.7536 
2025-04-23 07:50:15.199196: val_loss 2.5204 
2025-04-23 07:50:15.215201: PSNR 19.3467 
2025-04-23 07:50:15.229196: Epoch time: 1242.92 s 
2025-04-23 07:50:16.069226:  
2025-04-23 07:50:16.087227: Epoch 393 
2025-04-23 07:50:16.103226: Current learning rate: 0.0001354 
2025-04-23 08:10:53.563339: train_loss 3.7422 
2025-04-23 08:10:53.593338: val_loss 2.4916 
2025-04-23 08:10:53.619339: PSNR 19.3869 
2025-04-23 08:10:53.639339: Epoch time: 1237.5 s 
2025-04-23 08:10:54.680339:  
2025-04-23 08:10:54.692338: Epoch 394 
2025-04-23 08:10:54.703338: Current learning rate: 0.0001351 
2025-04-23 08:31:36.852396: train_loss 3.6745 
2025-04-23 08:31:36.869400: val_loss 2.7532 
2025-04-23 08:31:36.881408: PSNR 19.0471 
2025-04-23 08:31:36.891398: Epoch time: 1242.18 s 
2025-04-23 08:31:38.214454:  
2025-04-23 08:31:38.230457: Epoch 395 
2025-04-23 08:31:38.241457: Current learning rate: 0.0001348 
2025-04-23 08:52:28.561589: train_loss 3.7546 
2025-04-23 08:52:28.596589: val_loss 2.6948 
2025-04-23 08:52:28.628592: PSNR 19.7963 
2025-04-23 08:52:28.658592: Epoch time: 1250.35 s 
2025-04-23 08:52:29.629186:  
2025-04-23 08:52:29.658190: Epoch 396 
2025-04-23 08:52:29.680705: Current learning rate: 0.0001345 
2025-04-23 09:13:20.732238: train_loss 3.6543 
2025-04-23 09:13:20.752235: val_loss 2.7674 
2025-04-23 09:13:20.764239: PSNR 18.6097 
2025-04-23 09:13:20.778235: Epoch time: 1251.11 s 
2025-04-23 09:13:21.569807:  
2025-04-23 09:13:21.585810: Epoch 397 
2025-04-23 09:13:21.597806: Current learning rate: 0.0001342 
2025-04-23 09:34:10.584639: train_loss 3.6492 
2025-04-23 09:34:10.610640: val_loss 2.649 
2025-04-23 09:34:10.624640: PSNR 20.1846 
2025-04-23 09:34:10.636642: Epoch time: 1249.02 s 
2025-04-23 09:34:11.931156:  
2025-04-23 09:34:11.950151: Epoch 398 
2025-04-23 09:34:11.963158: Current learning rate: 0.0001339 
2025-04-23 09:55:01.847763: train_loss 3.6567 
2025-04-23 09:55:01.869763: val_loss 2.8081 
2025-04-23 09:55:01.883759: PSNR 19.0022 
2025-04-23 09:55:01.894759: Epoch time: 1249.92 s 
2025-04-23 09:55:02.876795:  
2025-04-23 09:55:02.891801: Epoch 399 
2025-04-23 09:55:02.901798: Current learning rate: 0.0001336 
2025-04-23 10:15:45.462326: train_loss 3.6838 
2025-04-23 10:15:45.481323: val_loss 2.8404 
2025-04-23 10:15:45.494324: PSNR 18.5882 
2025-04-23 10:15:45.508326: Epoch time: 1242.59 s 
2025-04-23 10:15:51.207132:  
2025-04-23 10:15:51.219131: Epoch 400 
2025-04-23 10:15:51.231130: Current learning rate: 0.0001333 
2025-04-23 10:36:30.986186: train_loss 3.6264 
2025-04-23 10:36:31.008184: val_loss 2.6977 
2025-04-23 10:36:31.030184: PSNR 19.4314 
2025-04-23 10:36:31.042189: Epoch time: 1239.78 s 
2025-04-23 10:36:31.976260:  
2025-04-23 10:36:31.988260: Epoch 401 
2025-04-23 10:36:32.005259: Current learning rate: 0.000133 
2025-04-23 10:57:17.329039: train_loss 3.6446 
2025-04-23 10:57:17.343046: val_loss 2.7261 
2025-04-23 10:57:17.357042: PSNR 19.2168 
2025-04-23 10:57:17.368039: Epoch time: 1245.36 s 
2025-04-23 10:57:18.189072:  
2025-04-23 10:57:18.204069: Epoch 402 
2025-04-23 10:57:18.215069: Current learning rate: 0.0001327 
2025-04-23 11:18:04.773469: train_loss 3.6803 
2025-04-23 11:18:04.791470: val_loss 2.6335 
2025-04-23 11:18:04.805477: PSNR 19.819 
2025-04-23 11:18:04.815470: Epoch time: 1246.59 s 
2025-04-23 11:18:05.624011:  
2025-04-23 11:18:05.639014: Epoch 403 
2025-04-23 11:18:05.653013: Current learning rate: 0.0001324 
2025-04-23 11:38:45.693562: train_loss 3.6377 
2025-04-23 11:38:45.722561: val_loss 2.5799 
2025-04-23 11:38:45.744565: PSNR 19.9351 
2025-04-23 11:38:45.766564: Epoch time: 1240.07 s 
2025-04-23 11:38:46.743117:  
2025-04-23 11:38:46.767111: Epoch 404 
2025-04-23 11:38:46.788112: Current learning rate: 0.0001321 
2025-04-23 11:59:32.549075: train_loss 3.6159 
2025-04-23 11:59:32.576077: val_loss 2.6243 
2025-04-23 11:59:32.597080: PSNR 19.4349 
2025-04-23 11:59:32.619076: Epoch time: 1245.81 s 
2025-04-23 11:59:33.444074:  
2025-04-23 11:59:33.467079: Epoch 405 
2025-04-23 11:59:33.481076: Current learning rate: 0.0001318 
