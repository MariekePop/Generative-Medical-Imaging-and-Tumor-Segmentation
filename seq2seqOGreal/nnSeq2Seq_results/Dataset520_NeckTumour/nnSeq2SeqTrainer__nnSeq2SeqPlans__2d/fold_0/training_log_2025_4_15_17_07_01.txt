
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-15 17:07:15.748892: unpacking dataset... 
2025-04-15 17:07:27.967711: unpacking done... 
2025-04-15 17:07:27.983329: do_dummy_2d_data_aug: False 
2025-04-15 17:07:27.998953: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-15 17:07:28.014577: The split file contains 5 splits. 
2025-04-15 17:07:28.030214: Desired fold for training: 0 
2025-04-15 17:07:28.030214: This split has 4 training and 1 validation cases. 
2025-04-15 17:07:28.811455: Unable to plot network architecture: 
2025-04-15 17:07:28.827080: No module named 'hiddenlayer' 
2025-04-15 17:07:28.920832:  
2025-04-15 17:07:28.936460: Epoch 150 
2025-04-15 17:07:28.936460: Current learning rate: 0.0001904 
2025-04-15 17:14:30.733650: train_loss 3.3795 
2025-04-15 17:14:30.749275: val_loss 2.0926 
2025-04-15 17:14:30.749275: PSNR 19.7641 
2025-04-15 17:14:30.764900: Epoch time: 421.83 s 
2025-04-15 17:14:31.840395:  
2025-04-15 17:14:31.856022: Epoch 151 
2025-04-15 17:14:31.856022: Current learning rate: 0.0001903 
2025-04-15 17:27:54.344960: train_loss 4.4062 
2025-04-15 17:27:54.376209: val_loss 2.6774 
2025-04-15 17:27:54.407456: PSNR 18.3846 
2025-04-15 17:27:54.423083: Epoch time: 802.5 s 
2025-04-15 17:27:55.636544:  
2025-04-15 17:27:55.667792: Epoch 152 
2025-04-15 17:27:55.677301: Current learning rate: 0.0001901 
2025-04-15 17:50:46.607508: train_loss 4.7803 
2025-04-15 17:50:46.623132: val_loss 2.8362 
2025-04-15 17:50:46.638758: PSNR 18.3591 
2025-04-15 17:50:46.654383: Epoch time: 1370.97 s 
2025-04-15 17:50:47.488924:  
2025-04-15 17:50:47.504551: Epoch 153 
2025-04-15 17:50:47.520175: Current learning rate: 0.00019 
2025-04-15 18:13:37.796771: train_loss 4.5898 
2025-04-15 18:13:37.812397: val_loss 2.7694 
2025-04-15 18:13:37.843647: PSNR 18.361 
2025-04-15 18:13:37.843647: Epoch time: 1370.31 s 
2025-04-15 18:13:38.992028:  
2025-04-15 18:13:39.007653: Epoch 154 
2025-04-15 18:13:39.023277: Current learning rate: 0.0001898 
2025-04-15 18:36:30.421523: train_loss 4.6807 
2025-04-15 18:36:30.437151: val_loss 2.6087 
2025-04-15 18:36:30.452776: PSNR 19.1143 
2025-04-15 18:36:30.468404: Epoch time: 1371.45 s 
2025-04-15 18:36:31.696162:  
2025-04-15 18:36:31.711787: Epoch 155 
2025-04-15 18:36:31.727412: Current learning rate: 0.0001897 
2025-04-15 18:59:27.332147: train_loss 4.6531 
2025-04-15 18:59:27.347774: val_loss 2.6872 
2025-04-15 18:59:27.363398: PSNR 18.8173 
2025-04-15 18:59:27.379025: Epoch time: 1375.64 s 
2025-04-15 18:59:28.347775:  
2025-04-15 18:59:28.347775: Epoch 156 
2025-04-15 18:59:28.379019: Current learning rate: 0.0001896 
2025-04-15 19:22:25.361294: train_loss 4.7723 
2025-04-15 19:22:25.361294: val_loss 2.5951 
2025-04-15 19:22:25.376924: PSNR 18.467 
2025-04-15 19:22:25.399505: Epoch time: 1377.03 s 
2025-04-15 19:22:26.424508:  
2025-04-15 19:22:26.440131: Epoch 157 
2025-04-15 19:22:26.440131: Current learning rate: 0.0001894 
2025-04-15 19:45:11.832441: train_loss 4.7211 
2025-04-15 19:45:11.848066: val_loss 2.5693 
2025-04-15 19:45:11.863692: PSNR 19.5579 
2025-04-15 19:45:11.879315: Epoch time: 1365.42 s 
2025-04-15 19:45:12.739297:  
2025-04-15 19:45:12.770548: Epoch 158 
2025-04-15 19:45:12.786173: Current learning rate: 0.0001893 
2025-04-15 20:08:07.472717: train_loss 4.8544 
2025-04-15 20:08:07.503966: val_loss 2.4839 
2025-04-15 20:08:07.503966: PSNR 19.2673 
2025-04-15 20:08:07.519592: Epoch time: 1374.73 s 
2025-04-15 20:08:08.727890:  
2025-04-15 20:08:08.743516: Epoch 159 
2025-04-15 20:08:08.759139: Current learning rate: 0.0001891 
2025-04-15 20:30:59.358930: train_loss 4.5095 
2025-04-15 20:30:59.374554: val_loss 2.697 
2025-04-15 20:30:59.390183: PSNR 19.0492 
2025-04-15 20:30:59.405808: Epoch time: 1370.63 s 
2025-04-15 20:31:00.364305:  
2025-04-15 20:31:00.379929: Epoch 160 
2025-04-15 20:31:00.379929: Current learning rate: 0.000189 
2025-04-15 20:53:51.351736: train_loss 4.6468 
2025-04-15 20:53:51.367361: val_loss 2.6235 
2025-04-15 20:53:51.382987: PSNR 19.1391 
2025-04-15 20:53:51.398615: Epoch time: 1371.0 s 
2025-04-15 20:53:52.591689:  
2025-04-15 20:53:52.607313: Epoch 161 
2025-04-15 20:53:52.622939: Current learning rate: 0.0001889 
2025-04-15 21:16:42.675596: train_loss 4.7218 
2025-04-15 21:16:42.691221: val_loss 2.6911 
2025-04-15 21:16:42.706846: PSNR 18.4237 
2025-04-15 21:16:42.722471: Epoch time: 1370.08 s 
2025-04-15 21:16:43.654107:  
2025-04-15 21:16:43.669731: Epoch 162 
2025-04-15 21:16:43.685363: Current learning rate: 0.0001887 
2025-04-15 21:39:28.365606: train_loss 4.7391 
2025-04-15 21:39:28.396856: val_loss 2.5863 
2025-04-15 21:39:28.412482: PSNR 18.9559 
2025-04-15 21:39:28.428107: Epoch time: 1364.71 s 
2025-04-15 21:39:29.471349:  
2025-04-15 21:39:29.486975: Epoch 163 
2025-04-15 21:39:29.518226: Current learning rate: 0.0001886 
2025-04-15 22:02:21.703667: train_loss 4.779 
2025-04-15 22:02:21.719299: val_loss 2.491 
2025-04-15 22:02:21.750543: PSNR 19.2818 
2025-04-15 22:02:21.766168: Epoch time: 1372.23 s 
2025-04-15 22:02:22.776057:  
2025-04-15 22:02:22.791681: Epoch 164 
2025-04-15 22:02:22.822931: Current learning rate: 0.0001884 
2025-04-15 22:25:12.939346: train_loss 4.6621 
2025-04-15 22:25:12.970598: val_loss 2.6766 
2025-04-15 22:25:12.986224: PSNR 18.6889 
2025-04-15 22:25:13.001848: Epoch time: 1370.16 s 
2025-04-15 22:25:13.855295:  
2025-04-15 22:25:13.870918: Epoch 165 
2025-04-15 22:25:13.886547: Current learning rate: 0.0001883 
2025-04-15 22:47:59.884859: train_loss 4.7589 
2025-04-15 22:47:59.900494: val_loss 2.5862 
2025-04-15 22:47:59.916120: PSNR 18.8959 
2025-04-15 22:47:59.947371: Epoch time: 1366.05 s 
2025-04-15 22:48:00.932477:  
2025-04-15 22:48:00.948104: Epoch 166 
2025-04-15 22:48:00.963729: Current learning rate: 0.0001881 
2025-04-15 23:10:46.012460: train_loss 4.6316 
2025-04-15 23:10:46.028081: val_loss 2.8167 
2025-04-15 23:10:46.043709: PSNR 18.6034 
2025-04-15 23:10:46.059336: Epoch time: 1365.08 s 
2025-04-15 23:10:46.981103:  
2025-04-15 23:10:46.996728: Epoch 167 
2025-04-15 23:10:47.012353: Current learning rate: 0.000188 
2025-04-15 23:33:40.565361: train_loss 4.6253 
2025-04-15 23:33:40.580988: val_loss 2.6405 
2025-04-15 23:33:40.596610: PSNR 19.2197 
2025-04-15 23:33:40.612242: Epoch time: 1373.58 s 
2025-04-15 23:33:41.559773:  
2025-04-15 23:33:41.591025: Epoch 168 
2025-04-15 23:33:41.606652: Current learning rate: 0.0001878 
2025-04-15 23:56:25.327276: train_loss 4.6907 
2025-04-15 23:56:25.342901: val_loss 2.5936 
2025-04-15 23:56:25.342901: PSNR 19.2222 
2025-04-15 23:56:25.358528: Epoch time: 1363.77 s 
2025-04-15 23:56:26.179945:  
2025-04-15 23:56:26.195575: Epoch 169 
2025-04-15 23:56:26.211199: Current learning rate: 0.0001877 
2025-04-16 00:19:11.681921: train_loss 4.6869 
2025-04-16 00:19:11.713169: val_loss 2.5227 
2025-04-16 00:19:11.728796: PSNR 19.4981 
2025-04-16 00:19:11.744422: Epoch time: 1365.5 s 
2025-04-16 00:19:13.035323:  
2025-04-16 00:19:13.066574: Epoch 170 
2025-04-16 00:19:13.082197: Current learning rate: 0.0001875 
2025-04-16 00:42:02.982512: train_loss 4.6329 
2025-04-16 00:42:02.998139: val_loss 2.4887 
2025-04-16 00:42:03.029389: PSNR 19.2034 
2025-04-16 00:42:03.045011: Epoch time: 1369.96 s 
2025-04-16 00:42:04.133163:  
2025-04-16 00:42:04.148787: Epoch 171 
2025-04-16 00:42:04.165059: Current learning rate: 0.0001874 
2025-04-16 01:04:54.884644: train_loss 4.5588 
2025-04-16 01:04:54.915892: val_loss 2.7688 
2025-04-16 01:04:54.931521: PSNR 18.8353 
2025-04-16 01:04:54.947145: Epoch time: 1370.75 s 
2025-04-16 01:04:56.513796:  
2025-04-16 01:04:56.545052: Epoch 172 
2025-04-16 01:04:56.560673: Current learning rate: 0.0001872 
2025-04-16 01:27:41.943575: train_loss 4.6256 
2025-04-16 01:27:41.959203: val_loss 2.707 
2025-04-16 01:27:41.974826: PSNR 19.3461 
2025-04-16 01:27:41.990451: Epoch time: 1365.43 s 
2025-04-16 01:27:43.043250:  
2025-04-16 01:27:43.058877: Epoch 173 
2025-04-16 01:27:43.074504: Current learning rate: 0.0001871 
2025-04-16 01:50:39.694038: train_loss 4.6532 
2025-04-16 01:50:39.725287: val_loss 2.6507 
2025-04-16 01:50:39.740914: PSNR 18.8264 
2025-04-16 01:50:39.756539: Epoch time: 1376.65 s 
2025-04-16 01:50:41.031244:  
2025-04-16 01:50:41.046868: Epoch 174 
2025-04-16 01:50:41.078120: Current learning rate: 0.0001869 
2025-04-16 02:13:36.176024: train_loss 4.5334 
2025-04-16 02:13:36.207270: val_loss 2.8659 
2025-04-16 02:13:36.222893: PSNR 18.493 
2025-04-16 02:13:36.254145: Epoch time: 1375.14 s 
2025-04-16 02:13:37.383271:  
2025-04-16 02:13:37.398896: Epoch 175 
2025-04-16 02:13:37.414524: Current learning rate: 0.0001867 
2025-04-16 02:36:30.600660: train_loss 4.6264 
2025-04-16 02:36:30.625341: val_loss 2.6718 
2025-04-16 02:36:30.640978: PSNR 19.4508 
2025-04-16 02:36:30.656602: Epoch time: 1373.22 s 
2025-04-16 02:36:31.820718:  
2025-04-16 02:36:31.851959: Epoch 176 
2025-04-16 02:36:31.851959: Current learning rate: 0.0001866 
2025-04-16 02:59:27.368753: train_loss 4.7649 
2025-04-16 02:59:27.384377: val_loss 2.4805 
2025-04-16 02:59:27.400004: PSNR 19.1686 
2025-04-16 02:59:27.415630: Epoch time: 1375.55 s 
2025-04-16 02:59:28.510708:  
2025-04-16 02:59:28.526333: Epoch 177 
2025-04-16 02:59:28.526333: Current learning rate: 0.0001864 
2025-04-16 03:22:18.499831: train_loss 4.656 
2025-04-16 03:22:18.515448: val_loss 2.6103 
2025-04-16 03:22:18.531077: PSNR 19.5526 
2025-04-16 03:22:18.546701: Epoch time: 1370.0 s 
2025-04-16 03:22:19.446310:  
2025-04-16 03:22:19.461938: Epoch 178 
2025-04-16 03:22:19.477562: Current learning rate: 0.0001863 
2025-04-16 03:45:13.717811: train_loss 4.5915 
2025-04-16 03:45:13.753174: val_loss 2.7788 
2025-04-16 03:45:13.784427: PSNR 18.6908 
2025-04-16 03:45:13.800052: Epoch time: 1374.29 s 
2025-04-16 03:45:15.044602:  
2025-04-16 03:45:15.060225: Epoch 179 
2025-04-16 03:45:15.075852: Current learning rate: 0.0001861 
2025-04-16 04:08:00.034707: train_loss 4.602 
2025-04-16 04:08:00.050341: val_loss 2.462 
2025-04-16 04:08:00.065967: PSNR 19.8651 
2025-04-16 04:08:00.065967: Epoch time: 1364.99 s 
2025-04-16 04:08:01.181785:  
2025-04-16 04:08:01.197411: Epoch 180 
2025-04-16 04:08:01.213039: Current learning rate: 0.0001859 
2025-04-16 04:30:48.614235: train_loss 4.6438 
2025-04-16 04:30:48.629862: val_loss 2.5682 
2025-04-16 04:30:48.661109: PSNR 19.2585 
2025-04-16 04:30:48.661109: Epoch time: 1367.43 s 
2025-04-16 04:30:50.190507:  
2025-04-16 04:30:50.206134: Epoch 181 
2025-04-16 04:30:50.206134: Current learning rate: 0.0001858 
2025-04-16 04:53:34.417265: train_loss 4.6427 
2025-04-16 04:53:34.432891: val_loss 2.5644 
2025-04-16 04:53:34.464142: PSNR 18.81 
2025-04-16 04:53:34.464142: Epoch time: 1364.23 s 
2025-04-16 04:53:35.545160:  
2025-04-16 04:53:35.547211: Epoch 182 
2025-04-16 04:53:35.562847: Current learning rate: 0.0001856 
2025-04-16 05:16:20.469389: train_loss 4.5802 
2025-04-16 05:16:20.485016: val_loss 2.5267 
2025-04-16 05:16:20.485016: PSNR 19.6054 
2025-04-16 05:16:20.500645: Epoch time: 1364.93 s 
2025-04-16 05:16:21.724594:  
2025-04-16 05:16:21.740220: Epoch 183 
2025-04-16 05:16:21.755844: Current learning rate: 0.0001855 
2025-04-16 05:39:19.418330: train_loss 4.6074 
2025-04-16 05:39:19.449582: val_loss 2.6831 
2025-04-16 05:39:19.465206: PSNR 18.9487 
2025-04-16 05:39:19.480831: Epoch time: 1377.69 s 
2025-04-16 05:39:20.865715:  
2025-04-16 05:39:20.881338: Epoch 184 
2025-04-16 05:39:20.912593: Current learning rate: 0.0001853 
2025-04-16 06:02:06.348982: train_loss 4.547 
2025-04-16 06:02:06.364610: val_loss 2.7458 
2025-04-16 06:02:06.380236: PSNR 19.265 
2025-04-16 06:02:06.395860: Epoch time: 1365.48 s 
2025-04-16 06:02:07.429188:  
2025-04-16 06:02:07.444817: Epoch 185 
2025-04-16 06:02:07.460441: Current learning rate: 0.0001851 
2025-04-16 06:24:51.424700: train_loss 4.6432 
2025-04-16 06:24:51.455950: val_loss 2.6982 
2025-04-16 06:24:51.471575: PSNR 18.9773 
2025-04-16 06:24:51.502825: Epoch time: 1364.0 s 
2025-04-16 06:24:52.669645:  
2025-04-16 06:24:52.700895: Epoch 186 
2025-04-16 06:24:52.716519: Current learning rate: 0.000185 
2025-04-16 06:47:47.086413: train_loss 4.579 
2025-04-16 06:47:47.102037: val_loss 2.4391 
2025-04-16 06:47:47.117665: PSNR 19.2196 
2025-04-16 06:47:47.117665: Epoch time: 1374.42 s 
2025-04-16 06:47:47.970388:  
2025-04-16 06:47:47.986010: Epoch 187 
2025-04-16 06:47:48.001642: Current learning rate: 0.0001848 
2025-04-16 07:10:43.687430: train_loss 4.6377 
2025-04-16 07:10:43.703054: val_loss 2.616 
2025-04-16 07:10:43.718682: PSNR 19.0799 
2025-04-16 07:10:43.734309: Epoch time: 1375.73 s 
2025-04-16 07:10:44.567041:  
2025-04-16 07:10:44.598289: Epoch 188 
2025-04-16 07:10:44.613917: Current learning rate: 0.0001846 
2025-04-16 07:33:29.440862: train_loss 4.5326 
2025-04-16 07:33:29.472112: val_loss 2.6212 
2025-04-16 07:33:29.472112: PSNR 19.4396 
2025-04-16 07:33:29.487740: Epoch time: 1364.87 s 
2025-04-16 07:33:30.715531:  
2025-04-16 07:33:30.731159: Epoch 189 
2025-04-16 07:33:30.746790: Current learning rate: 0.0001845 
2025-04-16 07:56:26.469565: train_loss 4.6382 
2025-04-16 07:56:26.485188: val_loss 2.6187 
2025-04-16 07:56:26.500813: PSNR 18.9589 
2025-04-16 07:56:26.516440: Epoch time: 1375.75 s 
2025-04-16 07:56:27.536612:  
2025-04-16 07:56:27.552233: Epoch 190 
2025-04-16 07:56:27.567858: Current learning rate: 0.0001843 
2025-04-16 08:19:15.854548: train_loss 4.5996 
2025-04-16 08:19:15.885800: val_loss 2.4588 
2025-04-16 08:19:15.885800: PSNR 19.6066 
2025-04-16 08:19:15.901428: Epoch time: 1368.32 s 
2025-04-16 08:19:17.269836:  
2025-04-16 08:19:17.285459: Epoch 191 
2025-04-16 08:19:17.301088: Current learning rate: 0.0001841 
2025-04-16 08:42:08.730014: train_loss 4.6946 
2025-04-16 08:42:08.761263: val_loss 2.6188 
2025-04-16 08:42:08.761263: PSNR 18.8274 
2025-04-16 08:42:08.776890: Epoch time: 1371.48 s 
2025-04-16 08:42:10.206501:  
2025-04-16 08:42:10.222119: Epoch 192 
2025-04-16 08:42:10.253368: Current learning rate: 0.0001839 
2025-04-16 09:05:04.634087: train_loss 4.6356 
2025-04-16 09:05:04.641656: val_loss 2.6963 
2025-04-16 09:05:04.657290: PSNR 19.3298 
2025-04-16 09:05:04.672914: Epoch time: 1374.43 s 
2025-04-16 09:05:05.737432:  
2025-04-16 09:05:05.753054: Epoch 193 
2025-04-16 09:05:05.784305: Current learning rate: 0.0001838 
2025-04-16 09:28:00.854205: train_loss 4.6589 
2025-04-16 09:28:00.869831: val_loss 2.5616 
2025-04-16 09:28:00.885454: PSNR 18.9851 
2025-04-16 09:28:00.916705: Epoch time: 1375.12 s 
2025-04-16 09:28:01.956933:  
2025-04-16 09:28:01.972558: Epoch 194 
2025-04-16 09:28:01.972558: Current learning rate: 0.0001836 
2025-04-16 09:50:56.500646: train_loss 4.6558 
2025-04-16 09:50:56.516270: val_loss 2.6151 
2025-04-16 09:50:56.531895: PSNR 19.0876 
2025-04-16 09:50:56.547520: Epoch time: 1374.54 s 
2025-04-16 09:50:57.373607:  
2025-04-16 09:50:57.404859: Epoch 195 
2025-04-16 09:50:57.404859: Current learning rate: 0.0001834 
2025-04-16 10:13:41.038683: train_loss 4.4373 
2025-04-16 10:13:41.054306: val_loss 2.7399 
2025-04-16 10:13:41.069933: PSNR 18.5573 
2025-04-16 10:13:41.085558: Epoch time: 1363.67 s 
2025-04-16 10:13:42.188502:  
2025-04-16 10:13:42.219751: Epoch 196 
2025-04-16 10:13:42.219751: Current learning rate: 0.0001833 
2025-04-16 10:36:36.529957: train_loss 4.628 
2025-04-16 10:36:36.545579: val_loss 2.7015 
2025-04-16 10:36:36.561203: PSNR 19.3038 
2025-04-16 10:36:36.576831: Epoch time: 1374.34 s 
2025-04-16 10:36:37.711224:  
2025-04-16 10:36:37.742476: Epoch 197 
2025-04-16 10:36:37.758103: Current learning rate: 0.0001831 
2025-04-16 10:59:31.338700: train_loss 4.6658 
2025-04-16 10:59:31.354325: val_loss 2.7151 
2025-04-16 10:59:31.354325: PSNR 18.5182 
2025-04-16 10:59:31.385577: Epoch time: 1373.63 s 
2025-04-16 10:59:32.272410:  
2025-04-16 10:59:32.303661: Epoch 198 
2025-04-16 10:59:32.319298: Current learning rate: 0.0001829 
2025-04-16 11:22:23.879622: train_loss 4.471 
2025-04-16 11:22:23.910874: val_loss 2.8086 
2025-04-16 11:22:23.910874: PSNR 18.2786 
2025-04-16 11:22:23.926496: Epoch time: 1371.61 s 
2025-04-16 11:22:25.013671:  
2025-04-16 11:22:25.029301: Epoch 199 
2025-04-16 11:22:25.044390: Current learning rate: 0.0001827 
2025-04-16 11:45:21.840159: train_loss 4.618 
2025-04-16 11:45:21.871644: val_loss 2.8086 
2025-04-16 11:45:21.887271: PSNR 18.6353 
2025-04-16 11:45:21.902894: Epoch time: 1376.84 s 
2025-04-16 11:45:28.526549:  
2025-04-16 11:45:28.542181: Epoch 200 
2025-04-16 11:45:28.573431: Current learning rate: 0.0001825 
2025-04-16 12:08:21.910957: train_loss 4.4762 
2025-04-16 12:08:21.926583: val_loss 2.4865 
2025-04-16 12:08:21.957833: PSNR 19.9761 
2025-04-16 12:08:21.957833: Epoch time: 1373.39 s 
2025-04-16 12:08:23.083447:  
2025-04-16 12:08:23.099067: Epoch 201 
2025-04-16 12:08:23.114694: Current learning rate: 0.0001824 
2025-04-16 12:31:17.408084: train_loss 4.6316 
2025-04-16 12:31:17.439334: val_loss 2.4917 
2025-04-16 12:31:17.470586: PSNR 19.0704 
2025-04-16 12:31:17.486212: Epoch time: 1374.32 s 
2025-04-16 12:31:19.145878:  
2025-04-16 12:31:19.145878: Epoch 202 
2025-04-16 12:31:19.161502: Current learning rate: 0.0001822 
2025-04-16 12:54:09.174447: train_loss 4.4167 
2025-04-16 12:54:09.190073: val_loss 2.6207 
2025-04-16 12:54:09.221317: PSNR 18.7636 
2025-04-16 12:54:09.236943: Epoch time: 1370.04 s 
2025-04-16 12:54:10.418204:  
2025-04-16 12:54:10.449453: Epoch 203 
2025-04-16 12:54:10.465081: Current learning rate: 0.000182 
2025-04-16 13:17:04.981191: train_loss 4.5899 
2025-04-16 13:17:05.012445: val_loss 2.7363 
2025-04-16 13:17:05.028068: PSNR 18.4959 
2025-04-16 13:17:05.043692: Epoch time: 1374.56 s 
2025-04-16 13:17:06.053687:  
2025-04-16 13:17:06.069310: Epoch 204 
2025-04-16 13:17:06.084935: Current learning rate: 0.0001818 
2025-04-16 13:40:01.723511: train_loss 4.5878 
2025-04-16 13:40:01.739139: val_loss 2.6291 
2025-04-16 13:40:01.754764: PSNR 18.6997 
2025-04-16 13:40:01.770389: Epoch time: 1375.67 s 
2025-04-16 13:40:03.070632:  
2025-04-16 13:40:03.086258: Epoch 205 
2025-04-16 13:40:03.086258: Current learning rate: 0.0001816 
