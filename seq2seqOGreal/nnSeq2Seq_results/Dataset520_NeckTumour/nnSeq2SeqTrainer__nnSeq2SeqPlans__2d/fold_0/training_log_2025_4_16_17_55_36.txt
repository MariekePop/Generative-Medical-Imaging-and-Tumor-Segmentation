
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-16 17:55:51.214244: unpacking dataset... 
2025-04-16 17:56:04.339221: unpacking done... 
2025-04-16 17:56:04.351227: do_dummy_2d_data_aug: False 
2025-04-16 17:56:04.363220: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-16 17:56:04.381220: The split file contains 5 splits. 
2025-04-16 17:56:04.390220: Desired fold for training: 0 
2025-04-16 17:56:04.399220: This split has 4 training and 1 validation cases. 
2025-04-16 17:56:04.907732: Unable to plot network architecture: 
2025-04-16 17:56:04.919252: No module named 'hiddenlayer' 
2025-04-16 17:56:04.980767:  
2025-04-16 17:56:04.994771: Epoch 200 
2025-04-16 17:56:05.005770: Current learning rate: 0.0001825 
2025-04-16 18:03:50.553203: train_loss 3.2015 
2025-04-16 18:03:50.586207: val_loss 2.1552 
2025-04-16 18:03:50.601201: PSNR 19.6072 
2025-04-16 18:03:50.611201: Epoch time: 465.58 s 
2025-04-16 18:03:51.876779:  
2025-04-16 18:03:51.891777: Epoch 201 
2025-04-16 18:03:51.903777: Current learning rate: 0.0001824 
2025-04-16 18:12:45.837563: train_loss 4.2096 
2025-04-16 18:12:45.854562: val_loss 2.8438 
2025-04-16 18:12:45.869574: PSNR 18.4169 
2025-04-16 18:12:45.882562: Epoch time: 533.96 s 
2025-04-16 18:12:46.657563:  
2025-04-16 18:12:46.672561: Epoch 202 
2025-04-16 18:12:46.685563: Current learning rate: 0.0001822 
2025-04-16 18:21:49.716294: train_loss 4.5542 
2025-04-16 18:21:49.746296: val_loss 2.5567 
2025-04-16 18:21:49.769294: PSNR 19.1347 
2025-04-16 18:21:49.786294: Epoch time: 543.06 s 
2025-04-16 18:21:50.925289:  
2025-04-16 18:21:50.948287: Epoch 203 
2025-04-16 18:21:50.970291: Current learning rate: 0.000182 
2025-04-16 18:30:39.655374: train_loss 4.6999 
2025-04-16 18:30:39.678374: val_loss 2.4778 
2025-04-16 18:30:39.698371: PSNR 20.0033 
2025-04-16 18:30:39.718373: Epoch time: 528.73 s 
2025-04-16 18:30:40.932401:  
2025-04-16 18:30:40.958410: Epoch 204 
2025-04-16 18:30:40.974403: Current learning rate: 0.0001818 
2025-04-16 18:39:33.521685: train_loss 4.5763 
2025-04-16 18:39:33.550688: val_loss 2.5496 
2025-04-16 18:39:33.572682: PSNR 19.006 
2025-04-16 18:39:33.591682: Epoch time: 532.6 s 
2025-04-16 18:39:34.786720:  
2025-04-16 18:39:34.809721: Epoch 205 
2025-04-16 18:39:34.829720: Current learning rate: 0.0001816 
2025-04-16 18:48:34.422425: train_loss 4.5115 
2025-04-16 18:48:34.441421: val_loss 2.6054 
2025-04-16 18:48:34.456422: PSNR 19.6012 
2025-04-16 18:48:34.467932: Epoch time: 539.64 s 
2025-04-16 18:48:35.220453:  
2025-04-16 18:48:35.237453: Epoch 206 
2025-04-16 18:48:35.253455: Current learning rate: 0.0001815 
2025-04-16 18:57:26.570193: train_loss 4.5678 
2025-04-16 18:57:26.591194: val_loss 2.5879 
2025-04-16 18:57:26.606192: PSNR 19.0809 
2025-04-16 18:57:26.627192: Epoch time: 531.35 s 
2025-04-16 18:57:27.388218:  
2025-04-16 18:57:27.405218: Epoch 207 
2025-04-16 18:57:27.423224: Current learning rate: 0.0001813 
2025-04-16 19:06:19.306123: train_loss 4.6724 
2025-04-16 19:06:19.347641: val_loss 2.323 
2025-04-16 19:06:19.369640: PSNR 20.4906 
2025-04-16 19:06:19.390643: Epoch time: 531.92 s 
2025-04-16 19:06:19.412640: Yayy! New best EMA pseudo PSNR: 19.2515 
2025-04-16 19:06:25.406873:  
2025-04-16 19:06:25.440911: Epoch 208 
2025-04-16 19:06:25.462911: Current learning rate: 0.0001811 
2025-04-16 19:15:17.555472: train_loss 4.5342 
2025-04-16 19:15:17.574470: val_loss 2.8077 
2025-04-16 19:15:17.587471: PSNR 19.0136 
2025-04-16 19:15:17.604468: Epoch time: 532.15 s 
2025-04-16 19:15:18.686211:  
2025-04-16 19:15:18.702210: Epoch 209 
2025-04-16 19:15:18.716211: Current learning rate: 0.0001809 
2025-04-16 19:24:15.844770: train_loss 4.3509 
2025-04-16 19:24:15.867772: val_loss 2.5982 
2025-04-16 19:24:15.882771: PSNR 19.2499 
2025-04-16 19:24:15.903770: Epoch time: 537.16 s 
2025-04-16 19:24:16.668793:  
2025-04-16 19:24:16.695827: Epoch 210 
2025-04-16 19:24:16.713829: Current learning rate: 0.0001807 
2025-04-16 19:33:04.215080: train_loss 4.4668 
2025-04-16 19:33:04.236084: val_loss 2.4252 
2025-04-16 19:33:04.255081: PSNR 19.1178 
2025-04-16 19:33:04.272084: Epoch time: 527.55 s 
2025-04-16 19:33:05.182755:  
2025-04-16 19:33:05.204756: Epoch 211 
2025-04-16 19:33:05.219759: Current learning rate: 0.0001805 
2025-04-16 19:41:52.843143: train_loss 4.3246 
2025-04-16 19:41:52.868142: val_loss 2.6538 
2025-04-16 19:41:52.882143: PSNR 19.1325 
2025-04-16 19:41:52.902141: Epoch time: 527.66 s 
2025-04-16 19:41:53.870143:  
2025-04-16 19:41:53.896144: Epoch 212 
2025-04-16 19:41:53.919145: Current learning rate: 0.0001804 
2025-04-16 19:50:50.714330: train_loss 4.4719 
2025-04-16 19:50:50.734328: val_loss 2.4973 
2025-04-16 19:50:50.747330: PSNR 19.6063 
2025-04-16 19:50:50.761330: Epoch time: 536.85 s 
2025-04-16 19:50:51.511849:  
2025-04-16 19:50:51.526844: Epoch 213 
2025-04-16 19:50:51.541844: Current learning rate: 0.0001802 
2025-04-16 19:59:42.590422: train_loss 4.3204 
2025-04-16 19:59:42.603422: val_loss 2.7203 
2025-04-16 19:59:42.618421: PSNR 19.0823 
2025-04-16 19:59:42.628419: Epoch time: 531.08 s 
2025-04-16 19:59:43.388937:  
2025-04-16 19:59:43.402936: Epoch 214 
2025-04-16 19:59:43.413939: Current learning rate: 0.00018 
2025-04-16 20:08:40.487782: train_loss 4.4147 
2025-04-16 20:08:40.502785: val_loss 2.6591 
2025-04-16 20:08:40.514786: PSNR 18.8444 
2025-04-16 20:08:40.528783: Epoch time: 537.1 s 
2025-04-16 20:08:41.278787:  
2025-04-16 20:08:41.291786: Epoch 215 
2025-04-16 20:08:41.306788: Current learning rate: 0.0001798 
2025-04-16 20:17:32.693333: train_loss 4.3532 
2025-04-16 20:17:32.713336: val_loss 2.6105 
2025-04-16 20:17:32.734334: PSNR 18.967 
2025-04-16 20:17:32.754332: Epoch time: 531.42 s 
2025-04-16 20:17:33.914397:  
2025-04-16 20:17:33.934397: Epoch 216 
2025-04-16 20:17:33.955396: Current learning rate: 0.0001796 
2025-04-16 20:26:31.608997: train_loss 4.2645 
2025-04-16 20:26:31.638994: val_loss 2.5534 
2025-04-16 20:26:31.659991: PSNR 19.6897 
2025-04-16 20:26:31.680990: Epoch time: 537.7 s 
2025-04-16 20:26:32.481993:  
2025-04-16 20:26:32.504992: Epoch 217 
2025-04-16 20:26:32.525992: Current learning rate: 0.0001794 
2025-04-16 20:35:30.522680: train_loss 4.4383 
2025-04-16 20:35:30.538684: val_loss 2.6613 
2025-04-16 20:35:30.553684: PSNR 18.6418 
2025-04-16 20:35:30.567683: Epoch time: 538.04 s 
2025-04-16 20:35:31.492714:  
2025-04-16 20:35:31.510715: Epoch 218 
2025-04-16 20:35:31.526718: Current learning rate: 0.0001792 
2025-04-16 20:44:22.155191: train_loss 4.3551 
2025-04-16 20:44:22.172193: val_loss 2.6214 
2025-04-16 20:44:22.184191: PSNR 19.4583 
2025-04-16 20:44:22.194193: Epoch time: 530.67 s 
2025-04-16 20:44:23.221702:  
2025-04-16 20:44:23.238707: Epoch 219 
2025-04-16 20:44:23.249705: Current learning rate: 0.000179 
2025-04-16 20:53:22.454604: train_loss 4.2598 
2025-04-16 20:53:22.466605: val_loss 2.6255 
2025-04-16 20:53:22.476608: PSNR 18.9758 
2025-04-16 20:53:22.487604: Epoch time: 539.24 s 
2025-04-16 20:53:23.434427:  
2025-04-16 20:53:23.448426: Epoch 220 
2025-04-16 20:53:23.461426: Current learning rate: 0.0001788 
2025-04-16 21:02:14.832777: train_loss 4.2604 
2025-04-16 21:02:14.858778: val_loss 2.6189 
2025-04-16 21:02:14.872775: PSNR 19.3686 
2025-04-16 21:02:14.883778: Epoch time: 531.4 s 
2025-04-16 21:02:15.827365:  
2025-04-16 21:02:15.840364: Epoch 221 
2025-04-16 21:02:15.850365: Current learning rate: 0.0001786 
2025-04-16 21:11:08.542877: train_loss 4.3931 
2025-04-16 21:11:08.557878: val_loss 2.6341 
2025-04-16 21:11:08.573877: PSNR 19.0459 
2025-04-16 21:11:08.588876: Epoch time: 532.72 s 
2025-04-16 21:11:09.506907:  
2025-04-16 21:11:09.524912: Epoch 222 
2025-04-16 21:11:09.534911: Current learning rate: 0.0001784 
2025-04-16 21:19:58.875971: train_loss 4.3593 
2025-04-16 21:19:58.890967: val_loss 2.5048 
2025-04-16 21:19:58.902968: PSNR 19.3605 
2025-04-16 21:19:58.916967: Epoch time: 529.37 s 
2025-04-16 21:19:59.837027:  
2025-04-16 21:19:59.853028: Epoch 223 
2025-04-16 21:19:59.868031: Current learning rate: 0.0001782 
2025-04-16 21:28:49.170298: train_loss 4.3473 
2025-04-16 21:28:49.186299: val_loss 2.6327 
2025-04-16 21:28:49.199303: PSNR 19.3908 
2025-04-16 21:28:49.213299: Epoch time: 529.34 s 
2025-04-16 21:28:50.398824:  
2025-04-16 21:28:50.413822: Epoch 224 
2025-04-16 21:28:50.426822: Current learning rate: 0.000178 
2025-04-16 21:37:42.103180: train_loss 4.3059 
2025-04-16 21:37:42.123181: val_loss 2.715 
2025-04-16 21:37:42.138182: PSNR 19.0795 
2025-04-16 21:37:42.149184: Epoch time: 531.71 s 
2025-04-16 21:37:43.123206:  
2025-04-16 21:37:43.135209: Epoch 225 
2025-04-16 21:37:43.151211: Current learning rate: 0.0001778 
2025-04-16 21:46:34.943110: train_loss 4.3126 
2025-04-16 21:46:34.969112: val_loss 2.6091 
2025-04-16 21:46:34.981112: PSNR 19.4257 
2025-04-16 21:46:34.992112: Epoch time: 531.82 s 
2025-04-16 21:46:36.099145:  
2025-04-16 21:46:36.114147: Epoch 226 
2025-04-16 21:46:36.127146: Current learning rate: 0.0001776 
2025-04-16 21:55:26.561312: train_loss 4.3275 
2025-04-16 21:55:26.589315: val_loss 2.7184 
2025-04-16 21:55:26.607313: PSNR 18.6752 
2025-04-16 21:55:26.623314: Epoch time: 530.47 s 
2025-04-16 21:55:27.601018:  
2025-04-16 21:55:27.623019: Epoch 227 
2025-04-16 21:55:27.638027: Current learning rate: 0.0001774 
2025-04-16 22:04:16.598036: train_loss 4.2885 
2025-04-16 22:04:16.615037: val_loss 2.5008 
2025-04-16 22:04:16.625033: PSNR 19.6095 
2025-04-16 22:04:16.640033: Epoch time: 529.0 s 
2025-04-16 22:04:17.815612:  
2025-04-16 22:04:17.836609: Epoch 228 
2025-04-16 22:04:17.852612: Current learning rate: 0.0001772 
2025-04-16 22:13:11.128539: train_loss 4.2961 
2025-04-16 22:13:11.151541: val_loss 2.6417 
2025-04-16 22:13:11.174541: PSNR 18.9711 
2025-04-16 22:13:11.196539: Epoch time: 533.32 s 
2025-04-16 22:13:12.171575:  
2025-04-16 22:13:12.194579: Epoch 229 
2025-04-16 22:13:12.216578: Current learning rate: 0.000177 
2025-04-16 22:22:01.411614: train_loss 4.3059 
2025-04-16 22:22:01.433614: val_loss 2.6045 
2025-04-16 22:22:01.456615: PSNR 18.8307 
2025-04-16 22:22:01.477613: Epoch time: 529.25 s 
2025-04-16 22:22:02.451644:  
2025-04-16 22:22:02.469648: Epoch 230 
2025-04-16 22:22:02.488648: Current learning rate: 0.0001768 
2025-04-16 22:30:48.802488: train_loss 4.2986 
2025-04-16 22:30:48.818478: val_loss 2.4692 
2025-04-16 22:30:48.830480: PSNR 19.8632 
2025-04-16 22:30:48.843481: Epoch time: 526.36 s 
2025-04-16 22:30:49.869511:  
2025-04-16 22:30:49.881516: Epoch 231 
2025-04-16 22:30:49.891514: Current learning rate: 0.0001766 
2025-04-16 22:39:42.192516: train_loss 4.3947 
2025-04-16 22:39:42.223516: val_loss 2.6449 
2025-04-16 22:39:42.238517: PSNR 19.1245 
2025-04-16 22:39:42.249516: Epoch time: 532.33 s 
2025-04-16 22:39:43.179491:  
2025-04-16 22:39:43.191490: Epoch 232 
2025-04-16 22:39:43.202487: Current learning rate: 0.0001764 
2025-04-16 22:48:34.862082: train_loss 4.1945 
2025-04-16 22:48:34.890083: val_loss 2.5979 
2025-04-16 22:48:34.911081: PSNR 19.5457 
2025-04-16 22:48:34.926080: Epoch time: 531.69 s 
2025-04-16 22:48:35.703121:  
2025-04-16 22:48:35.723125: Epoch 233 
2025-04-16 22:48:35.746118: Current learning rate: 0.0001762 
2025-04-16 22:57:27.147258: train_loss 4.3657 
2025-04-16 22:57:27.165261: val_loss 2.6232 
2025-04-16 22:57:27.176260: PSNR 19.3326 
2025-04-16 22:57:27.189261: Epoch time: 531.45 s 
2025-04-16 22:57:27.201258: Yayy! New best EMA pseudo PSNR: 19.2559 
2025-04-16 22:57:32.827059:  
2025-04-16 22:57:32.838059: Epoch 234 
2025-04-16 22:57:32.852058: Current learning rate: 0.000176 
2025-04-16 23:06:30.761202: train_loss 4.3247 
2025-04-16 23:06:30.787203: val_loss 2.6989 
2025-04-16 23:06:30.807206: PSNR 18.6599 
2025-04-16 23:06:30.828234: Epoch time: 537.94 s 
2025-04-16 23:06:32.263264:  
2025-04-16 23:06:32.290269: Epoch 235 
2025-04-16 23:06:32.305264: Current learning rate: 0.0001758 
2025-04-16 23:15:30.115003: train_loss 4.2785 
2025-04-16 23:15:30.137014: val_loss 2.337 
2025-04-16 23:15:30.159005: PSNR 20.495 
2025-04-16 23:15:30.184005: Epoch time: 537.86 s 
2025-04-16 23:15:30.202007: Yayy! New best EMA pseudo PSNR: 19.3262 
2025-04-16 23:15:36.362720:  
2025-04-16 23:15:36.384720: Epoch 236 
2025-04-16 23:15:36.402717: Current learning rate: 0.0001756 
2025-04-16 23:24:27.590719: train_loss 4.2572 
2025-04-16 23:24:27.612711: val_loss 2.4927 
2025-04-16 23:24:27.633711: PSNR 19.1848 
2025-04-16 23:24:27.647709: Epoch time: 531.23 s 
2025-04-16 23:24:28.608181:  
2025-04-16 23:24:28.624179: Epoch 237 
2025-04-16 23:24:28.643187: Current learning rate: 0.0001754 
2025-04-16 23:33:19.326931: train_loss 4.3728 
2025-04-16 23:33:19.341934: val_loss 2.6654 
2025-04-16 23:33:19.362935: PSNR 18.8073 
2025-04-16 23:33:19.378934: Epoch time: 530.72 s 
2025-04-16 23:33:20.287452:  
2025-04-16 23:33:20.303456: Epoch 238 
2025-04-16 23:33:20.319453: Current learning rate: 0.0001752 
2025-04-16 23:42:18.259401: train_loss 4.288 
2025-04-16 23:42:18.289399: val_loss 2.4711 
2025-04-16 23:42:18.311401: PSNR 19.684 
2025-04-16 23:42:18.332400: Epoch time: 537.98 s 
2025-04-16 23:42:19.305429:  
2025-04-16 23:42:19.327432: Epoch 239 
2025-04-16 23:42:19.345433: Current learning rate: 0.000175 
2025-04-16 23:51:10.109394: train_loss 4.3968 
2025-04-16 23:51:10.156432: val_loss 2.3946 
2025-04-16 23:51:10.177431: PSNR 19.8134 
2025-04-16 23:51:10.198431: Epoch time: 530.81 s 
2025-04-16 23:51:10.219429: Yayy! New best EMA pseudo PSNR: 19.3548 
2025-04-16 23:51:16.034201:  
2025-04-16 23:51:16.063200: Epoch 240 
2025-04-16 23:51:16.081200: Current learning rate: 0.0001748 
2025-04-17 00:00:06.449407: train_loss 4.231 
2025-04-17 00:00:06.473408: val_loss 2.5634 
2025-04-17 00:00:06.488409: PSNR 19.5776 
2025-04-17 00:00:06.510408: Epoch time: 530.42 s 
2025-04-17 00:00:06.524405: Yayy! New best EMA pseudo PSNR: 19.377 
2025-04-17 00:00:11.921979:  
2025-04-17 00:00:11.955496: Epoch 241 
2025-04-17 00:00:11.970496: Current learning rate: 0.0001746 
2025-04-17 00:09:03.453853: train_loss 4.2772 
2025-04-17 00:09:03.483854: val_loss 2.5896 
2025-04-17 00:09:03.504856: PSNR 19.3859 
2025-04-17 00:09:03.525856: Epoch time: 531.54 s 
2025-04-17 00:09:03.546854: Yayy! New best EMA pseudo PSNR: 19.3779 
2025-04-17 00:09:11.250641:  
2025-04-17 00:09:11.266649: Epoch 242 
2025-04-17 00:09:11.281644: Current learning rate: 0.0001744 
2025-04-17 00:18:08.738529: train_loss 4.3988 
2025-04-17 00:18:08.755532: val_loss 2.7833 
2025-04-17 00:18:08.770531: PSNR 18.0032 
2025-04-17 00:18:08.784528: Epoch time: 537.49 s 
2025-04-17 00:18:09.570171:  
2025-04-17 00:18:09.587175: Epoch 243 
2025-04-17 00:18:09.601210: Current learning rate: 0.0001742 
2025-04-17 00:26:57.425053: train_loss 4.1453 
2025-04-17 00:26:57.444054: val_loss 2.5235 
2025-04-17 00:26:57.460055: PSNR 19.6038 
2025-04-17 00:26:57.475057: Epoch time: 527.86 s 
2025-04-17 00:26:58.387056:  
2025-04-17 00:26:58.403058: Epoch 244 
2025-04-17 00:26:58.418058: Current learning rate: 0.0001739 
2025-04-17 00:35:48.496585: train_loss 4.4063 
2025-04-17 00:35:48.516586: val_loss 2.6274 
2025-04-17 00:35:48.527585: PSNR 18.8145 
2025-04-17 00:35:48.542587: Epoch time: 530.11 s 
2025-04-17 00:35:49.513622:  
2025-04-17 00:35:49.533619: Epoch 245 
2025-04-17 00:35:49.547618: Current learning rate: 0.0001737 
2025-04-17 00:44:38.833015: train_loss 4.2481 
2025-04-17 00:44:38.848017: val_loss 2.7125 
2025-04-17 00:44:38.867014: PSNR 18.2801 
2025-04-17 00:44:38.879015: Epoch time: 529.32 s 
2025-04-17 00:44:39.670075:  
2025-04-17 00:44:39.685079: Epoch 246 
2025-04-17 00:44:39.695076: Current learning rate: 0.0001735 
2025-04-17 00:53:31.615365: train_loss 4.2551 
2025-04-17 00:53:31.633366: val_loss 2.6504 
2025-04-17 00:53:31.646366: PSNR 19.071 
2025-04-17 00:53:31.660366: Epoch time: 531.95 s 
2025-04-17 00:53:32.447396:  
2025-04-17 00:53:32.460401: Epoch 247 
2025-04-17 00:53:32.475401: Current learning rate: 0.0001733 
2025-04-17 01:02:24.877734: train_loss 4.2724 
2025-04-17 01:02:24.897738: val_loss 2.7937 
2025-04-17 01:02:24.913738: PSNR 18.8482 
2025-04-17 01:02:24.925738: Epoch time: 532.43 s 
2025-04-17 01:02:26.057766:  
2025-04-17 01:02:26.071767: Epoch 248 
2025-04-17 01:02:26.085765: Current learning rate: 0.0001731 
2025-04-17 01:11:14.174519: train_loss 4.2324 
2025-04-17 01:11:14.190521: val_loss 2.797 
2025-04-17 01:11:14.204524: PSNR 18.7518 
2025-04-17 01:11:14.215521: Epoch time: 528.12 s 
2025-04-17 01:11:15.129553:  
2025-04-17 01:11:15.150553: Epoch 249 
2025-04-17 01:11:15.160554: Current learning rate: 0.0001729 
2025-04-17 01:20:04.204946: train_loss 4.3552 
2025-04-17 01:20:04.219947: val_loss 2.3402 
2025-04-17 01:20:04.231945: PSNR 18.925 
2025-04-17 01:20:04.242947: Epoch time: 529.08 s 
2025-04-17 01:20:11.577776:  
2025-04-17 01:20:11.594773: Epoch 250 
2025-04-17 01:20:11.606772: Current learning rate: 0.0001727 
2025-04-17 01:29:01.916891: train_loss 4.1453 
2025-04-17 01:29:01.935890: val_loss 2.5548 
2025-04-17 01:29:01.950891: PSNR 19.498 
2025-04-17 01:29:01.961890: Epoch time: 530.34 s 
2025-04-17 01:29:02.907677:  
2025-04-17 01:29:02.928678: Epoch 251 
2025-04-17 01:29:02.941679: Current learning rate: 0.0001724 
2025-04-17 01:37:54.692027: train_loss 4.2457 
2025-04-17 01:37:54.717027: val_loss 2.6986 
2025-04-17 01:37:54.730030: PSNR 18.7126 
2025-04-17 01:37:54.741030: Epoch time: 531.79 s 
2025-04-17 01:37:55.694545:  
2025-04-17 01:37:55.709547: Epoch 252 
2025-04-17 01:37:55.720549: Current learning rate: 0.0001722 
2025-04-17 01:46:46.849836: train_loss 4.2267 
2025-04-17 01:46:46.863838: val_loss 2.4958 
2025-04-17 01:46:46.876838: PSNR 20.0153 
2025-04-17 01:46:46.886840: Epoch time: 531.16 s 
2025-04-17 01:46:47.856400:  
2025-04-17 01:46:47.871400: Epoch 253 
2025-04-17 01:46:47.882400: Current learning rate: 0.000172 
2025-04-17 01:55:35.519403: train_loss 4.3188 
2025-04-17 01:55:35.551403: val_loss 2.3509 
2025-04-17 01:55:35.571402: PSNR 20.2539 
2025-04-17 01:55:35.587404: Epoch time: 527.67 s 
2025-04-17 01:55:36.528756:  
2025-04-17 01:55:36.543759: Epoch 254 
2025-04-17 01:55:36.552761: Current learning rate: 0.0001718 
2025-04-17 02:04:27.340149: train_loss 4.2562 
2025-04-17 02:04:27.367150: val_loss 2.6004 
2025-04-17 02:04:27.384150: PSNR 19.5657 
2025-04-17 02:04:27.395150: Epoch time: 530.82 s 
2025-04-17 02:04:28.366188:  
2025-04-17 02:04:28.377189: Epoch 255 
2025-04-17 02:04:28.389190: Current learning rate: 0.0001716 
2025-04-17 02:13:26.996860: train_loss 4.2622 
2025-04-17 02:13:27.013859: val_loss 2.6517 
2025-04-17 02:13:27.023863: PSNR 19.5843 
2025-04-17 02:13:27.035861: Epoch time: 538.63 s 
2025-04-17 02:13:27.785929:  
2025-04-17 02:13:27.798931: Epoch 256 
2025-04-17 02:13:27.808930: Current learning rate: 0.0001713 
2025-04-17 02:22:16.365787: train_loss 4.3154 
2025-04-17 02:22:16.395788: val_loss 2.4898 
2025-04-17 02:22:16.408789: PSNR 19.7166 
2025-04-17 02:22:16.429790: Epoch time: 528.58 s 
2025-04-17 02:22:17.219344:  
2025-04-17 02:22:17.230342: Epoch 257 
2025-04-17 02:22:17.241345: Current learning rate: 0.0001711 
2025-04-17 02:31:15.922891: train_loss 4.1821 
2025-04-17 02:31:15.936888: val_loss 2.5045 
2025-04-17 02:31:15.950891: PSNR 19.9829 
2025-04-17 02:31:15.960889: Epoch time: 538.71 s 
2025-04-17 02:31:15.973889: Yayy! New best EMA pseudo PSNR: 19.4245 
2025-04-17 02:31:23.694554:  
2025-04-17 02:31:23.707556: Epoch 258 
2025-04-17 02:31:23.719557: Current learning rate: 0.0001709 
2025-04-17 02:40:14.774117: train_loss 4.3487 
2025-04-17 02:40:14.788119: val_loss 2.5907 
2025-04-17 02:40:14.810119: PSNR 18.8182 
2025-04-17 02:40:14.828119: Epoch time: 531.08 s 
2025-04-17 02:40:15.833122:  
2025-04-17 02:40:15.870153: Epoch 259 
2025-04-17 02:40:15.884155: Current learning rate: 0.0001707 
2025-04-17 02:49:05.270326: train_loss 4.1777 
2025-04-17 02:49:05.298328: val_loss 2.7222 
2025-04-17 02:49:05.314327: PSNR 19.029 
2025-04-17 02:49:05.325327: Epoch time: 529.44 s 
2025-04-17 02:49:06.306331:  
2025-04-17 02:49:06.321331: Epoch 260 
2025-04-17 02:49:06.332331: Current learning rate: 0.0001704 
2025-04-17 02:57:55.124167: train_loss 4.2147 
2025-04-17 02:57:55.147170: val_loss 2.8111 
2025-04-17 02:57:55.166169: PSNR 18.7128 
2025-04-17 02:57:55.178172: Epoch time: 528.82 s 
2025-04-17 02:57:56.336855:  
2025-04-17 02:57:56.348855: Epoch 261 
2025-04-17 02:57:56.363859: Current learning rate: 0.0001702 
2025-04-17 03:06:53.955948: train_loss 4.325 
2025-04-17 03:06:53.970956: val_loss 2.4814 
2025-04-17 03:06:53.980951: PSNR 19.6867 
2025-04-17 03:06:53.990950: Epoch time: 537.62 s 
2025-04-17 03:06:54.752470:  
2025-04-17 03:06:54.767472: Epoch 262 
2025-04-17 03:06:54.781474: Current learning rate: 0.00017 
2025-04-17 03:15:50.819554: train_loss 4.1761 
2025-04-17 03:15:50.845550: val_loss 2.5982 
2025-04-17 03:15:50.858552: PSNR 19.2969 
2025-04-17 03:15:50.879550: Epoch time: 536.07 s 
2025-04-17 03:15:52.076644:  
2025-04-17 03:15:52.092641: Epoch 263 
2025-04-17 03:15:52.106641: Current learning rate: 0.0001698 
2025-04-17 03:24:39.165215: train_loss 4.2091 
2025-04-17 03:24:39.180210: val_loss 2.7225 
2025-04-17 03:24:39.193207: PSNR 19.787 
2025-04-17 03:24:39.207211: Epoch time: 527.09 s 
2025-04-17 03:24:39.976817:  
2025-04-17 03:24:39.993818: Epoch 264 
2025-04-17 03:24:40.008820: Current learning rate: 0.0001695 
2025-04-17 03:33:27.444735: train_loss 4.3137 
2025-04-17 03:33:27.462738: val_loss 2.5989 
2025-04-17 03:33:27.475738: PSNR 19.0628 
2025-04-17 03:33:27.485736: Epoch time: 527.47 s 
2025-04-17 03:33:28.255769:  
2025-04-17 03:33:28.271769: Epoch 265 
2025-04-17 03:33:28.287769: Current learning rate: 0.0001693 
2025-04-17 03:42:15.693383: train_loss 4.1424 
2025-04-17 03:42:15.712387: val_loss 2.6962 
2025-04-17 03:42:15.723384: PSNR 19.4621 
2025-04-17 03:42:15.736385: Epoch time: 527.44 s 
2025-04-17 03:42:16.531428:  
2025-04-17 03:42:16.549457: Epoch 266 
2025-04-17 03:42:16.563456: Current learning rate: 0.0001691 
2025-04-17 03:51:06.754067: train_loss 4.2643 
2025-04-17 03:51:06.782070: val_loss 2.5163 
2025-04-17 03:51:06.800069: PSNR 19.4597 
2025-04-17 03:51:06.816069: Epoch time: 530.23 s 
2025-04-17 03:51:07.687096:  
2025-04-17 03:51:07.703111: Epoch 267 
2025-04-17 03:51:07.716106: Current learning rate: 0.0001689 
2025-04-17 03:59:56.469546: train_loss 4.2536 
2025-04-17 03:59:56.490552: val_loss 2.5153 
2025-04-17 03:59:56.511546: PSNR 19.7113 
2025-04-17 03:59:56.523546: Epoch time: 528.79 s 
2025-04-17 03:59:57.505087:  
2025-04-17 03:59:57.517095: Epoch 268 
2025-04-17 03:59:57.528098: Current learning rate: 0.0001686 
2025-04-17 04:08:44.128430: train_loss 4.1553 
2025-04-17 04:08:44.147434: val_loss 2.6451 
2025-04-17 04:08:44.158431: PSNR 19.7106 
2025-04-17 04:08:44.179433: Epoch time: 526.63 s 
2025-04-17 04:08:45.188007:  
2025-04-17 04:08:45.203007: Epoch 269 
2025-04-17 04:08:45.221008: Current learning rate: 0.0001684 
2025-04-17 04:17:42.196221: train_loss 4.1621 
2025-04-17 04:17:42.213222: val_loss 2.583 
2025-04-17 04:17:42.227219: PSNR 19.4579 
2025-04-17 04:17:42.240219: Epoch time: 537.01 s 
2025-04-17 04:17:42.250224: Yayy! New best EMA pseudo PSNR: 19.4246 
2025-04-17 04:17:47.947619:  
2025-04-17 04:17:47.963622: Epoch 270 
2025-04-17 04:17:47.978618: Current learning rate: 0.0001682 
2025-04-17 04:26:38.425604: train_loss 4.2721 
2025-04-17 04:26:38.443604: val_loss 2.5962 
2025-04-17 04:26:38.458602: PSNR 19.4404 
2025-04-17 04:26:38.472602: Epoch time: 530.48 s 
2025-04-17 04:26:38.486603: Yayy! New best EMA pseudo PSNR: 19.4261 
2025-04-17 04:26:44.309456:  
2025-04-17 04:26:44.325460: Epoch 271 
2025-04-17 04:26:44.340453: Current learning rate: 0.0001679 
2025-04-17 04:35:33.168111: train_loss 4.2009 
2025-04-17 04:35:33.183120: val_loss 2.5946 
2025-04-17 04:35:33.197111: PSNR 19.3777 
2025-04-17 04:35:33.207111: Epoch time: 528.86 s 
2025-04-17 04:35:34.000111:  
2025-04-17 04:35:34.012111: Epoch 272 
2025-04-17 04:35:34.022114: Current learning rate: 0.0001677 
2025-04-17 04:44:26.582339: train_loss 4.0905 
2025-04-17 04:44:26.609342: val_loss 2.5962 
2025-04-17 04:44:26.620347: PSNR 19.3798 
2025-04-17 04:44:26.631338: Epoch time: 532.59 s 
2025-04-17 04:44:27.732408:  
2025-04-17 04:44:27.745407: Epoch 273 
2025-04-17 04:44:27.757414: Current learning rate: 0.0001675 
2025-04-17 04:53:19.161073: train_loss 4.2836 
2025-04-17 04:53:19.195072: val_loss 2.8348 
2025-04-17 04:53:19.215072: PSNR 18.4816 
2025-04-17 04:53:19.236076: Epoch time: 531.43 s 
2025-04-17 04:53:20.340505:  
2025-04-17 04:53:20.356507: Epoch 274 
2025-04-17 04:53:20.376503: Current learning rate: 0.0001672 
2025-04-17 05:02:12.785952: train_loss 4.1606 
2025-04-17 05:02:12.808950: val_loss 2.658 
2025-04-17 05:02:12.828951: PSNR 19.2931 
2025-04-17 05:02:12.848952: Epoch time: 532.45 s 
2025-04-17 05:02:13.881509:  
2025-04-17 05:02:13.904512: Epoch 275 
2025-04-17 05:02:13.920509: Current learning rate: 0.000167 
2025-04-17 05:11:05.836521: train_loss 4.2516 
2025-04-17 05:11:05.849520: val_loss 2.5478 
2025-04-17 05:11:05.859522: PSNR 19.4237 
2025-04-17 05:11:05.872519: Epoch time: 531.96 s 
2025-04-17 05:11:06.855751:  
2025-04-17 05:11:06.873753: Epoch 276 
2025-04-17 05:11:06.888750: Current learning rate: 0.0001668 
2025-04-17 05:19:56.116504: train_loss 4.1948 
2025-04-17 05:19:56.138505: val_loss 2.2621 
2025-04-17 05:19:56.158504: PSNR 20.3947 
2025-04-17 05:19:56.178505: Epoch time: 529.26 s 
2025-04-17 05:19:56.197515: Yayy! New best EMA pseudo PSNR: 19.4372 
2025-04-17 05:20:01.833389:  
2025-04-17 05:20:01.849393: Epoch 277 
2025-04-17 05:20:01.867393: Current learning rate: 0.0001665 
2025-04-17 05:29:00.524988: train_loss 4.2127 
2025-04-17 05:29:00.543988: val_loss 2.6379 
2025-04-17 05:29:00.560991: PSNR 19.5518 
2025-04-17 05:29:00.585989: Epoch time: 538.7 s 
2025-04-17 05:29:00.597989: Yayy! New best EMA pseudo PSNR: 19.4487 
2025-04-17 05:29:07.568825:  
2025-04-17 05:29:07.584827: Epoch 278 
2025-04-17 05:29:07.605827: Current learning rate: 0.0001663 
2025-04-17 05:37:56.312019: train_loss 4.2878 
2025-04-17 05:37:56.326017: val_loss 2.7269 
2025-04-17 05:37:56.340017: PSNR 18.7111 
2025-04-17 05:37:56.358020: Epoch time: 528.75 s 
2025-04-17 05:37:57.176053:  
2025-04-17 05:37:57.194057: Epoch 279 
2025-04-17 05:37:57.214061: Current learning rate: 0.0001661 
2025-04-17 05:46:47.551824: train_loss 4.2023 
2025-04-17 05:46:47.571826: val_loss 2.7031 
2025-04-17 05:46:47.585827: PSNR 19.0251 
2025-04-17 05:46:47.601827: Epoch time: 530.38 s 
2025-04-17 05:46:48.403828:  
2025-04-17 05:46:48.421826: Epoch 280 
2025-04-17 05:46:48.437829: Current learning rate: 0.0001658 
2025-04-17 05:55:38.547193: train_loss 4.1604 
2025-04-17 05:55:38.561193: val_loss 2.6366 
2025-04-17 05:55:38.585197: PSNR 19.1618 
2025-04-17 05:55:38.599192: Epoch time: 530.15 s 
2025-04-17 05:55:39.417684:  
2025-04-17 05:55:39.439691: Epoch 281 
2025-04-17 05:55:39.460686: Current learning rate: 0.0001656 
2025-04-17 06:04:30.291485: train_loss 4.1665 
2025-04-17 06:04:30.306478: val_loss 2.6455 
2025-04-17 06:04:30.320479: PSNR 19.1074 
2025-04-17 06:04:30.333482: Epoch time: 530.88 s 
2025-04-17 06:04:31.322564:  
2025-04-17 06:04:31.337563: Epoch 282 
2025-04-17 06:04:31.351565: Current learning rate: 0.0001654 
2025-04-17 06:13:28.271638: train_loss 4.2254 
2025-04-17 06:13:28.296927: val_loss 2.4438 
2025-04-17 06:13:28.310928: PSNR 19.6658 
2025-04-17 06:13:28.324928: Epoch time: 536.95 s 
2025-04-17 06:13:29.151130:  
2025-04-17 06:13:29.165129: Epoch 283 
2025-04-17 06:13:29.179131: Current learning rate: 0.0001651 
2025-04-17 06:22:23.891265: train_loss 4.101 
2025-04-17 06:22:23.908267: val_loss 2.6449 
2025-04-17 06:22:23.921265: PSNR 18.9058 
2025-04-17 06:22:23.931267: Epoch time: 534.74 s 
2025-04-17 06:22:24.882334:  
2025-04-17 06:22:24.897979: Epoch 284 
2025-04-17 06:22:24.913979: Current learning rate: 0.0001649 
2025-04-17 06:31:17.138046: train_loss 4.1678 
2025-04-17 06:31:17.163049: val_loss 2.5243 
2025-04-17 06:31:17.183047: PSNR 19.8296 
2025-04-17 06:31:17.206051: Epoch time: 532.26 s 
2025-04-17 06:31:18.290631:  
2025-04-17 06:31:18.314632: Epoch 285 
2025-04-17 06:31:18.325630: Current learning rate: 0.0001646 
2025-04-17 06:40:10.549481: train_loss 4.144 
2025-04-17 06:40:10.585484: val_loss 2.7255 
2025-04-17 06:40:10.601481: PSNR 19.1111 
2025-04-17 06:40:10.612484: Epoch time: 532.26 s 
2025-04-17 06:40:11.832512:  
2025-04-17 06:40:11.848511: Epoch 286 
2025-04-17 06:40:11.872543: Current learning rate: 0.0001644 
2025-04-17 06:49:09.025289: train_loss 4.1794 
2025-04-17 06:49:09.056317: val_loss 2.5205 
2025-04-17 06:49:09.079321: PSNR 19.4599 
2025-04-17 06:49:09.095320: Epoch time: 537.2 s 
2025-04-17 06:49:09.924356:  
2025-04-17 06:49:09.950357: Epoch 287 
2025-04-17 06:49:09.972353: Current learning rate: 0.0001641 
2025-04-17 06:57:58.205707: train_loss 4.2091 
2025-04-17 06:57:58.236706: val_loss 2.58 
2025-04-17 06:57:58.263705: PSNR 19.4232 
2025-04-17 06:57:58.287709: Epoch time: 528.28 s 
2025-04-17 06:57:59.240736:  
2025-04-17 06:57:59.253736: Epoch 288 
2025-04-17 06:57:59.266737: Current learning rate: 0.0001639 
2025-04-17 07:06:47.425083: train_loss 4.0559 
2025-04-17 07:06:47.448085: val_loss 2.6448 
2025-04-17 07:06:47.461083: PSNR 19.1431 
2025-04-17 07:06:47.473085: Epoch time: 528.19 s 
2025-04-17 07:06:48.483124:  
2025-04-17 07:06:48.507123: Epoch 289 
2025-04-17 07:06:48.523124: Current learning rate: 0.0001637 
2025-04-17 07:15:37.799090: train_loss 4.1003 
2025-04-17 07:15:37.830089: val_loss 2.582 
2025-04-17 07:15:37.845089: PSNR 19.7516 
2025-04-17 07:15:37.857095: Epoch time: 529.32 s 
2025-04-17 07:15:39.073638:  
2025-04-17 07:15:39.086639: Epoch 290 
2025-04-17 07:15:39.100635: Current learning rate: 0.0001634 
2025-04-17 07:24:27.256897: train_loss 4.2044 
2025-04-17 07:24:27.275899: val_loss 2.6599 
2025-04-17 07:24:27.289897: PSNR 18.6497 
2025-04-17 07:24:27.303895: Epoch time: 528.19 s 
2025-04-17 07:24:28.071512:  
2025-04-17 07:24:28.088514: Epoch 291 
2025-04-17 07:24:28.103510: Current learning rate: 0.0001632 
2025-04-17 07:33:24.898387: train_loss 4.1918 
2025-04-17 07:33:24.930388: val_loss 2.6423 
2025-04-17 07:33:24.946395: PSNR 19.0721 
2025-04-17 07:33:24.965389: Epoch time: 536.83 s 
2025-04-17 07:33:25.925425:  
2025-04-17 07:33:25.941424: Epoch 292 
2025-04-17 07:33:25.961428: Current learning rate: 0.0001629 
2025-04-17 07:42:23.630831: train_loss 4.1093 
2025-04-17 07:42:23.659871: val_loss 2.6061 
2025-04-17 07:42:23.673867: PSNR 19.4306 
2025-04-17 07:42:23.685872: Epoch time: 537.71 s 
2025-04-17 07:42:24.459907:  
2025-04-17 07:42:24.475913: Epoch 293 
2025-04-17 07:42:24.502948: Current learning rate: 0.0001627 
2025-04-17 07:51:14.812919: train_loss 4.1925 
2025-04-17 07:51:14.832919: val_loss 2.657 
2025-04-17 07:51:14.851927: PSNR 18.9316 
2025-04-17 07:51:14.867922: Epoch time: 530.36 s 
2025-04-17 07:51:15.665959:  
2025-04-17 07:51:15.686964: Epoch 294 
2025-04-17 07:51:15.704957: Current learning rate: 0.0001624 
2025-04-17 08:00:12.418729: train_loss 4.1479 
2025-04-17 08:00:12.444716: val_loss 2.7849 
2025-04-17 08:00:12.461720: PSNR 18.9491 
2025-04-17 08:00:12.484722: Epoch time: 536.76 s 
2025-04-17 08:00:13.280760:  
2025-04-17 08:00:13.304755: Epoch 295 
2025-04-17 08:00:13.326762: Current learning rate: 0.0001622 
2025-04-17 08:09:01.076656: train_loss 4.1245 
2025-04-17 08:09:01.093658: val_loss 2.4959 
2025-04-17 08:09:01.107657: PSNR 19.5337 
2025-04-17 08:09:01.119655: Epoch time: 527.8 s 
2025-04-17 08:09:01.916700:  
2025-04-17 08:09:01.928699: Epoch 296 
2025-04-17 08:09:01.939698: Current learning rate: 0.0001619 
2025-04-17 08:17:53.606371: train_loss 4.113 
2025-04-17 08:17:53.622378: val_loss 2.6326 
2025-04-17 08:17:53.633369: PSNR 19.3141 
2025-04-17 08:17:53.645369: Epoch time: 531.69 s 
2025-04-17 08:17:54.414561:  
2025-04-17 08:17:54.429563: Epoch 297 
2025-04-17 08:17:54.443562: Current learning rate: 0.0001617 
2025-04-17 08:26:51.700225: train_loss 4.1065 
2025-04-17 08:26:51.728229: val_loss 2.5547 
2025-04-17 08:26:51.750225: PSNR 19.6205 
2025-04-17 08:26:51.769226: Epoch time: 537.29 s 
2025-04-17 08:26:52.988740:  
2025-04-17 08:26:53.013743: Epoch 298 
2025-04-17 08:26:53.038739: Current learning rate: 0.0001614 
2025-04-17 08:35:41.016381: train_loss 4.0526 
2025-04-17 08:35:41.036376: val_loss 2.6681 
2025-04-17 08:35:41.048383: PSNR 19.634 
2025-04-17 08:35:41.063373: Epoch time: 528.03 s 
2025-04-17 08:35:41.831670:  
2025-04-17 08:35:41.857665: Epoch 299 
2025-04-17 08:35:41.881668: Current learning rate: 0.0001612 
2025-04-17 08:44:31.634881: train_loss 4.2805 
2025-04-17 08:44:31.659879: val_loss 2.8346 
2025-04-17 08:44:31.682882: PSNR 18.4812 
2025-04-17 08:44:31.699881: Epoch time: 529.81 s 
2025-04-17 08:44:36.999067:  
2025-04-17 08:44:37.029067: Epoch 300 
2025-04-17 08:44:37.056066: Current learning rate: 0.0001609 
2025-04-17 08:53:33.872107: train_loss 4.0921 
2025-04-17 08:53:34.026104: val_loss 2.7216 
2025-04-17 08:53:34.048106: PSNR 18.5798 
2025-04-17 08:53:34.064110: Epoch time: 536.88 s 
2025-04-17 08:53:35.081107:  
2025-04-17 08:53:35.101107: Epoch 301 
2025-04-17 08:53:35.118108: Current learning rate: 0.0001607 
2025-04-17 09:02:32.583865: train_loss 4.0965 
2025-04-17 09:02:32.615867: val_loss 2.5999 
2025-04-17 09:02:32.639866: PSNR 18.9584 
2025-04-17 09:02:32.655871: Epoch time: 537.51 s 
2025-04-17 09:02:33.461423:  
2025-04-17 09:02:33.487423: Epoch 302 
2025-04-17 09:02:33.511423: Current learning rate: 0.0001604 
2025-04-17 09:11:29.939394: train_loss 3.875 
2025-04-17 09:11:29.959394: val_loss 2.5837 
2025-04-17 09:11:29.978394: PSNR 19.4754 
2025-04-17 09:11:29.992393: Epoch time: 536.48 s 
2025-04-17 09:11:31.241529:  
2025-04-17 09:11:31.267831: Epoch 303 
2025-04-17 09:11:31.287641: Current learning rate: 0.0001602 
2025-04-17 09:20:20.934231: train_loss 3.8182 
2025-04-17 09:20:20.952235: val_loss 2.6588 
2025-04-17 09:20:20.968236: PSNR 19.7529 
2025-04-17 09:20:20.992231: Epoch time: 529.7 s 
2025-04-17 09:20:21.808235:  
2025-04-17 09:20:21.833237: Epoch 304 
2025-04-17 09:20:21.855244: Current learning rate: 0.0001599 
2025-04-17 09:29:08.737455: train_loss 3.984 
2025-04-17 09:29:08.756453: val_loss 2.6477 
2025-04-17 09:29:08.769454: PSNR 18.9416 
2025-04-17 09:29:08.781459: Epoch time: 526.93 s 
2025-04-17 09:29:09.683521:  
2025-04-17 09:29:09.707520: Epoch 305 
2025-04-17 09:29:09.732523: Current learning rate: 0.0001597 
2025-04-17 09:38:01.515584: train_loss 3.8831 
2025-04-17 09:38:01.541593: val_loss 2.4066 
2025-04-17 09:38:01.566584: PSNR 20.1478 
2025-04-17 09:38:01.582588: Epoch time: 531.84 s 
2025-04-17 09:38:02.380648:  
2025-04-17 09:38:02.403646: Epoch 306 
2025-04-17 09:38:02.424647: Current learning rate: 0.0001594 
2025-04-17 09:46:50.374976: train_loss 3.9747 
2025-04-17 09:46:50.402972: val_loss 2.3708 
2025-04-17 09:46:50.416972: PSNR 19.7255 
2025-04-17 09:46:50.428570: Epoch time: 528.0 s 
2025-04-17 09:46:51.416601:  
2025-04-17 09:46:51.430602: Epoch 307 
2025-04-17 09:46:51.445605: Current learning rate: 0.0001592 
2025-04-17 09:55:37.981983: train_loss 3.7635 
2025-04-17 09:55:38.009982: val_loss 2.64 
2025-04-17 09:55:38.024984: PSNR 19.4899 
2025-04-17 09:55:38.036982: Epoch time: 526.57 s 
2025-04-17 09:55:39.327014:  
2025-04-17 09:55:39.352020: Epoch 308 
2025-04-17 09:55:39.371018: Current learning rate: 0.0001589 
2025-04-17 10:04:26.871532: train_loss 3.876 
2025-04-17 10:04:26.888534: val_loss 2.4598 
2025-04-17 10:04:26.899535: PSNR 19.6796 
2025-04-17 10:04:26.915530: Epoch time: 527.55 s 
2025-04-17 10:04:27.727139:  
2025-04-17 10:04:27.741139: Epoch 309 
2025-04-17 10:04:27.757140: Current learning rate: 0.0001587 
2025-04-17 10:13:24.534920: train_loss 3.8366 
2025-04-17 10:13:24.554906: val_loss 2.794 
2025-04-17 10:13:24.567906: PSNR 19.1813 
2025-04-17 10:13:24.581909: Epoch time: 536.81 s 
2025-04-17 10:13:25.647735:  
2025-04-17 10:13:25.661729: Epoch 310 
2025-04-17 10:13:25.673730: Current learning rate: 0.0001584 
2025-04-17 10:22:15.434367: train_loss 3.8209 
2025-04-17 10:22:15.449368: val_loss 2.5503 
2025-04-17 10:22:15.462370: PSNR 19.7208 
2025-04-17 10:22:15.478369: Epoch time: 529.79 s 
2025-04-17 10:22:16.262886:  
2025-04-17 10:22:16.277885: Epoch 311 
2025-04-17 10:22:16.289886: Current learning rate: 0.0001582 
2025-04-17 10:31:03.793257: train_loss 3.8509 
2025-04-17 10:31:03.814259: val_loss 2.567 
2025-04-17 10:31:03.827258: PSNR 19.524 
2025-04-17 10:31:03.842256: Epoch time: 527.53 s 
2025-04-17 10:31:04.632258:  
2025-04-17 10:31:04.650259: Epoch 312 
2025-04-17 10:31:04.663258: Current learning rate: 0.0001579 
2025-04-17 10:39:56.761467: train_loss 3.8981 
2025-04-17 10:39:56.781465: val_loss 2.6935 
2025-04-17 10:39:56.796468: PSNR 19.2311 
2025-04-17 10:39:56.814468: Epoch time: 532.13 s 
2025-04-17 10:39:57.647887:  
2025-04-17 10:39:57.665888: Epoch 313 
2025-04-17 10:39:57.680890: Current learning rate: 0.0001576 
2025-04-17 10:48:54.302771: train_loss 3.8534 
2025-04-17 10:48:54.321768: val_loss 2.6161 
2025-04-17 10:48:54.336770: PSNR 19.2902 
2025-04-17 10:48:54.347777: Epoch time: 536.66 s 
2025-04-17 10:48:55.609793:  
2025-04-17 10:48:55.626793: Epoch 314 
2025-04-17 10:48:55.641793: Current learning rate: 0.0001574 
2025-04-17 10:57:46.031351: train_loss 3.7838 
2025-04-17 10:57:46.050351: val_loss 2.5456 
2025-04-17 10:57:46.065353: PSNR 19.5581 
2025-04-17 10:57:46.079353: Epoch time: 530.42 s 
2025-04-17 10:57:47.054868:  
2025-04-17 10:57:47.070870: Epoch 315 
2025-04-17 10:57:47.082869: Current learning rate: 0.0001571 
2025-04-17 11:06:42.654329: train_loss 3.846 
2025-04-17 11:07:14.571392: val_loss 2.6116 
2025-04-17 11:07:14.600765: PSNR 19.3055 
2025-04-17 11:07:14.616767: Epoch time: 535.6 s 
2025-04-17 11:07:15.919937:  
2025-04-17 11:07:15.938937: Epoch 316 
2025-04-17 11:07:15.966940: Current learning rate: 0.0001569 
