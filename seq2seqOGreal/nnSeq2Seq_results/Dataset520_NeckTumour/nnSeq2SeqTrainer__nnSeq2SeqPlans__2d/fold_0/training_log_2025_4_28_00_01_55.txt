
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-28 00:02:08.350466: unpacking dataset... 
2025-04-28 00:02:22.352140: unpacking done... 
2025-04-28 00:02:22.366427: do_dummy_2d_data_aug: False 
2025-04-28 00:02:22.379109: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-28 00:02:22.397487: The split file contains 5 splits. 
2025-04-28 00:02:22.415748: Desired fold for training: 0 
2025-04-28 00:02:22.430620: This split has 4 training and 1 validation cases. 
2025-04-28 00:02:23.464302: Unable to plot network architecture: 
2025-04-28 00:02:23.480301: No module named 'hiddenlayer' 
2025-04-28 00:02:23.562895:  
2025-04-28 00:02:23.578899: Epoch 450 
2025-04-28 00:02:23.598013: Current learning rate: 0.0001181 
2025-04-28 00:09:11.955287: train_loss 2.3122 
2025-04-28 00:09:11.969527: val_loss 2.1536 
2025-04-28 00:09:11.981526: PSNR 19.7319 
2025-04-28 00:09:11.995004: Epoch time: 408.39 s 
2025-04-28 00:09:13.048485:  
2025-04-28 00:09:13.070250: Epoch 451 
2025-04-28 00:09:13.082244: Current learning rate: 0.0001178 
2025-04-28 00:23:17.509748: train_loss 3.2592 
2025-04-28 00:23:17.525378: val_loss 2.702 
2025-04-28 00:23:17.556629: PSNR 19.0673 
2025-04-28 00:23:17.572252: Epoch time: 844.47 s 
2025-04-28 00:23:18.685794:  
2025-04-28 00:23:18.701421: Epoch 452 
2025-04-28 00:23:18.717044: Current learning rate: 0.0001175 
2025-04-28 00:46:19.172633: train_loss 3.5496 
2025-04-28 00:46:19.188260: val_loss 2.6365 
2025-04-28 00:46:19.203887: PSNR 19.6322 
2025-04-28 00:46:19.219512: Epoch time: 1380.5 s 
2025-04-28 00:46:20.151088:  
2025-04-28 00:46:20.182336: Epoch 453 
2025-04-28 00:46:20.197962: Current learning rate: 0.0001172 
2025-04-28 01:09:21.322991: train_loss 3.6093 
2025-04-28 01:09:21.354240: val_loss 2.669 
2025-04-28 01:09:21.354240: PSNR 19.3474 
2025-04-28 01:09:21.369867: Epoch time: 1381.17 s 
2025-04-28 01:09:22.244864:  
2025-04-28 01:09:22.278592: Epoch 454 
2025-04-28 01:09:22.285084: Current learning rate: 0.0001169 
2025-04-28 01:32:25.496687: train_loss 3.5589 
2025-04-28 01:32:25.527937: val_loss 2.7114 
2025-04-28 01:32:25.543563: PSNR 19.3756 
2025-04-28 01:32:25.559187: Epoch time: 1383.25 s 
2025-04-28 01:32:26.436365:  
2025-04-28 01:32:26.451989: Epoch 455 
2025-04-28 01:32:26.476418: Current learning rate: 0.0001165 
2025-04-28 01:55:26.610107: train_loss 3.6046 
2025-04-28 01:55:26.641358: val_loss 2.8424 
2025-04-28 01:55:26.656984: PSNR 18.709 
2025-04-28 01:55:26.672606: Epoch time: 1380.19 s 
2025-04-28 01:55:27.689657:  
2025-04-28 01:55:27.720911: Epoch 456 
2025-04-28 01:55:27.720911: Current learning rate: 0.0001162 
2025-04-28 02:18:23.389182: train_loss 3.6009 
2025-04-28 02:18:23.404805: val_loss 2.633 
2025-04-28 02:18:23.420433: PSNR 19.1349 
2025-04-28 02:18:23.451683: Epoch time: 1375.7 s 
2025-04-28 02:18:24.492653:  
2025-04-28 02:18:24.523903: Epoch 457 
2025-04-28 02:18:24.523903: Current learning rate: 0.0001159 
2025-04-28 02:41:11.636005: train_loss 3.561 
2025-04-28 02:41:11.651635: val_loss 2.7349 
2025-04-28 02:41:11.683435: PSNR 19.1218 
2025-04-28 02:41:11.699068: Epoch time: 1367.14 s 
2025-04-28 02:41:12.891103:  
2025-04-28 02:41:12.906725: Epoch 458 
2025-04-28 02:41:12.922359: Current learning rate: 0.0001156 
2025-04-28 03:04:12.729717: train_loss 3.521 
2025-04-28 03:04:12.745347: val_loss 2.8436 
2025-04-28 03:04:12.760969: PSNR 18.9482 
2025-04-28 03:04:12.776591: Epoch time: 1379.84 s 
2025-04-28 03:04:13.844080:  
2025-04-28 03:04:13.859705: Epoch 459 
2025-04-28 03:04:13.875329: Current learning rate: 0.0001153 
2025-04-28 03:27:13.788620: train_loss 3.6011 
2025-04-28 03:27:13.804245: val_loss 2.7936 
2025-04-28 03:27:13.819871: PSNR 18.7937 
2025-04-28 03:27:13.819871: Epoch time: 1379.94 s 
2025-04-28 03:27:14.673031:  
2025-04-28 03:27:14.704279: Epoch 460 
2025-04-28 03:27:14.704279: Current learning rate: 0.000115 
2025-04-28 03:50:18.173096: train_loss 3.5643 
2025-04-28 03:50:18.188718: val_loss 2.6483 
2025-04-28 03:50:18.204346: PSNR 19.8231 
2025-04-28 03:50:18.219969: Epoch time: 1383.5 s 
2025-04-28 03:50:19.166400:  
2025-04-28 03:50:19.195029: Epoch 461 
2025-04-28 03:50:19.195029: Current learning rate: 0.0001147 
2025-04-28 04:13:05.681550: train_loss 3.5176 
2025-04-28 04:13:05.712801: val_loss 2.6275 
2025-04-28 04:13:05.728427: PSNR 20.1384 
2025-04-28 04:13:05.728427: Epoch time: 1366.53 s 
2025-04-28 04:13:06.687080:  
2025-04-28 04:13:06.718335: Epoch 462 
2025-04-28 04:13:06.733957: Current learning rate: 0.0001144 
2025-04-28 04:36:07.832496: train_loss 3.5495 
2025-04-28 04:36:07.863746: val_loss 2.6855 
2025-04-28 04:36:07.863746: PSNR 19.4361 
2025-04-28 04:36:07.879370: Epoch time: 1381.15 s 
2025-04-28 04:36:08.726865:  
2025-04-28 04:36:08.732512: Epoch 463 
2025-04-28 04:36:08.763773: Current learning rate: 0.000114 
2025-04-28 04:58:55.525606: train_loss 3.5221 
2025-04-28 04:58:55.541238: val_loss 2.6923 
2025-04-28 04:58:55.556861: PSNR 19.3823 
2025-04-28 04:58:55.572486: Epoch time: 1366.82 s 
2025-04-28 04:58:56.577476:  
2025-04-28 04:58:56.593100: Epoch 464 
2025-04-28 04:58:56.608725: Current learning rate: 0.0001137 
2025-04-28 05:21:56.810465: train_loss 3.6128 
2025-04-28 05:21:56.841713: val_loss 2.5961 
2025-04-28 05:21:56.857343: PSNR 19.6839 
2025-04-28 05:21:56.857343: Epoch time: 1380.25 s 
2025-04-28 05:21:57.909442:  
2025-04-28 05:21:57.932361: Epoch 465 
2025-04-28 05:21:57.940799: Current learning rate: 0.0001134 
2025-04-28 05:44:51.256165: train_loss 3.548 
2025-04-28 05:44:51.287407: val_loss 2.9266 
2025-04-28 05:44:51.303034: PSNR 19.0006 
2025-04-28 05:44:51.303034: Epoch time: 1373.35 s 
2025-04-28 05:44:52.593480:  
2025-04-28 05:44:52.609114: Epoch 466 
2025-04-28 05:44:52.624744: Current learning rate: 0.0001131 
2025-04-28 06:07:45.869299: train_loss 3.5966 
2025-04-28 06:07:45.884924: val_loss 2.9185 
2025-04-28 06:07:45.916173: PSNR 18.8986 
2025-04-28 06:07:45.916173: Epoch time: 1373.28 s 
2025-04-28 06:07:46.858914:  
2025-04-28 06:07:46.874538: Epoch 467 
2025-04-28 06:07:46.905788: Current learning rate: 0.0001128 
2025-04-28 06:30:42.581857: train_loss 3.4018 
2025-04-28 06:30:42.597483: val_loss 2.6916 
2025-04-28 06:30:42.613108: PSNR 19.7836 
2025-04-28 06:30:42.628732: Epoch time: 1375.72 s 
2025-04-28 06:30:43.753732:  
2025-04-28 06:30:43.769362: Epoch 468 
2025-04-28 06:30:43.784985: Current learning rate: 0.0001125 
2025-04-28 06:53:48.461721: train_loss 3.5946 
2025-04-28 06:53:48.492971: val_loss 2.8378 
2025-04-28 06:53:48.508595: PSNR 18.6595 
2025-04-28 06:53:48.524222: Epoch time: 1384.71 s 
2025-04-28 06:53:49.654572:  
2025-04-28 06:53:49.670211: Epoch 469 
2025-04-28 06:53:49.670211: Current learning rate: 0.0001122 
2025-04-28 07:16:54.280715: train_loss 3.6668 
2025-04-28 07:16:54.311965: val_loss 2.3837 
2025-04-28 07:16:54.327589: PSNR 19.7619 
2025-04-28 07:16:54.343215: Epoch time: 1384.63 s 
2025-04-28 07:16:55.455231:  
2025-04-28 07:16:55.470857: Epoch 470 
2025-04-28 07:16:55.486480: Current learning rate: 0.0001119 
2025-04-28 07:39:55.692714: train_loss 3.3973 
2025-04-28 07:39:55.708339: val_loss 2.6246 
2025-04-28 07:39:55.723962: PSNR 19.5591 
2025-04-28 07:39:55.755214: Epoch time: 1380.24 s 
2025-04-28 07:39:56.894085:  
2025-04-28 07:39:56.909709: Epoch 471 
2025-04-28 07:39:56.940958: Current learning rate: 0.0001115 
2025-04-28 08:02:50.806204: train_loss 3.5949 
2025-04-28 08:02:50.821825: val_loss 2.5728 
2025-04-28 08:02:50.837448: PSNR 19.8177 
2025-04-28 08:02:50.853077: Epoch time: 1373.91 s 
2025-04-28 08:02:51.902445:  
2025-04-28 08:02:51.918071: Epoch 472 
2025-04-28 08:02:51.933694: Current learning rate: 0.0001112 
2025-04-28 08:25:44.763978: train_loss 3.566 
2025-04-28 08:25:44.795230: val_loss 2.5845 
2025-04-28 08:25:44.810855: PSNR 20.1707 
2025-04-28 08:25:44.826480: Epoch time: 1372.86 s 
2025-04-28 08:25:45.757453:  
2025-04-28 08:25:45.773087: Epoch 473 
2025-04-28 08:25:45.804335: Current learning rate: 0.0001109 
2025-04-28 08:48:32.827103: train_loss 3.6456 
2025-04-28 08:48:32.842728: val_loss 2.5975 
2025-04-28 08:48:32.858352: PSNR 19.3894 
2025-04-28 08:48:32.873977: Epoch time: 1367.07 s 
2025-04-28 08:48:33.957646:  
2025-04-28 08:48:33.957646: Epoch 474 
2025-04-28 08:48:33.988899: Current learning rate: 0.0001106 
2025-04-28 09:11:34.634331: train_loss 3.6655 
2025-04-28 09:11:34.649957: val_loss 2.6485 
2025-04-28 09:11:34.649957: PSNR 19.7498 
2025-04-28 09:11:34.665585: Epoch time: 1380.69 s 
2025-04-28 09:11:35.493709:  
2025-04-28 09:11:35.509334: Epoch 475 
2025-04-28 09:11:35.524960: Current learning rate: 0.0001103 
2025-04-28 09:34:37.818607: train_loss 3.6408 
2025-04-28 09:34:37.834235: val_loss 2.5084 
2025-04-28 09:34:37.849861: PSNR 19.7612 
2025-04-28 09:34:37.865484: Epoch time: 1382.34 s 
2025-04-28 09:34:37.881110: Yayy! New best EMA pseudo PSNR: 19.5259 
2025-04-28 09:34:43.490650:  
2025-04-28 09:34:43.494791: Epoch 476 
2025-04-28 09:34:43.526049: Current learning rate: 0.00011 
2025-04-28 09:57:46.170178: train_loss 3.5506 
2025-04-28 09:57:46.201430: val_loss 2.5799 
2025-04-28 09:57:46.217055: PSNR 19.3577 
2025-04-28 09:57:46.248304: Epoch time: 1382.68 s 
2025-04-28 09:57:47.242862:  
2025-04-28 09:57:47.258490: Epoch 477 
2025-04-28 09:57:47.274115: Current learning rate: 0.0001097 
2025-04-28 10:20:50.500703: train_loss 3.5991 
2025-04-28 10:20:50.547576: val_loss 2.5594 
2025-04-28 10:20:50.563203: PSNR 19.7012 
2025-04-28 10:20:50.578835: Epoch time: 1383.27 s 
2025-04-28 10:20:50.594455: Yayy! New best EMA pseudo PSNR: 19.5283 
2025-04-28 10:20:58.277888:  
2025-04-28 10:20:58.293514: Epoch 478 
2025-04-28 10:20:58.309141: Current learning rate: 0.0001093 
2025-04-28 10:44:02.206887: train_loss 3.5215 
2025-04-28 10:44:02.238139: val_loss 2.7864 
2025-04-28 10:44:02.253764: PSNR 19.569 
2025-04-28 10:44:02.269389: Epoch time: 1383.93 s 
2025-04-28 10:44:02.285013: Yayy! New best EMA pseudo PSNR: 19.5324 
2025-04-28 10:44:09.519262:  
2025-04-28 10:44:09.550512: Epoch 479 
2025-04-28 10:44:09.566139: Current learning rate: 0.000109 
2025-04-28 11:07:13.121485: train_loss 3.4892 
2025-04-28 11:07:13.121485: val_loss 2.5731 
2025-04-28 11:07:13.162841: PSNR 19.7217 
2025-04-28 11:07:13.178471: Epoch time: 1383.6 s 
2025-04-28 11:07:13.194092: Yayy! New best EMA pseudo PSNR: 19.5513 
2025-04-28 11:07:19.991151:  
2025-04-28 11:07:19.991151: Epoch 480 
2025-04-28 11:07:20.006774: Current learning rate: 0.0001087 
2025-04-28 11:30:04.977101: train_loss 3.6327 
2025-04-28 11:30:05.008352: val_loss 2.5735 
2025-04-28 11:30:05.023976: PSNR 19.8516 
2025-04-28 11:30:05.039601: Epoch time: 1365.0 s 
2025-04-28 11:30:05.055225: Yayy! New best EMA pseudo PSNR: 19.5813 
2025-04-28 11:30:11.972832:  
2025-04-28 11:30:11.988466: Epoch 481 
2025-04-28 11:30:11.988466: Current learning rate: 0.0001084 
2025-04-28 11:53:06.618135: train_loss 3.5161 
2025-04-28 11:53:06.633762: val_loss 2.695 
2025-04-28 11:53:06.649386: PSNR 19.3456 
2025-04-28 11:53:06.665011: Epoch time: 1374.65 s 
2025-04-28 11:53:07.780385:  
2025-04-28 11:53:07.796014: Epoch 482 
2025-04-28 11:53:07.811638: Current learning rate: 0.0001081 
2025-04-28 12:16:08.008353: train_loss 3.5096 
2025-04-28 12:16:08.039603: val_loss 2.6141 
2025-04-28 12:16:08.055228: PSNR 19.9299 
2025-04-28 12:16:08.070853: Epoch time: 1380.23 s 
2025-04-28 12:16:08.086477: Yayy! New best EMA pseudo PSNR: 19.595 
2025-04-28 12:16:14.022655:  
2025-04-28 12:16:14.038281: Epoch 483 
2025-04-28 12:16:14.053908: Current learning rate: 0.0001078 
2025-04-28 12:39:06.707048: train_loss 3.5923 
2025-04-28 12:39:06.738299: val_loss 2.5834 
2025-04-28 12:39:06.753924: PSNR 19.8315 
2025-04-28 12:39:06.769551: Epoch time: 1372.68 s 
2025-04-28 12:39:06.785175: Yayy! New best EMA pseudo PSNR: 19.6186 
2025-04-28 12:39:14.150439:  
2025-04-28 12:39:14.166065: Epoch 484 
2025-04-28 12:39:14.181696: Current learning rate: 0.0001075 
2025-04-28 13:02:10.760265: train_loss 3.5511 
2025-04-28 13:02:10.775893: val_loss 2.8405 
2025-04-28 13:02:10.775893: PSNR 19.7349 
2025-04-28 13:02:10.807151: Epoch time: 1376.63 s 
2025-04-28 13:02:10.807151: Yayy! New best EMA pseudo PSNR: 19.6303 
2025-04-28 13:02:16.618627:  
2025-04-28 13:02:16.649877: Epoch 485 
2025-04-28 13:02:16.649877: Current learning rate: 0.0001071 
2025-04-28 13:25:19.203896: train_loss 3.4933 
2025-04-28 13:25:19.219523: val_loss 2.8576 
2025-04-28 13:25:19.235152: PSNR 19.2683 
2025-04-28 13:25:19.251546: Epoch time: 1382.59 s 
2025-04-28 13:25:20.118936:  
2025-04-28 13:25:20.150185: Epoch 486 
2025-04-28 13:25:20.150185: Current learning rate: 0.0001068 
2025-04-28 13:48:05.668216: train_loss 3.5764 
2025-04-28 13:48:05.699467: val_loss 2.623 
2025-04-28 13:48:05.715092: PSNR 19.909 
2025-04-28 13:48:05.730718: Epoch time: 1365.55 s 
2025-04-28 13:48:06.730721:  
2025-04-28 13:48:06.746345: Epoch 487 
2025-04-28 13:48:06.761971: Current learning rate: 0.0001065 
2025-04-28 14:10:53.827651: train_loss 3.5058 
2025-04-28 14:10:53.874525: val_loss 2.7645 
2025-04-28 14:10:53.890157: PSNR 18.9372 
2025-04-28 14:10:53.905778: Epoch time: 1367.1 s 
2025-04-28 14:10:54.961793:  
2025-04-28 14:10:54.993041: Epoch 488 
2025-04-28 14:10:55.008668: Current learning rate: 0.0001062 
2025-04-28 14:33:47.552903: train_loss 3.4699 
2025-04-28 14:33:47.568526: val_loss 2.7854 
2025-04-28 14:33:47.584154: PSNR 18.9376 
2025-04-28 14:33:47.599780: Epoch time: 1372.59 s 
2025-04-28 14:33:48.687932:  
2025-04-28 14:33:48.703558: Epoch 489 
2025-04-28 14:33:48.719187: Current learning rate: 0.0001059 
2025-04-28 14:56:50.274234: train_loss 3.5155 
2025-04-28 14:56:50.305483: val_loss 2.655 
2025-04-28 14:56:50.321109: PSNR 19.7577 
2025-04-28 14:56:50.336734: Epoch time: 1381.6 s 
2025-04-28 14:56:51.367983:  
2025-04-28 14:56:51.383610: Epoch 490 
2025-04-28 14:56:51.399236: Current learning rate: 0.0001056 
2025-04-28 15:19:50.950349: train_loss 3.5975 
2025-04-28 15:19:50.982227: val_loss 2.6074 
2025-04-28 15:19:50.997860: PSNR 19.9222 
2025-04-28 15:19:51.013485: Epoch time: 1379.58 s 
2025-04-28 15:19:52.163153:  
2025-04-28 15:19:52.194404: Epoch 491 
2025-04-28 15:19:52.210037: Current learning rate: 0.0001053 
2025-04-28 15:42:53.078161: train_loss 3.4245 
2025-04-28 15:42:53.118503: val_loss 2.739 
2025-04-28 15:42:53.134137: PSNR 19.4381 
2025-04-28 15:42:53.149760: Epoch time: 1380.92 s 
2025-04-28 15:42:54.035273:  
2025-04-28 15:42:54.050898: Epoch 492 
2025-04-28 15:42:54.066523: Current learning rate: 0.0001049 
2025-04-28 16:05:40.977507: train_loss 3.5617 
2025-04-28 16:05:40.988881: val_loss 2.6861 
2025-04-28 16:05:41.020142: PSNR 19.2463 
2025-04-28 16:05:41.035775: Epoch time: 1366.94 s 
2025-04-28 16:05:41.927010:  
2025-04-28 16:05:41.942634: Epoch 493 
2025-04-28 16:05:41.958257: Current learning rate: 0.0001046 
2025-04-28 16:28:39.713927: train_loss 3.4805 
2025-04-28 16:28:39.745174: val_loss 2.6403 
2025-04-28 16:28:39.760801: PSNR 19.2402 
2025-04-28 16:28:39.776427: Epoch time: 1377.79 s 
2025-04-28 16:28:40.801386:  
2025-04-28 16:28:40.817014: Epoch 494 
2025-04-28 16:28:40.832640: Current learning rate: 0.0001043 
2025-04-28 16:51:42.771991: train_loss 3.5171 
2025-04-28 16:51:42.787618: val_loss 2.716 
2025-04-28 16:51:42.803241: PSNR 19.048 
2025-04-28 16:51:42.818868: Epoch time: 1381.97 s 
2025-04-28 16:51:43.640544:  
2025-04-28 16:51:43.671794: Epoch 495 
2025-04-28 16:51:43.671794: Current learning rate: 0.000104 
2025-04-28 17:14:30.309195: train_loss 3.4879 
2025-04-28 17:14:30.340446: val_loss 3.0163 
2025-04-28 17:14:30.356071: PSNR 18.6472 
2025-04-28 17:14:30.371695: Epoch time: 1366.67 s 
2025-04-28 17:14:31.492996:  
2025-04-28 17:14:31.492996: Epoch 496 
2025-04-28 17:14:31.508618: Current learning rate: 0.0001037 
2025-04-28 17:37:23.662820: train_loss 3.5222 
2025-04-28 17:37:23.678446: val_loss 2.6568 
2025-04-28 17:37:23.694068: PSNR 19.3877 
2025-04-28 17:37:23.709693: Epoch time: 1372.19 s 
2025-04-28 17:37:24.624767:  
2025-04-28 17:37:24.640387: Epoch 497 
2025-04-28 17:37:24.640387: Current learning rate: 0.0001034 
2025-04-28 18:00:12.558171: train_loss 3.5581 
2025-04-28 18:00:12.573795: val_loss 2.7056 
2025-04-28 18:00:12.573795: PSNR 19.4741 
2025-04-28 18:00:12.589423: Epoch time: 1367.95 s 
2025-04-28 18:00:13.551554:  
2025-04-28 18:00:13.567176: Epoch 498 
2025-04-28 18:00:13.582804: Current learning rate: 0.000103 
2025-04-28 18:23:16.642393: train_loss 3.4692 
2025-04-28 18:23:16.673647: val_loss 2.6497 
2025-04-28 18:23:16.689270: PSNR 20.5372 
2025-04-28 18:23:16.704895: Epoch time: 1383.09 s 
2025-04-28 18:23:17.907538:  
2025-04-28 18:23:17.923163: Epoch 499 
2025-04-28 18:23:17.938789: Current learning rate: 0.0001027 
2025-04-28 18:46:18.414457: train_loss 3.5354 
2025-04-28 18:46:18.430082: val_loss 2.4643 
2025-04-28 18:46:18.465400: PSNR 20.0791 
2025-04-28 18:46:18.465400: Epoch time: 1380.51 s 
2025-04-28 18:46:25.306087:  
2025-04-28 18:46:25.321711: Epoch 500 
2025-04-28 18:46:25.337339: Current learning rate: 0.0001024 
2025-04-28 19:09:21.403548: train_loss 3.5909 
2025-04-28 19:09:21.419172: val_loss 2.4782 
2025-04-28 19:09:21.450422: PSNR 20.1166 
2025-04-28 19:09:21.466046: Epoch time: 1376.11 s 
2025-04-28 19:09:22.391904:  
2025-04-28 19:09:22.391904: Epoch 501 
2025-04-28 19:09:22.423156: Current learning rate: 0.0001021 
2025-04-28 19:32:26.270799: train_loss 3.5026 
2025-04-28 19:32:26.270799: val_loss 2.9302 
2025-04-28 19:32:26.286425: PSNR 19.3084 
2025-04-28 19:32:26.302051: Epoch time: 1383.89 s 
2025-04-28 19:32:27.218230:  
2025-04-28 19:32:27.233853: Epoch 502 
2025-04-28 19:32:27.249489: Current learning rate: 0.0001018 
2025-04-28 19:55:29.972212: train_loss 3.3852 
2025-04-28 19:55:29.995238: val_loss 2.7493 
2025-04-28 19:55:30.010870: PSNR 19.1303 
2025-04-28 19:55:30.026495: Epoch time: 1382.75 s 
2025-04-28 19:55:31.029655:  
2025-04-28 19:55:31.045285: Epoch 503 
2025-04-28 19:55:31.060905: Current learning rate: 0.0001015 
2025-04-28 20:18:58.719166: train_loss 3.5358 
2025-04-28 21:13:37.899834: val_loss 2.6271 
2025-04-28 21:13:37.899834: PSNR 19.3564 
2025-04-28 21:13:37.939806: Epoch time: 1407.69 s 
2025-04-28 21:13:39.205483:  
2025-04-28 21:13:39.222584: Epoch 504 
2025-04-28 21:13:39.234589: Current learning rate: 0.0001012 
2025-04-28 21:36:37.603803: train_loss 3.4971 
2025-04-28 21:36:37.619428: val_loss 2.6708 
2025-04-28 21:36:37.635051: PSNR 19.4354 
2025-04-28 21:36:37.650680: Epoch time: 1378.4 s 
2025-04-28 21:36:38.572552:  
2025-04-28 21:36:38.572552: Epoch 505 
2025-04-28 21:36:38.588181: Current learning rate: 0.0001008 
2025-04-28 21:59:31.012616: train_loss 3.6209 
2025-04-28 21:59:31.028243: val_loss 2.7518 
2025-04-28 21:59:31.043871: PSNR 18.7864 
2025-04-28 21:59:31.059491: Epoch time: 1372.46 s 
2025-04-28 21:59:32.271548:  
2025-04-28 21:59:32.287170: Epoch 506 
2025-04-28 21:59:32.287170: Current learning rate: 0.0001005 
2025-04-28 22:22:32.792349: train_loss 3.4383 
2025-04-28 22:22:32.792349: val_loss 2.6526 
2025-04-28 22:22:32.807975: PSNR 19.775 
2025-04-28 22:22:32.823601: Epoch time: 1380.54 s 
2025-04-28 22:22:33.696933:  
2025-04-28 22:22:33.719560: Epoch 507 
2025-04-28 22:22:33.735195: Current learning rate: 0.0001002 
2025-04-28 22:45:25.245815: train_loss 3.4788 
2025-04-28 22:45:25.277065: val_loss 2.7926 
2025-04-28 22:45:25.277065: PSNR 19.147 
2025-04-28 22:45:25.292694: Epoch time: 1371.55 s 
2025-04-28 22:45:26.289087:  
2025-04-28 22:45:26.289087: Epoch 508 
2025-04-28 22:45:26.304718: Current learning rate: 9.99e-05 
2025-04-28 23:08:26.137096: train_loss 3.4685 
2025-04-28 23:08:26.168346: val_loss 2.7538 
2025-04-28 23:08:26.183975: PSNR 19.3902 
2025-04-28 23:08:26.204394: Epoch time: 1379.86 s 
2025-04-28 23:08:27.074009:  
2025-04-28 23:08:27.089639: Epoch 509 
2025-04-28 23:08:27.105261: Current learning rate: 9.96e-05 
2025-04-28 23:31:19.671033: train_loss 3.52 
2025-04-28 23:31:19.702282: val_loss 2.7383 
2025-04-28 23:31:19.702282: PSNR 19.2878 
2025-04-28 23:31:19.717908: Epoch time: 1372.6 s 
2025-04-28 23:31:20.586517:  
2025-04-28 23:31:20.602140: Epoch 510 
2025-04-28 23:31:20.617767: Current learning rate: 9.93e-05 
2025-04-28 23:54:22.665462: train_loss 3.4035 
2025-04-28 23:54:22.681090: val_loss 2.5249 
2025-04-28 23:54:22.696717: PSNR 20.336 
2025-04-28 23:54:22.712341: Epoch time: 1382.09 s 
2025-04-28 23:54:23.681489:  
2025-04-28 23:54:23.697119: Epoch 511 
2025-04-28 23:54:23.712739: Current learning rate: 9.89e-05 
2025-04-29 00:17:23.473256: train_loss 3.4517 
2025-04-29 00:17:23.488883: val_loss 2.5563 
2025-04-29 00:17:23.504510: PSNR 20.4269 
2025-04-29 00:17:23.504510: Epoch time: 1379.81 s 
2025-04-29 00:17:24.404932:  
2025-04-29 00:17:24.436183: Epoch 512 
2025-04-29 00:17:24.436183: Current learning rate: 9.86e-05 
2025-04-29 00:40:25.328078: train_loss 3.5154 
2025-04-29 00:40:25.343714: val_loss 2.5236 
2025-04-29 00:40:25.343714: PSNR 20.5516 
2025-04-29 00:40:25.370620: Epoch time: 1380.92 s 
2025-04-29 00:40:25.379986: Yayy! New best EMA pseudo PSNR: 19.697 
2025-04-29 00:40:31.477056:  
2025-04-29 00:40:31.492684: Epoch 513 
2025-04-29 00:40:31.508311: Current learning rate: 9.83e-05 
2025-04-29 01:03:34.662607: train_loss 3.4927 
2025-04-29 01:03:34.678231: val_loss 2.8659 
2025-04-29 01:03:34.693856: PSNR 19.5264 
2025-04-29 01:03:34.709482: Epoch time: 1383.2 s 
2025-04-29 01:03:35.781665:  
2025-04-29 01:03:35.799004: Epoch 514 
2025-04-29 01:03:35.807537: Current learning rate: 9.8e-05 
2025-04-29 01:26:23.048813: train_loss 3.3907 
2025-04-29 01:26:23.064436: val_loss 2.6984 
2025-04-29 01:26:23.095687: PSNR 19.8319 
2025-04-29 01:26:23.111310: Epoch time: 1367.28 s 
2025-04-29 01:26:24.133063:  
2025-04-29 01:26:24.148687: Epoch 515 
2025-04-29 01:26:24.172240: Current learning rate: 9.77e-05 
2025-04-29 01:49:28.849874: train_loss 3.4683 
2025-04-29 01:49:28.881124: val_loss 2.591 
2025-04-29 01:49:28.896761: PSNR 19.7 
2025-04-29 01:49:28.921586: Epoch time: 1384.72 s 
2025-04-29 01:49:29.988524:  
2025-04-29 01:49:30.004147: Epoch 516 
2025-04-29 01:49:30.019776: Current learning rate: 9.74e-05 
2025-04-29 02:12:15.559661: train_loss 3.5134 
2025-04-29 02:12:15.590909: val_loss 2.711 
2025-04-29 02:12:15.606538: PSNR 19.5702 
2025-04-29 02:12:15.622162: Epoch time: 1365.57 s 
2025-04-29 02:12:16.913445:  
2025-04-29 02:12:16.913445: Epoch 517 
2025-04-29 02:12:16.944695: Current learning rate: 9.71e-05 
2025-04-29 02:35:10.177630: train_loss 3.3885 
2025-04-29 02:35:10.193254: val_loss 2.6353 
2025-04-29 02:35:10.217805: PSNR 19.6453 
2025-04-29 02:35:10.233438: Epoch time: 1373.28 s 
2025-04-29 02:35:11.372376:  
2025-04-29 02:35:11.388002: Epoch 518 
2025-04-29 02:35:11.403625: Current learning rate: 9.67e-05 
2025-04-29 02:58:04.375514: train_loss 3.5551 
2025-04-29 02:58:04.406765: val_loss 2.6866 
2025-04-29 02:58:04.422394: PSNR 19.4257 
2025-04-29 02:58:04.438015: Epoch time: 1373.0 s 
2025-04-29 02:58:05.479579:  
2025-04-29 02:58:05.495205: Epoch 519 
2025-04-29 02:58:05.495205: Current learning rate: 9.64e-05 
2025-04-29 03:21:08.541677: train_loss 3.4897 
2025-04-29 03:21:08.557305: val_loss 2.849 
2025-04-29 03:21:08.588557: PSNR 19.1132 
2025-04-29 03:21:08.604179: Epoch time: 1383.08 s 
2025-04-29 03:21:09.848380:  
2025-04-29 03:21:09.864015: Epoch 520 
2025-04-29 03:21:09.879640: Current learning rate: 9.61e-05 
2025-04-29 03:44:03.546024: train_loss 3.4559 
2025-04-29 03:44:03.561651: val_loss 2.8871 
2025-04-29 03:44:03.577277: PSNR 19.3872 
2025-04-29 03:44:03.608524: Epoch time: 1373.7 s 
2025-04-29 03:44:04.675615:  
2025-04-29 03:44:04.691239: Epoch 521 
2025-04-29 03:44:04.706864: Current learning rate: 9.58e-05 
2025-04-29 04:06:59.106292: train_loss 3.5034 
2025-04-29 04:06:59.121919: val_loss 2.4464 
2025-04-29 04:06:59.137554: PSNR 20.4194 
2025-04-29 04:06:59.137554: Epoch time: 1374.45 s 
2025-04-29 04:06:59.918794:  
2025-04-29 04:06:59.934422: Epoch 522 
2025-04-29 04:06:59.934422: Current learning rate: 9.55e-05 
2025-04-29 04:29:53.989291: train_loss 3.5507 
2025-04-29 04:29:54.004918: val_loss 2.6468 
2025-04-29 04:29:54.004918: PSNR 19.261 
2025-04-29 04:29:54.020545: Epoch time: 1374.07 s 
2025-04-29 04:29:54.983287:  
2025-04-29 04:29:54.998913: Epoch 523 
2025-04-29 04:29:55.014538: Current learning rate: 9.52e-05 
2025-04-29 04:52:57.556173: train_loss 3.4578 
2025-04-29 04:52:57.571797: val_loss 2.8026 
2025-04-29 04:52:57.571797: PSNR 19.0567 
2025-04-29 04:52:57.587422: Epoch time: 1382.57 s 
2025-04-29 04:52:58.722475:  
2025-04-29 04:52:58.738104: Epoch 524 
2025-04-29 04:52:58.753726: Current learning rate: 9.48e-05 
2025-04-29 05:15:52.176939: train_loss 3.4258 
2025-04-29 05:15:52.192561: val_loss 2.7952 
2025-04-29 05:15:52.208191: PSNR 19.239 
2025-04-29 05:15:52.208191: Epoch time: 1373.45 s 
2025-04-29 05:15:53.292471:  
2025-04-29 05:15:53.308097: Epoch 525 
2025-04-29 05:15:53.323722: Current learning rate: 9.45e-05 
2025-04-29 05:38:54.671366: train_loss 3.4597 
2025-04-29 05:38:54.702615: val_loss 2.5596 
2025-04-29 05:38:54.718243: PSNR 19.8463 
2025-04-29 05:38:54.733868: Epoch time: 1381.38 s 
2025-04-29 05:38:55.707615:  
2025-04-29 05:38:55.723244: Epoch 526 
2025-04-29 05:38:55.738869: Current learning rate: 9.42e-05 
2025-04-29 06:01:41.721156: train_loss 3.4721 
2025-04-29 06:01:41.736781: val_loss 2.57 
2025-04-29 06:01:41.752409: PSNR 20.054 
2025-04-29 06:01:41.783657: Epoch time: 1366.01 s 
2025-04-29 06:01:42.864122:  
2025-04-29 06:01:42.879750: Epoch 527 
2025-04-29 06:01:42.895375: Current learning rate: 9.39e-05 
2025-04-29 06:24:37.295023: train_loss 3.4885 
2025-04-29 06:24:37.310649: val_loss 2.9049 
2025-04-29 06:24:37.326275: PSNR 19.12 
2025-04-29 06:24:37.341901: Epoch time: 1374.45 s 
2025-04-29 06:24:38.438073:  
2025-04-29 06:24:38.469320: Epoch 528 
2025-04-29 06:24:38.484947: Current learning rate: 9.36e-05 
2025-04-29 06:47:25.922745: train_loss 3.3823 
2025-04-29 06:47:25.938372: val_loss 2.7944 
2025-04-29 06:47:25.953996: PSNR 19.2884 
2025-04-29 06:47:25.969620: Epoch time: 1367.48 s 
2025-04-29 06:47:26.988137:  
2025-04-29 06:47:27.003759: Epoch 529 
2025-04-29 06:47:27.003759: Current learning rate: 9.33e-05 
2025-04-29 07:10:21.287080: train_loss 3.4624 
2025-04-29 07:10:21.324067: val_loss 2.8034 
2025-04-29 07:10:21.324067: PSNR 18.504 
2025-04-29 07:10:21.339699: Epoch time: 1374.31 s 
2025-04-29 07:10:22.183448:  
2025-04-29 07:10:22.214705: Epoch 530 
2025-04-29 07:10:22.214705: Current learning rate: 9.3e-05 
2025-04-29 07:33:22.376569: train_loss 3.4302 
2025-04-29 07:33:22.392194: val_loss 2.6922 
2025-04-29 07:33:22.407818: PSNR 19.7433 
2025-04-29 07:33:22.423443: Epoch time: 1380.19 s 
2025-04-29 07:33:23.354319:  
2025-04-29 07:33:23.354319: Epoch 531 
2025-04-29 07:33:23.369944: Current learning rate: 9.26e-05 
2025-04-29 07:56:16.707406: train_loss 3.4842 
2025-04-29 07:56:16.723032: val_loss 2.5126 
2025-04-29 07:56:16.738658: PSNR 20.0468 
2025-04-29 07:56:16.754289: Epoch time: 1373.37 s 
2025-04-29 07:56:17.806988:  
2025-04-29 07:56:17.823333: Epoch 532 
2025-04-29 07:56:17.823333: Current learning rate: 9.23e-05 
2025-04-29 08:19:04.420760: train_loss 3.5227 
2025-04-29 08:19:04.436388: val_loss 2.6024 
2025-04-29 08:19:04.452012: PSNR 19.4458 
2025-04-29 08:19:04.467639: Epoch time: 1366.61 s 
2025-04-29 08:19:05.436387:  
2025-04-29 08:19:05.452017: Epoch 533 
2025-04-29 08:19:05.467641: Current learning rate: 9.2e-05 
2025-04-29 08:41:51.671190: train_loss 3.4829 
2025-04-29 08:41:51.718066: val_loss 2.6587 
2025-04-29 08:41:51.733693: PSNR 19.9588 
2025-04-29 08:41:51.733693: Epoch time: 1366.23 s 
2025-04-29 08:41:52.962193:  
2025-04-29 08:41:52.977815: Epoch 534 
2025-04-29 08:41:52.993445: Current learning rate: 9.17e-05 
2025-04-29 09:04:39.615429: train_loss 3.4456 
2025-04-29 09:04:39.631053: val_loss 2.7386 
2025-04-29 09:04:39.646684: PSNR 19.3794 
2025-04-29 09:04:39.662310: Epoch time: 1366.65 s 
2025-04-29 09:04:40.817013:  
2025-04-29 09:04:40.832639: Epoch 535 
2025-04-29 09:04:40.832639: Current learning rate: 9.14e-05 
2025-04-29 09:27:35.744359: train_loss 3.4925 
2025-04-29 09:27:35.759988: val_loss 2.439 
2025-04-29 09:27:35.791237: PSNR 20.0829 
2025-04-29 09:27:35.791237: Epoch time: 1374.93 s 
2025-04-29 09:27:36.806310:  
2025-04-29 09:27:36.821935: Epoch 536 
2025-04-29 09:27:36.837563: Current learning rate: 9.11e-05 
2025-04-29 09:50:37.521030: train_loss 3.5189 
2025-04-29 09:50:37.552282: val_loss 2.9455 
2025-04-29 09:50:37.567907: PSNR 18.6012 
2025-04-29 09:50:37.599153: Epoch time: 1380.73 s 
2025-04-29 09:50:38.737699:  
2025-04-29 09:50:38.768946: Epoch 537 
2025-04-29 09:50:38.793674: Current learning rate: 9.08e-05 
2025-04-29 10:13:32.824573: train_loss 3.4604 
2025-04-29 10:13:32.840199: val_loss 2.7603 
2025-04-29 10:13:32.855827: PSNR 19.2537 
2025-04-29 10:13:32.855827: Epoch time: 1374.09 s 
2025-04-29 10:13:33.817977:  
2025-04-29 10:13:33.849225: Epoch 538 
2025-04-29 10:13:33.849225: Current learning rate: 9.04e-05 
2025-04-29 10:36:27.156844: train_loss 3.4289 
2025-04-29 10:36:27.172468: val_loss 2.7473 
2025-04-29 10:36:27.188096: PSNR 19.8293 
2025-04-29 10:36:27.203722: Epoch time: 1373.34 s 
2025-04-29 10:36:28.094349:  
2025-04-29 10:36:28.109971: Epoch 539 
2025-04-29 10:36:28.125605: Current learning rate: 9.01e-05 
2025-04-29 10:59:15.095110: train_loss 3.3802 
2025-04-29 10:59:15.135444: val_loss 2.8725 
2025-04-29 10:59:15.135952: PSNR 19.045 
2025-04-29 10:59:15.151587: Epoch time: 1367.0 s 
2025-04-29 10:59:16.319217:  
2025-04-29 10:59:16.319217: Epoch 540 
2025-04-29 10:59:16.350466: Current learning rate: 8.98e-05 
2025-04-29 11:22:20.146612: train_loss 3.4182 
2025-04-29 11:22:20.162238: val_loss 2.7896 
2025-04-29 11:22:20.177862: PSNR 19.0594 
2025-04-29 11:22:20.193486: Epoch time: 1383.84 s 
2025-04-29 11:22:21.239133:  
2025-04-29 11:22:21.254759: Epoch 541 
2025-04-29 11:22:21.254759: Current learning rate: 8.95e-05 
2025-04-29 11:45:21.198377: train_loss 3.4438 
2025-04-29 11:45:21.214004: val_loss 2.6977 
2025-04-29 11:45:21.229630: PSNR 19.7419 
2025-04-29 11:45:21.245259: Epoch time: 1379.97 s 
2025-04-29 11:45:22.104984:  
2025-04-29 11:45:22.136242: Epoch 542 
2025-04-29 11:45:22.136242: Current learning rate: 8.92e-05 
2025-04-29 12:08:10.859549: train_loss 3.4868 
2025-04-29 12:08:10.890805: val_loss 2.715 
2025-04-29 12:08:10.906427: PSNR 19.3665 
2025-04-29 12:08:10.922051: Epoch time: 1368.75 s 
2025-04-29 12:08:11.750176:  
2025-04-29 12:08:11.765800: Epoch 543 
2025-04-29 12:08:11.781427: Current learning rate: 8.89e-05 
2025-04-29 12:31:14.287825: train_loss 3.4957 
2025-04-29 12:31:14.303447: val_loss 2.5983 
2025-04-29 12:31:14.343690: PSNR 19.6708 
2025-04-29 12:31:14.359325: Epoch time: 1382.54 s 
2025-04-29 12:31:15.527670:  
2025-04-29 12:31:15.543291: Epoch 544 
2025-04-29 12:31:15.558919: Current learning rate: 8.86e-05 
2025-04-29 12:54:02.477898: train_loss 3.4116 
2025-04-29 12:54:02.493522: val_loss 2.5198 
2025-04-29 12:54:02.524775: PSNR 20.0802 
2025-04-29 12:54:02.540399: Epoch time: 1366.95 s 
2025-04-29 12:54:03.549850:  
2025-04-29 12:54:03.566062: Epoch 545 
2025-04-29 12:54:03.581697: Current learning rate: 8.82e-05 
2025-04-29 13:17:05.977262: train_loss 3.5008 
2025-04-29 13:17:06.008514: val_loss 2.7587 
2025-04-29 13:17:06.024158: PSNR 19.3623 
2025-04-29 13:17:06.039764: Epoch time: 1382.43 s 
2025-04-29 13:17:07.111401:  
2025-04-29 13:17:07.111401: Epoch 546 
2025-04-29 13:17:07.142650: Current learning rate: 8.79e-05 
2025-04-29 13:40:01.057240: train_loss 3.4809 
2025-04-29 13:40:01.072862: val_loss 2.6456 
2025-04-29 13:40:01.104115: PSNR 19.2114 
2025-04-29 13:40:01.104115: Epoch time: 1373.96 s 
2025-04-29 13:40:02.035027:  
2025-04-29 13:40:02.050654: Epoch 547 
2025-04-29 13:40:02.050654: Current learning rate: 8.76e-05 
2025-04-29 14:03:01.010035: train_loss 3.4603 
2025-04-29 14:03:01.025669: val_loss 2.6994 
2025-04-29 14:03:01.041287: PSNR 19.2337 
2025-04-29 14:03:01.056908: Epoch time: 1378.98 s 
2025-04-29 14:03:02.041287:  
2025-04-29 14:03:02.041287: Epoch 548 
2025-04-29 14:03:02.056913: Current learning rate: 8.73e-05 
2025-04-29 14:25:55.882062: train_loss 3.4298 
2025-04-29 14:25:55.897687: val_loss 2.4454 
2025-04-29 14:25:55.928937: PSNR 20.2185 
2025-04-29 14:25:55.944561: Epoch time: 1373.86 s 
2025-04-29 14:25:56.860586:  
2025-04-29 14:25:56.876210: Epoch 549 
2025-04-29 14:25:56.891839: Current learning rate: 8.7e-05 
2025-04-29 14:49:02.309824: train_loss 3.5139 
2025-04-29 14:49:02.325452: val_loss 2.5737 
2025-04-29 14:49:02.341079: PSNR 20.0022 
2025-04-29 14:49:02.372325: Epoch time: 1385.45 s 
2025-04-29 14:49:08.222769:  
2025-04-29 14:49:08.222769: Epoch 550 
2025-04-29 14:49:08.254020: Current learning rate: 8.67e-05 
2025-04-29 15:11:56.441230: train_loss 3.4384 
2025-04-29 15:11:56.472480: val_loss 2.6721 
2025-04-29 15:11:56.488106: PSNR 19.2865 
2025-04-29 15:11:56.503732: Epoch time: 1368.23 s 
2025-04-29 15:11:57.488108:  
2025-04-29 15:11:57.503731: Epoch 551 
2025-04-29 15:11:57.519356: Current learning rate: 8.64e-05 
2025-04-29 15:34:52.514573: train_loss 3.5506 
2025-04-29 15:34:52.530201: val_loss 2.6145 
2025-04-29 15:34:52.545826: PSNR 19.3842 
2025-04-29 15:34:52.561448: Epoch time: 1375.03 s 
2025-04-29 15:34:53.627513:  
2025-04-29 15:34:53.627513: Epoch 552 
2025-04-29 15:34:53.658765: Current learning rate: 8.61e-05 
2025-04-29 15:57:56.132182: train_loss 3.3676 
2025-04-29 15:57:56.147803: val_loss 2.8042 
2025-04-29 15:57:56.163432: PSNR 19.5735 
2025-04-29 15:57:56.194681: Epoch time: 1382.52 s 
2025-04-29 15:57:57.151825:  
2025-04-29 15:57:57.167450: Epoch 553 
2025-04-29 15:57:57.183075: Current learning rate: 8.57e-05 
2025-04-29 16:20:59.286416: train_loss 3.409 
2025-04-29 16:20:59.302038: val_loss 2.7392 
2025-04-29 16:20:59.317663: PSNR 19.1967 
2025-04-29 16:20:59.348921: Epoch time: 1382.13 s 
2025-04-29 16:21:00.155102:  
2025-04-29 16:21:00.155102: Epoch 554 
2025-04-29 16:21:00.170728: Current learning rate: 8.54e-05 
2025-04-29 16:43:48.435841: train_loss 3.5294 
2025-04-29 16:43:48.451463: val_loss 2.6607 
2025-04-29 16:43:48.476947: PSNR 19.4397 
2025-04-29 16:43:48.492578: Epoch time: 1368.3 s 
2025-04-29 16:43:49.336331:  
2025-04-29 16:43:49.367581: Epoch 555 
2025-04-29 16:43:49.383208: Current learning rate: 8.51e-05 
2025-04-29 17:06:42.372148: train_loss 3.3887 
2025-04-29 17:06:42.403400: val_loss 2.7269 
2025-04-29 17:06:42.419024: PSNR 19.1677 
2025-04-29 17:06:42.434652: Epoch time: 1373.04 s 
2025-04-29 17:06:43.584491:  
2025-04-29 17:06:43.600117: Epoch 556 
2025-04-29 17:06:43.615740: Current learning rate: 8.48e-05 
2025-04-29 17:29:30.706114: train_loss 3.4275 
2025-04-29 17:29:30.706114: val_loss 2.6617 
2025-04-29 17:29:30.737359: PSNR 19.5576 
2025-04-29 17:29:30.752988: Epoch time: 1367.14 s 
2025-04-29 17:29:31.837888:  
2025-04-29 17:29:31.837888: Epoch 557 
2025-04-29 17:29:31.869136: Current learning rate: 8.45e-05 
2025-04-29 17:52:27.196101: train_loss 3.461 
2025-04-29 17:52:27.227348: val_loss 2.5729 
2025-04-29 17:52:27.242972: PSNR 19.802 
2025-04-29 17:52:27.258598: Epoch time: 1375.37 s 
2025-04-29 17:52:28.117974:  
2025-04-29 17:52:28.133602: Epoch 558 
2025-04-29 17:52:28.149225: Current learning rate: 8.42e-05 
2025-04-29 18:15:32.770032: train_loss 3.3797 
2025-04-29 18:15:32.785655: val_loss 2.9976 
2025-04-29 18:15:32.801284: PSNR 18.3218 
2025-04-29 18:15:32.832534: Epoch time: 1384.67 s 
2025-04-29 18:15:33.854173:  
2025-04-29 18:15:33.869800: Epoch 559 
2025-04-29 18:15:33.885426: Current learning rate: 8.39e-05 
2025-04-29 18:38:34.206558: train_loss 3.3832 
2025-04-29 18:38:34.237800: val_loss 2.8694 
2025-04-29 18:38:34.253428: PSNR 19.0338 
2025-04-29 18:38:34.269052: Epoch time: 1380.35 s 
2025-04-29 18:38:35.372142:  
2025-04-29 18:38:35.403391: Epoch 560 
2025-04-29 18:38:35.403391: Current learning rate: 8.36e-05 
2025-04-29 19:01:38.765915: train_loss 3.4315 
2025-04-29 19:01:38.797164: val_loss 2.746 
2025-04-29 19:01:38.812792: PSNR 19.379 
2025-04-29 19:01:38.828417: Epoch time: 1383.39 s 
2025-04-29 19:01:39.821789:  
2025-04-29 19:01:39.837415: Epoch 561 
2025-04-29 19:01:39.853042: Current learning rate: 8.32e-05 
2025-04-29 19:24:41.132147: train_loss 3.353 
2025-04-29 19:24:41.147774: val_loss 2.7307 
2025-04-29 19:24:41.179026: PSNR 19.5382 
2025-04-29 19:24:41.194649: Epoch time: 1381.33 s 
2025-04-29 19:24:42.275328:  
2025-04-29 19:24:42.290954: Epoch 562 
2025-04-29 19:24:42.306580: Current learning rate: 8.29e-05 
2025-04-29 19:47:41.722319: train_loss 3.464 
2025-04-29 19:47:41.753567: val_loss 2.7169 
2025-04-29 19:47:41.753567: PSNR 19.443 
2025-04-29 19:47:41.769196: Epoch time: 1379.45 s 
2025-04-29 19:47:42.784821:  
2025-04-29 19:47:42.800452: Epoch 563 
2025-04-29 19:47:42.816070: Current learning rate: 8.26e-05 
2025-04-29 20:10:43.683031: train_loss 3.4471 
2025-04-29 20:10:43.698655: val_loss 2.6509 
2025-04-29 20:10:43.714283: PSNR 19.6401 
2025-04-29 20:10:43.745531: Epoch time: 1380.91 s 
2025-04-29 20:10:44.583067:  
2025-04-29 20:10:44.598694: Epoch 564 
2025-04-29 20:10:44.598694: Current learning rate: 8.23e-05 
2025-04-29 20:33:45.756769: train_loss 3.3006 
2025-04-29 20:33:45.772394: val_loss 2.7947 
2025-04-29 20:33:45.803645: PSNR 19.6905 
2025-04-29 20:33:45.819267: Epoch time: 1381.17 s 
2025-04-29 20:33:46.756769:  
2025-04-29 20:33:46.781409: Epoch 565 
2025-04-29 20:33:46.781409: Current learning rate: 8.2e-05 
2025-04-29 20:56:49.406047: train_loss 3.3645 
2025-04-29 20:56:49.421680: val_loss 2.6985 
2025-04-29 20:56:49.437299: PSNR 19.8556 
2025-04-29 20:56:49.452922: Epoch time: 1382.65 s 
2025-04-29 20:56:50.402153:  
2025-04-29 20:56:50.417780: Epoch 566 
2025-04-29 20:56:50.433406: Current learning rate: 8.17e-05 
2025-04-29 21:19:39.201120: train_loss 3.4233 
2025-04-29 21:19:39.216748: val_loss 2.7192 
2025-04-29 21:19:39.232374: PSNR 19.6324 
2025-04-29 21:19:39.247997: Epoch time: 1368.81 s 
2025-04-29 21:19:40.282632:  
2025-04-29 21:19:40.313883: Epoch 567 
2025-04-29 21:19:40.329509: Current learning rate: 8.14e-05 
2025-04-29 21:42:25.886564: train_loss 3.3794 
2025-04-29 21:42:25.902188: val_loss 2.812 
2025-04-29 21:42:25.917810: PSNR 19.4342 
2025-04-29 21:42:25.933437: Epoch time: 1365.6 s 
2025-04-29 21:42:26.809078:  
2025-04-29 21:42:26.824702: Epoch 568 
2025-04-29 21:42:26.840328: Current learning rate: 8.11e-05 
2025-04-29 22:05:19.969191: train_loss 3.3671 
2025-04-29 22:05:19.984816: val_loss 2.812 
2025-04-29 22:05:20.000445: PSNR 19.1205 
2025-04-29 22:05:20.031698: Epoch time: 1373.16 s 
2025-04-29 22:05:20.911303:  
2025-04-29 22:05:20.942553: Epoch 569 
2025-04-29 22:05:20.958184: Current learning rate: 8.08e-05 
