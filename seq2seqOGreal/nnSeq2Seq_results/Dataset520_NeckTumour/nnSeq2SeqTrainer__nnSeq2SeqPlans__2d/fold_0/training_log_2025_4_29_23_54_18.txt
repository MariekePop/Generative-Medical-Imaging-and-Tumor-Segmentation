
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-29 23:54:38.338401: unpacking dataset... 
2025-04-29 23:54:49.760171: unpacking done... 
2025-04-29 23:54:49.775802: do_dummy_2d_data_aug: False 
2025-04-29 23:54:49.791426: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-29 23:54:49.807047: The split file contains 5 splits. 
2025-04-29 23:54:49.822676: Desired fold for training: 0 
2025-04-29 23:54:49.822676: This split has 4 training and 1 validation cases. 
2025-04-29 23:54:50.713232: Unable to plot network architecture: 
2025-04-29 23:54:50.728887: No module named 'hiddenlayer' 
2025-04-29 23:54:50.807032:  
2025-04-29 23:54:50.807032: Epoch 550 
2025-04-29 23:54:50.822658: Current learning rate: 8.67e-05 
2025-04-30 00:02:02.229849: train_loss 2.112 
2025-04-30 00:02:02.245488: val_loss 2.0855 
2025-04-30 00:02:02.261105: PSNR 20.526 
2025-04-30 00:02:02.276730: Epoch time: 431.44 s 
2025-04-30 00:02:03.479807:  
2025-04-30 00:02:03.511059: Epoch 551 
2025-04-30 00:02:03.514995: Current learning rate: 8.64e-05 
2025-04-30 00:15:33.077431: train_loss 3.1295 
2025-04-30 00:15:33.093058: val_loss 2.6486 
2025-04-30 00:15:33.113363: PSNR 19.2446 
2025-04-30 00:15:33.133111: Epoch time: 809.6 s 
2025-04-30 00:15:34.254048:  
2025-04-30 00:15:34.269674: Epoch 552 
2025-04-30 00:15:34.285305: Current learning rate: 8.61e-05 
2025-04-30 00:29:08.213031: train_loss 3.4071 
2025-04-30 00:29:08.244278: val_loss 3.0474 
2025-04-30 00:29:08.275533: PSNR 18.1673 
2025-04-30 00:29:08.291160: Epoch time: 813.97 s 
2025-04-30 00:29:09.738381:  
2025-04-30 00:29:09.754002: Epoch 553 
2025-04-30 00:29:09.769629: Current learning rate: 8.57e-05 
2025-04-30 00:52:21.047557: train_loss 3.4073 
2025-04-30 00:52:21.110065: val_loss 2.7553 
2025-04-30 00:52:21.125692: PSNR 19.3462 
2025-04-30 00:52:21.141311: Epoch time: 1391.32 s 
2025-04-30 00:52:22.633222:  
2025-04-30 00:52:22.658297: Epoch 554 
2025-04-30 00:52:22.658297: Current learning rate: 8.54e-05 
2025-04-30 01:15:34.181477: train_loss 3.4831 
2025-04-30 01:15:34.197104: val_loss 2.7488 
2025-04-30 01:15:34.197104: PSNR 19.3266 
2025-04-30 01:15:34.228353: Epoch time: 1391.56 s 
2025-04-30 01:15:35.434850:  
2025-04-30 01:15:35.434850: Epoch 555 
2025-04-30 01:15:35.466105: Current learning rate: 8.51e-05 
2025-04-30 01:38:52.690704: train_loss 3.43 
2025-04-30 01:38:52.706330: val_loss 2.8091 
2025-04-30 01:38:52.721955: PSNR 19.0008 
2025-04-30 01:38:52.753208: Epoch time: 1397.27 s 
2025-04-30 01:38:54.079259:  
2025-04-30 01:38:54.110511: Epoch 556 
2025-04-30 01:38:54.126141: Current learning rate: 8.48e-05 
2025-04-30 02:02:15.572518: train_loss 3.3105 
2025-04-30 02:02:15.588154: val_loss 2.5924 
2025-04-30 02:02:15.603776: PSNR 19.7766 
2025-04-30 02:02:15.619397: Epoch time: 1401.49 s 
2025-04-30 02:02:16.609520:  
2025-04-30 02:02:16.625151: Epoch 557 
2025-04-30 02:02:16.640771: Current learning rate: 8.45e-05 
2025-04-30 02:25:37.869482: train_loss 3.4295 
2025-04-30 02:25:37.916363: val_loss 2.8617 
2025-04-30 02:25:37.931987: PSNR 19.7229 
2025-04-30 02:25:37.931987: Epoch time: 1401.26 s 
2025-04-30 02:25:39.139198:  
2025-04-30 02:25:39.170456: Epoch 558 
2025-04-30 02:25:39.170456: Current learning rate: 8.42e-05 
2025-04-30 02:48:58.132039: train_loss 3.4505 
2025-04-30 02:48:58.170599: val_loss 2.5107 
2025-04-30 02:48:58.186229: PSNR 20.1579 
2025-04-30 02:48:58.201851: Epoch time: 1398.99 s 
2025-04-30 02:48:59.226519:  
2025-04-30 02:48:59.269065: Epoch 559 
2025-04-30 02:48:59.284695: Current learning rate: 8.39e-05 
2025-04-30 03:12:15.884738: train_loss 3.4903 
2025-04-30 03:12:15.900364: val_loss 2.6581 
2025-04-30 03:12:15.915987: PSNR 19.841 
2025-04-30 03:12:15.931619: Epoch time: 1396.66 s 
2025-04-30 03:12:16.967916:  
2025-04-30 03:12:16.989234: Epoch 560 
2025-04-30 03:12:16.992383: Current learning rate: 8.36e-05 
2025-04-30 03:35:36.642862: train_loss 3.5415 
2025-04-30 03:35:36.674115: val_loss 2.7486 
2025-04-30 03:35:36.674115: PSNR 19.1573 
2025-04-30 03:35:36.689740: Epoch time: 1399.69 s 
2025-04-30 03:35:37.549644:  
2025-04-30 03:35:37.580895: Epoch 561 
2025-04-30 03:35:37.596529: Current learning rate: 8.32e-05 
2025-04-30 03:58:49.140329: train_loss 3.3608 
2025-04-30 03:58:49.171581: val_loss 2.8926 
2025-04-30 03:58:49.202835: PSNR 18.8113 
2025-04-30 03:58:49.202835: Epoch time: 1391.59 s 
2025-04-30 03:58:50.105571:  
2025-04-30 03:58:50.121206: Epoch 562 
2025-04-30 03:58:50.136821: Current learning rate: 8.29e-05 
2025-04-30 04:22:05.951885: train_loss 3.5686 
2025-04-30 04:22:05.983143: val_loss 2.4254 
2025-04-30 04:22:05.998769: PSNR 19.5684 
2025-04-30 04:22:06.014401: Epoch time: 1395.86 s 
2025-04-30 04:22:07.412496:  
2025-04-30 04:22:07.443746: Epoch 563 
2025-04-30 04:22:07.459373: Current learning rate: 8.26e-05 
2025-04-30 04:45:23.631249: train_loss 3.3807 
2025-04-30 04:45:23.662500: val_loss 2.6689 
2025-04-30 04:45:23.678130: PSNR 19.5967 
2025-04-30 04:45:23.693755: Epoch time: 1396.22 s 
2025-04-30 04:45:25.015201:  
2025-04-30 04:45:25.030830: Epoch 564 
2025-04-30 04:45:25.046458: Current learning rate: 8.23e-05 
2025-04-30 05:08:43.406673: train_loss 3.3901 
2025-04-30 05:08:43.437926: val_loss 2.6824 
2025-04-30 05:08:43.453561: PSNR 19.9153 
2025-04-30 05:08:43.469179: Epoch time: 1398.39 s 
2025-04-30 05:08:44.866243:  
2025-04-30 05:08:44.897496: Epoch 565 
2025-04-30 05:08:44.913116: Current learning rate: 8.2e-05 
2025-04-30 05:32:06.488235: train_loss 3.4104 
2025-04-30 05:32:06.519488: val_loss 2.722 
2025-04-30 05:32:06.535120: PSNR 19.4727 
2025-04-30 05:32:06.535120: Epoch time: 1401.62 s 
2025-04-30 05:32:07.404078:  
2025-04-30 05:32:07.419707: Epoch 566 
2025-04-30 05:32:07.450972: Current learning rate: 8.17e-05 
2025-04-30 05:55:16.824709: train_loss 3.4646 
2025-04-30 05:55:16.840334: val_loss 2.6627 
2025-04-30 05:55:16.855964: PSNR 19.7881 
2025-04-30 05:55:16.871586: Epoch time: 1389.42 s 
2025-04-30 05:55:17.849204:  
2025-04-30 05:55:17.864849: Epoch 567 
2025-04-30 05:55:17.880456: Current learning rate: 8.14e-05 
2025-04-30 06:18:39.464667: train_loss 3.4697 
2025-04-30 06:18:39.480294: val_loss 2.4508 
2025-04-30 06:18:39.495927: PSNR 19.5102 
2025-04-30 06:18:39.511550: Epoch time: 1401.62 s 
2025-04-30 06:18:40.678250:  
2025-04-30 06:18:40.694517: Epoch 568 
2025-04-30 06:18:40.710161: Current learning rate: 8.11e-05 
2025-04-30 06:41:58.002721: train_loss 3.3401 
2025-04-30 06:41:58.033975: val_loss 2.4766 
2025-04-30 06:41:58.049602: PSNR 20.0429 
2025-04-30 06:41:58.065227: Epoch time: 1397.32 s 
2025-04-30 06:41:59.395792:  
2025-04-30 06:41:59.427043: Epoch 569 
2025-04-30 06:41:59.427043: Current learning rate: 8.08e-05 
2025-04-30 07:05:15.601981: train_loss 3.48 
2025-04-30 07:05:15.647976: val_loss 2.8142 
2025-04-30 07:05:15.663600: PSNR 18.928 
2025-04-30 07:05:15.679229: Epoch time: 1396.21 s 
2025-04-30 07:05:16.852751:  
2025-04-30 07:05:16.868376: Epoch 570 
2025-04-30 07:05:16.899624: Current learning rate: 8.05e-05 
2025-04-30 07:28:24.730872: train_loss 3.4428 
2025-04-30 07:28:24.746500: val_loss 2.7537 
2025-04-30 07:28:24.762131: PSNR 19.1426 
2025-04-30 07:28:24.762131: Epoch time: 1387.88 s 
2025-04-30 07:28:26.130734:  
2025-04-30 07:28:26.161983: Epoch 571 
2025-04-30 07:28:26.161983: Current learning rate: 8.01e-05 
2025-04-30 07:51:49.295538: train_loss 3.3955 
2025-04-30 07:51:49.311165: val_loss 2.5232 
2025-04-30 07:51:49.342417: PSNR 20.3388 
2025-04-30 07:51:49.358045: Epoch time: 1403.16 s 
2025-04-30 07:51:50.751409:  
2025-04-30 07:51:50.767055: Epoch 572 
2025-04-30 07:51:50.782691: Current learning rate: 7.98e-05 
2025-04-30 08:15:10.691018: train_loss 3.4175 
2025-04-30 08:15:10.722268: val_loss 2.9858 
2025-04-30 08:15:10.737899: PSNR 18.5043 
2025-04-30 08:15:10.753522: Epoch time: 1399.94 s 
2025-04-30 08:15:12.138418:  
2025-04-30 08:15:12.169666: Epoch 573 
2025-04-30 08:15:12.185296: Current learning rate: 7.95e-05 
2025-04-30 08:38:31.439472: train_loss 3.3527 
2025-04-30 08:38:31.472773: val_loss 2.7746 
2025-04-30 08:38:31.483287: PSNR 19.1926 
2025-04-30 08:38:31.498931: Epoch time: 1399.3 s 
2025-04-30 08:38:32.539109:  
2025-04-30 08:38:32.570379: Epoch 574 
2025-04-30 08:38:32.585989: Current learning rate: 7.92e-05 
2025-04-30 09:01:54.500701: train_loss 3.5484 
2025-04-30 09:01:54.516354: val_loss 2.5718 
2025-04-30 09:01:54.531969: PSNR 19.507 
2025-04-30 09:01:54.531969: Epoch time: 1401.96 s 
2025-04-30 09:01:55.450967:  
2025-04-30 09:01:55.466613: Epoch 575 
2025-04-30 09:01:55.482220: Current learning rate: 7.89e-05 
2025-04-30 09:25:14.298067: train_loss 3.3185 
2025-04-30 09:25:14.329318: val_loss 2.8837 
2025-04-30 09:25:14.360574: PSNR 19.1599 
2025-04-30 09:25:14.376200: Epoch time: 1398.85 s 
2025-04-30 09:25:15.698571:  
2025-04-30 09:25:15.729819: Epoch 576 
2025-04-30 09:25:15.745444: Current learning rate: 7.86e-05 
2025-04-30 09:48:28.201998: train_loss 3.4407 
2025-04-30 09:48:28.233250: val_loss 2.7209 
2025-04-30 09:48:28.233250: PSNR 19.2112 
2025-04-30 09:48:28.264510: Epoch time: 1392.5 s 
2025-04-30 09:48:29.524329:  
2025-04-30 09:48:29.539963: Epoch 577 
2025-04-30 09:48:29.555583: Current learning rate: 7.83e-05 
2025-04-30 10:11:40.509787: train_loss 3.3848 
2025-04-30 10:11:40.541036: val_loss 2.699 
2025-04-30 10:11:40.556665: PSNR 19.8927 
2025-04-30 10:11:40.572292: Epoch time: 1390.99 s 
2025-04-30 10:11:41.938799:  
2025-04-30 10:11:41.954420: Epoch 578 
2025-04-30 10:11:41.985670: Current learning rate: 7.8e-05 
2025-04-30 10:34:59.358317: train_loss 3.462 
2025-04-30 10:34:59.390195: val_loss 2.6566 
2025-04-30 10:34:59.405828: PSNR 19.6669 
2025-04-30 10:34:59.418439: Epoch time: 1397.44 s 
2025-04-30 10:35:00.843889:  
2025-04-30 10:35:00.875140: Epoch 579 
2025-04-30 10:35:00.890769: Current learning rate: 7.77e-05 
2025-04-30 10:58:19.633713: train_loss 3.4428 
2025-04-30 10:58:19.678760: val_loss 2.902 
2025-04-30 10:58:19.694387: PSNR 19.7551 
2025-04-30 10:58:19.710015: Epoch time: 1398.81 s 
2025-04-30 10:58:20.819391:  
2025-04-30 10:58:20.835016: Epoch 580 
2025-04-30 10:58:20.850645: Current learning rate: 7.74e-05 
2025-04-30 11:21:36.461729: train_loss 3.3331 
2025-04-30 11:21:36.477363: val_loss 2.8391 
2025-04-30 11:21:36.492983: PSNR 19.0683 
2025-04-30 11:21:36.508607: Epoch time: 1395.64 s 
2025-04-30 11:21:37.690460:  
2025-04-30 11:21:37.706086: Epoch 581 
2025-04-30 11:21:37.721716: Current learning rate: 7.71e-05 
2025-04-30 11:44:54.145403: train_loss 3.4914 
2025-04-30 11:44:54.161032: val_loss 2.69 
2025-04-30 11:44:54.176661: PSNR 19.124 
2025-04-30 11:44:54.192290: Epoch time: 1396.45 s 
2025-04-30 11:44:55.170162:  
2025-04-30 11:44:55.185794: Epoch 582 
2025-04-30 11:44:55.201418: Current learning rate: 7.68e-05 
2025-04-30 12:08:14.334941: train_loss 3.3938 
2025-04-30 12:08:14.350570: val_loss 2.7083 
2025-04-30 12:08:14.366193: PSNR 19.8941 
2025-04-30 12:08:14.397439: Epoch time: 1399.18 s 
2025-04-30 12:08:15.421965:  
2025-04-30 12:08:15.453214: Epoch 583 
2025-04-30 12:08:15.484474: Current learning rate: 7.64e-05 
2025-04-30 12:31:31.965236: train_loss 3.4031 
2025-04-30 12:31:31.996489: val_loss 2.7513 
2025-04-30 12:31:32.012130: PSNR 19.5809 
2025-04-30 12:31:32.027747: Epoch time: 1396.54 s 
2025-04-30 12:31:33.154472:  
2025-04-30 12:31:33.170098: Epoch 584 
2025-04-30 12:31:33.185728: Current learning rate: 7.61e-05 
2025-04-30 12:54:43.070786: train_loss 3.3479 
2025-04-30 12:54:43.086410: val_loss 2.7368 
2025-04-30 12:54:43.117659: PSNR 19.2044 
2025-04-30 12:54:43.117659: Epoch time: 1389.92 s 
2025-04-30 12:54:44.095500:  
2025-04-30 12:54:44.126747: Epoch 585 
2025-04-30 12:54:44.143044: Current learning rate: 7.58e-05 
2025-04-30 13:18:06.868755: train_loss 3.3743 
2025-04-30 13:18:06.900007: val_loss 2.6591 
2025-04-30 13:18:06.915647: PSNR 19.9155 
2025-04-30 13:18:06.931259: Epoch time: 1402.77 s 
2025-04-30 13:18:07.956012:  
2025-04-30 13:18:07.987277: Epoch 586 
2025-04-30 13:18:07.987277: Current learning rate: 7.55e-05 
2025-04-30 13:41:27.992717: train_loss 3.4086 
2025-04-30 13:41:28.023970: val_loss 2.6888 
2025-04-30 13:41:28.023970: PSNR 19.1447 
2025-04-30 13:41:28.055219: Epoch time: 1400.04 s 
2025-04-30 13:41:29.052165:  
2025-04-30 13:41:29.083415: Epoch 587 
2025-04-30 13:41:29.099068: Current learning rate: 7.52e-05 
2025-04-30 14:04:40.684774: train_loss 3.4328 
2025-04-30 14:04:40.700408: val_loss 2.8133 
2025-04-30 14:04:40.716031: PSNR 19.0859 
2025-04-30 14:04:40.716031: Epoch time: 1391.63 s 
2025-04-30 14:04:41.662660:  
2025-04-30 14:04:41.693928: Epoch 588 
2025-04-30 14:04:41.709535: Current learning rate: 7.49e-05 
2025-04-30 14:27:56.385018: train_loss 3.4198 
2025-04-30 14:27:56.416277: val_loss 2.5877 
2025-04-30 14:27:56.431902: PSNR 19.5423 
2025-04-30 14:27:56.447528: Epoch time: 1394.72 s 
2025-04-30 14:27:57.831427:  
2025-04-30 14:27:57.862695: Epoch 589 
2025-04-30 14:27:57.878309: Current learning rate: 7.46e-05 
2025-04-30 14:51:07.505281: train_loss 3.4311 
2025-04-30 14:51:07.520908: val_loss 3.0542 
2025-04-30 14:51:07.536534: PSNR 18.1527 
2025-04-30 14:51:07.552157: Epoch time: 1389.67 s 
2025-04-30 14:51:08.646869:  
2025-04-30 14:51:08.651074: Epoch 590 
2025-04-30 14:51:08.666726: Current learning rate: 7.43e-05 
2025-04-30 15:14:30.222269: train_loss 3.4263 
2025-04-30 15:14:30.237891: val_loss 2.8314 
2025-04-30 15:14:30.269145: PSNR 19.3044 
2025-04-30 15:14:30.284771: Epoch time: 1401.58 s 
2025-04-30 15:14:31.680223:  
2025-04-30 15:14:31.695850: Epoch 591 
2025-04-30 15:14:31.711476: Current learning rate: 7.4e-05 
2025-04-30 15:37:51.018183: train_loss 3.2753 
2025-04-30 15:37:51.049440: val_loss 2.9898 
2025-04-30 15:37:51.080686: PSNR 19.3738 
2025-04-30 15:37:51.096313: Epoch time: 1399.34 s 
2025-04-30 15:37:52.683584:  
2025-04-30 15:37:52.699209: Epoch 592 
2025-04-30 15:37:52.714839: Current learning rate: 7.37e-05 
2025-04-30 16:01:12.834458: train_loss 3.4886 
2025-04-30 16:01:12.850085: val_loss 2.6236 
2025-04-30 16:01:12.865717: PSNR 19.2376 
2025-04-30 16:01:12.881340: Epoch time: 1400.17 s 
2025-04-30 16:01:13.937298:  
2025-04-30 16:01:13.968549: Epoch 593 
2025-04-30 16:01:13.968549: Current learning rate: 7.34e-05 
2025-04-30 16:24:29.500907: train_loss 3.4432 
2025-04-30 16:24:29.532154: val_loss 2.7973 
2025-04-30 16:24:29.563405: PSNR 18.955 
2025-04-30 16:24:29.579036: Epoch time: 1395.56 s 
2025-04-30 16:24:31.304010:  
2025-04-30 16:24:31.319662: Epoch 594 
2025-04-30 16:24:31.350890: Current learning rate: 7.31e-05 
2025-04-30 16:47:47.809462: train_loss 3.3867 
2025-04-30 16:47:47.853546: val_loss 2.9548 
2025-04-30 16:47:47.869175: PSNR 19.0609 
2025-04-30 16:47:47.884799: Epoch time: 1396.52 s 
2025-04-30 16:47:49.018897:  
2025-04-30 16:47:49.050147: Epoch 595 
2025-04-30 16:47:49.050147: Current learning rate: 7.28e-05 
2025-04-30 17:11:00.732050: train_loss 3.2647 
2025-04-30 17:11:00.747679: val_loss 2.9669 
2025-04-30 17:11:00.778932: PSNR 18.7302 
2025-04-30 17:11:00.794556: Epoch time: 1391.71 s 
2025-04-30 17:11:02.332399:  
2025-04-30 17:11:02.348026: Epoch 596 
2025-04-30 17:11:02.363641: Current learning rate: 7.25e-05 
2025-04-30 17:34:21.070051: train_loss 3.3224 
2025-04-30 17:34:21.085680: val_loss 2.8801 
2025-04-30 17:34:21.101301: PSNR 19.2501 
2025-04-30 17:34:21.116927: Epoch time: 1398.75 s 
2025-04-30 17:34:22.761065:  
2025-04-30 17:34:22.792312: Epoch 597 
2025-04-30 17:34:22.807940: Current learning rate: 7.22e-05 
2025-04-30 17:57:40.252009: train_loss 3.4337 
2025-04-30 17:57:40.283259: val_loss 2.6411 
2025-04-30 17:57:40.298885: PSNR 19.6416 
2025-04-30 17:57:40.314516: Epoch time: 1397.49 s 
2025-04-30 17:57:41.861862:  
2025-04-30 17:57:41.877486: Epoch 598 
2025-04-30 17:57:41.893113: Current learning rate: 7.19e-05 
2025-04-30 18:20:59.212137: train_loss 3.374 
2025-04-30 18:20:59.227767: val_loss 2.5564 
2025-04-30 18:20:59.243390: PSNR 20.139 
2025-04-30 18:20:59.274637: Epoch time: 1397.37 s 
2025-04-30 18:21:00.497029:  
2025-04-30 18:21:00.512655: Epoch 599 
2025-04-30 18:21:00.528287: Current learning rate: 7.16e-05 
2025-04-30 18:44:16.926085: train_loss 3.3933 
2025-04-30 18:44:16.941713: val_loss 2.8704 
2025-04-30 18:44:16.972969: PSNR 19.088 
2025-04-30 18:44:16.988594: Epoch time: 1396.43 s 
2025-04-30 18:44:25.261619:  
2025-04-30 18:44:25.277243: Epoch 600 
2025-04-30 18:44:25.317753: Current learning rate: 7.13e-05 
2025-04-30 19:07:44.601961: train_loss 3.3758 
2025-04-30 19:07:44.633210: val_loss 2.8334 
2025-04-30 19:07:44.648849: PSNR 19.0077 
2025-04-30 19:07:44.664475: Epoch time: 1399.34 s 
2025-04-30 19:07:46.150127:  
2025-04-30 19:07:46.165768: Epoch 601 
2025-04-30 19:07:46.181376: Current learning rate: 7.1e-05 
2025-04-30 19:31:07.011625: train_loss 3.3678 
2025-04-30 19:31:07.042887: val_loss 2.4553 
2025-04-30 19:31:07.058515: PSNR 20.3617 
2025-04-30 19:31:07.058515: Epoch time: 1400.88 s 
2025-04-30 19:31:08.514330:  
2025-04-30 19:31:08.529945: Epoch 602 
2025-04-30 19:31:08.561194: Current learning rate: 7.07e-05 
2025-04-30 19:54:24.625267: train_loss 3.4265 
2025-04-30 19:54:24.640894: val_loss 2.7834 
2025-04-30 19:54:24.656519: PSNR 19.1734 
2025-04-30 19:54:24.687771: Epoch time: 1396.13 s 
2025-04-30 19:54:25.931307:  
2025-04-30 19:54:25.962577: Epoch 603 
2025-04-30 19:54:25.978191: Current learning rate: 7.04e-05 
2025-04-30 20:17:36.505599: train_loss 3.3822 
2025-04-30 20:17:36.521224: val_loss 2.5072 
2025-04-30 20:17:36.552474: PSNR 20.0984 
2025-04-30 20:17:36.552474: Epoch time: 1390.57 s 
2025-04-30 20:17:37.875634:  
2025-04-30 20:17:37.906884: Epoch 604 
2025-04-30 20:17:37.938136: Current learning rate: 7.01e-05 
2025-04-30 20:40:56.685555: train_loss 3.3255 
2025-04-30 20:40:56.716805: val_loss 2.7557 
2025-04-30 20:40:56.732432: PSNR 19.2111 
2025-04-30 20:40:56.748061: Epoch time: 1398.81 s 
2025-04-30 20:40:57.713711:  
2025-04-30 20:40:57.744963: Epoch 605 
2025-04-30 20:40:57.760591: Current learning rate: 6.98e-05 
2025-04-30 21:04:14.090734: train_loss 3.3908 
2025-04-30 21:04:14.121984: val_loss 2.727 
2025-04-30 21:04:14.145460: PSNR 19.653 
2025-04-30 21:04:14.161100: Epoch time: 1396.38 s 
2025-04-30 21:04:15.506203:  
2025-04-30 21:04:15.521829: Epoch 606 
2025-04-30 21:04:15.537454: Current learning rate: 6.95e-05 
2025-04-30 21:27:26.325377: train_loss 3.4201 
2025-04-30 21:27:26.356631: val_loss 2.4742 
2025-04-30 21:27:26.372256: PSNR 20.2639 
2025-04-30 21:27:26.403510: Epoch time: 1390.82 s 
2025-04-30 21:27:27.772127:  
2025-04-30 21:27:27.803382: Epoch 607 
2025-04-30 21:27:27.819008: Current learning rate: 6.92e-05 
2025-04-30 21:50:48.242622: train_loss 3.4199 
2025-04-30 21:50:48.258248: val_loss 2.7652 
2025-04-30 21:50:48.273871: PSNR 19.6599 
2025-04-30 21:50:48.289502: Epoch time: 1400.47 s 
2025-04-30 21:50:49.671518:  
2025-04-30 21:50:49.687144: Epoch 608 
2025-04-30 21:50:49.702773: Current learning rate: 6.89e-05 
2025-04-30 22:14:03.897227: train_loss 3.2897 
2025-04-30 22:14:03.912863: val_loss 2.7039 
2025-04-30 22:14:03.944106: PSNR 19.5545 
2025-04-30 22:14:03.959729: Epoch time: 1394.23 s 
2025-04-30 22:14:05.140589:  
2025-04-30 22:14:05.156224: Epoch 609 
2025-04-30 22:14:05.187466: Current learning rate: 6.86e-05 
2025-04-30 22:37:27.251905: train_loss 3.3416 
2025-04-30 22:37:27.267541: val_loss 2.8404 
2025-04-30 22:37:27.283159: PSNR 19.1784 
2025-04-30 22:37:27.298786: Epoch time: 1402.11 s 
2025-04-30 22:37:28.854864:  
2025-04-30 22:37:28.870497: Epoch 610 
2025-04-30 22:37:28.901743: Current learning rate: 6.83e-05 
2025-04-30 23:00:43.124838: train_loss 3.4554 
2025-04-30 23:00:43.140458: val_loss 2.6146 
2025-04-30 23:00:43.156090: PSNR 19.7044 
2025-04-30 23:00:43.171706: Epoch time: 1394.27 s 
2025-04-30 23:00:44.350122:  
2025-04-30 23:00:44.365755: Epoch 611 
2025-04-30 23:00:44.381381: Current learning rate: 6.8e-05 
2025-04-30 23:24:08.275770: train_loss 3.3313 
2025-04-30 23:24:08.307022: val_loss 2.7464 
2025-04-30 23:24:08.322654: PSNR 19.2443 
2025-04-30 23:24:08.338282: Epoch time: 1403.93 s 
2025-04-30 23:24:09.660833:  
2025-04-30 23:24:09.692100: Epoch 612 
2025-04-30 23:24:09.707710: Current learning rate: 6.77e-05 
2025-04-30 23:47:26.797772: train_loss 3.2863 
2025-04-30 23:47:26.833027: val_loss 2.5781 
2025-04-30 23:47:26.848675: PSNR 20.0821 
2025-04-30 23:47:26.873427: Epoch time: 1397.14 s 
2025-04-30 23:47:28.233534:  
2025-04-30 23:47:28.257737: Epoch 613 
2025-04-30 23:47:28.273348: Current learning rate: 6.74e-05 
2025-05-01 00:10:51.779679: train_loss 3.41 
2025-05-01 00:10:51.810935: val_loss 2.5938 
2025-05-01 00:10:51.826557: PSNR 19.8397 
2025-05-01 00:10:51.842184: Epoch time: 1403.56 s 
2025-05-01 00:10:53.282634:  
2025-05-01 00:10:53.319499: Epoch 614 
2025-05-01 00:10:53.325914: Current learning rate: 6.71e-05 
2025-05-01 00:34:11.818881: train_loss 3.2568 
2025-05-01 00:34:11.850130: val_loss 2.6298 
2025-05-01 00:34:11.865756: PSNR 20.0807 
2025-05-01 00:34:11.897009: Epoch time: 1398.54 s 
2025-05-01 00:34:13.073477:  
2025-05-01 00:34:13.073477: Epoch 615 
2025-05-01 00:34:13.104727: Current learning rate: 6.68e-05 
2025-05-01 00:57:27.266225: train_loss 3.5122 
2025-05-01 00:57:27.297472: val_loss 2.645 
2025-05-01 00:57:27.313105: PSNR 19.7161 
2025-05-01 00:57:27.328729: Epoch time: 1394.21 s 
2025-05-01 00:57:28.409174:  
2025-05-01 00:57:28.440422: Epoch 616 
2025-05-01 00:57:28.440422: Current learning rate: 6.65e-05 
2025-05-01 01:20:49.457379: train_loss 3.3448 
2025-05-01 01:20:49.489239: val_loss 2.958 
2025-05-01 01:20:49.504865: PSNR 18.7552 
2025-05-01 01:20:49.520494: Epoch time: 1401.05 s 
2025-05-01 01:20:51.123238:  
2025-05-01 01:20:51.138855: Epoch 617 
2025-05-01 01:20:51.170105: Current learning rate: 6.62e-05 
2025-05-01 01:44:11.672753: train_loss 3.3477 
2025-05-01 01:44:11.704005: val_loss 2.6063 
2025-05-01 01:44:11.719636: PSNR 20.1027 
2025-05-01 01:44:11.735266: Epoch time: 1400.57 s 
2025-05-01 01:44:13.044186:  
2025-05-01 01:44:13.091062: Epoch 618 
2025-05-01 01:44:13.106688: Current learning rate: 6.59e-05 
2025-05-01 02:07:37.095581: train_loss 3.3789 
2025-05-01 02:07:37.111216: val_loss 2.6342 
2025-05-01 02:07:37.111216: PSNR 19.3798 
2025-05-01 02:07:37.142457: Epoch time: 1404.05 s 
2025-05-01 02:07:38.182976:  
2025-05-01 02:07:38.214227: Epoch 619 
2025-05-01 02:07:38.214227: Current learning rate: 6.56e-05 
2025-05-01 02:30:55.890053: train_loss 3.3374 
2025-05-01 02:30:55.905677: val_loss 2.7996 
2025-05-01 02:30:55.921306: PSNR 19.5103 
2025-05-01 02:30:55.936935: Epoch time: 1397.71 s 
2025-05-01 02:30:57.492710:  
2025-05-01 02:30:57.523961: Epoch 620 
2025-05-01 02:30:57.539586: Current learning rate: 6.53e-05 
2025-05-01 02:54:16.011164: train_loss 3.349 
2025-05-01 02:54:16.026804: val_loss 2.6917 
2025-05-01 02:54:16.042416: PSNR 19.54 
2025-05-01 02:54:16.064846: Epoch time: 1398.53 s 
2025-05-01 02:54:17.605120:  
2025-05-01 02:54:17.620743: Epoch 621 
2025-05-01 02:54:17.636364: Current learning rate: 6.5e-05 
2025-05-01 03:17:30.533413: train_loss 3.2883 
2025-05-01 03:17:30.564661: val_loss 2.7555 
2025-05-01 03:17:30.580291: PSNR 19.2747 
2025-05-01 03:17:30.595922: Epoch time: 1392.94 s 
2025-05-01 03:17:31.980226:  
2025-05-01 03:17:32.011471: Epoch 622 
2025-05-01 03:17:32.027099: Current learning rate: 6.47e-05 
2025-05-01 03:40:56.920959: train_loss 3.3696 
2025-05-01 03:40:56.952209: val_loss 2.6307 
2025-05-01 03:40:56.967839: PSNR 19.9482 
2025-05-01 03:40:56.983462: Epoch time: 1404.96 s 
2025-05-01 03:40:58.383316:  
2025-05-01 03:40:58.398939: Epoch 623 
2025-05-01 03:40:58.417253: Current learning rate: 6.44e-05 
2025-05-01 04:04:18.977330: train_loss 3.3704 
2025-05-01 04:04:19.008589: val_loss 2.6177 
2025-05-01 04:04:19.024209: PSNR 19.7896 
2025-05-01 04:04:19.039833: Epoch time: 1400.59 s 
2025-05-01 04:04:20.556748:  
2025-05-01 04:04:20.572364: Epoch 624 
2025-05-01 04:04:20.603613: Current learning rate: 6.41e-05 
2025-05-01 04:27:42.378404: train_loss 3.3748 
2025-05-01 04:27:42.415832: val_loss 2.685 
2025-05-01 04:27:42.415832: PSNR 19.6862 
2025-05-01 04:27:42.447215: Epoch time: 1401.84 s 
2025-05-01 04:27:43.477430:  
2025-05-01 04:27:43.493056: Epoch 625 
2025-05-01 04:27:43.508679: Current learning rate: 6.38e-05 
2025-05-01 04:51:01.544069: train_loss 3.2678 
2025-05-01 04:51:01.568879: val_loss 2.8756 
2025-05-01 04:51:01.584529: PSNR 19.5154 
2025-05-01 04:51:01.600153: Epoch time: 1398.07 s 
2025-05-01 04:51:02.772059:  
2025-05-01 04:51:02.787668: Epoch 626 
2025-05-01 04:51:02.803295: Current learning rate: 6.35e-05 
2025-05-01 05:14:24.704675: train_loss 3.3658 
2025-05-01 05:14:24.720300: val_loss 3.0114 
2025-05-01 05:14:24.751551: PSNR 19.0976 
2025-05-01 05:14:24.767178: Epoch time: 1401.95 s 
2025-05-01 05:14:26.088709:  
2025-05-01 05:14:26.104326: Epoch 627 
2025-05-01 05:14:26.119956: Current learning rate: 6.32e-05 
2025-05-01 05:37:40.860721: train_loss 3.2892 
2025-05-01 05:37:40.876359: val_loss 2.7844 
2025-05-01 05:37:40.900882: PSNR 18.9816 
2025-05-01 05:37:40.900882: Epoch time: 1394.79 s 
2025-05-01 05:37:41.959523:  
2025-05-01 05:37:41.991418: Epoch 628 
2025-05-01 05:37:42.007077: Current learning rate: 6.29e-05 
2025-05-01 06:01:02.289038: train_loss 3.3422 
2025-05-01 06:01:02.304662: val_loss 2.7097 
2025-05-01 06:01:02.335930: PSNR 19.5824 
2025-05-01 06:01:02.367048: Epoch time: 1400.33 s 
2025-05-01 06:01:03.813708:  
2025-05-01 06:01:03.844947: Epoch 629 
2025-05-01 06:01:03.860574: Current learning rate: 6.26e-05 
2025-05-01 06:24:20.617111: train_loss 3.3538 
2025-05-01 06:24:20.632743: val_loss 2.5006 
2025-05-01 06:24:20.648360: PSNR 19.6375 
2025-05-01 06:24:20.663987: Epoch time: 1396.82 s 
2025-05-01 06:24:21.579499:  
2025-05-01 06:24:21.606186: Epoch 630 
2025-05-01 06:24:21.621848: Current learning rate: 6.23e-05 
2025-05-01 06:47:39.713255: train_loss 3.3234 
2025-05-01 06:47:39.744507: val_loss 2.586 
2025-05-01 06:47:39.760137: PSNR 20.1586 
2025-05-01 06:47:39.790595: Epoch time: 1398.13 s 
2025-05-01 06:47:41.347522:  
2025-05-01 06:47:41.378773: Epoch 631 
2025-05-01 06:47:41.378773: Current learning rate: 6.21e-05 
2025-05-01 07:11:02.473446: train_loss 3.314 
2025-05-01 07:11:02.489071: val_loss 2.616 
2025-05-01 07:11:02.520315: PSNR 19.9781 
2025-05-01 07:11:02.535944: Epoch time: 1401.13 s 
2025-05-01 07:11:03.661151:  
2025-05-01 07:11:03.692389: Epoch 632 
2025-05-01 07:11:03.708017: Current learning rate: 6.18e-05 
2025-05-01 07:34:20.897005: train_loss 3.3491 
2025-05-01 07:34:20.912631: val_loss 2.9422 
2025-05-01 07:34:20.928258: PSNR 19.1802 
2025-05-01 07:34:20.943887: Epoch time: 1397.24 s 
2025-05-01 07:34:22.328010:  
2025-05-01 07:34:22.343625: Epoch 633 
2025-05-01 07:34:22.359248: Current learning rate: 6.15e-05 
2025-05-01 07:57:35.667927: train_loss 3.2305 
2025-05-01 07:57:35.699181: val_loss 2.7478 
2025-05-01 07:57:35.714811: PSNR 19.3853 
2025-05-01 07:57:35.730435: Epoch time: 1393.34 s 
2025-05-01 07:57:36.671211:  
2025-05-01 07:57:36.702463: Epoch 634 
2025-05-01 07:57:36.702463: Current learning rate: 6.12e-05 
2025-05-01 08:20:59.344017: train_loss 3.0341 
2025-05-01 08:20:59.375261: val_loss 2.6403 
2025-05-01 08:20:59.390891: PSNR 19.6978 
2025-05-01 08:20:59.406516: Epoch time: 1402.67 s 
2025-05-01 08:21:00.865392:  
2025-05-01 08:21:00.896630: Epoch 635 
2025-05-01 08:21:00.912258: Current learning rate: 6.09e-05 
2025-05-01 08:44:18.572992: train_loss 3.0617 
2025-05-01 08:44:18.619074: val_loss 2.7756 
2025-05-01 08:44:18.634701: PSNR 19.6588 
2025-05-01 08:44:18.650334: Epoch time: 1397.72 s 
2025-05-01 08:44:20.215765:  
2025-05-01 08:44:20.231379: Epoch 636 
2025-05-01 08:44:20.262626: Current learning rate: 6.06e-05 
2025-05-01 09:07:33.260708: train_loss 2.9819 
2025-05-01 09:07:33.291961: val_loss 2.9226 
2025-05-01 09:07:33.307588: PSNR 19.0093 
2025-05-01 09:07:33.307588: Epoch time: 1393.06 s 
2025-05-01 09:07:34.394810:  
2025-05-01 09:07:34.410436: Epoch 637 
2025-05-01 09:07:34.426058: Current learning rate: 6.03e-05 
2025-05-01 09:30:59.637949: train_loss 3.0742 
2025-05-01 09:30:59.653573: val_loss 2.852 
2025-05-01 09:30:59.684827: PSNR 18.9333 
2025-05-01 09:30:59.700455: Epoch time: 1405.24 s 
2025-05-01 09:31:00.968482:  
2025-05-01 09:31:00.999732: Epoch 638 
2025-05-01 09:31:01.015361: Current learning rate: 6e-05 
2025-05-01 09:54:19.007883: train_loss 3.0174 
2025-05-01 09:54:19.023516: val_loss 2.7545 
2025-05-01 09:54:19.039135: PSNR 19.6203 
2025-05-01 09:54:19.054762: Epoch time: 1398.04 s 
2025-05-01 09:54:20.156485:  
2025-05-01 09:54:20.187738: Epoch 639 
2025-05-01 09:54:20.203365: Current learning rate: 5.97e-05 
2025-05-01 10:17:33.618893: train_loss 3.0347 
2025-05-01 10:17:33.650151: val_loss 2.8872 
2025-05-01 10:17:33.665767: PSNR 19.1289 
2025-05-01 10:17:33.681395: Epoch time: 1393.46 s 
2025-05-01 10:17:34.721690:  
2025-05-01 10:17:34.752941: Epoch 640 
2025-05-01 10:17:34.768571: Current learning rate: 5.94e-05 
2025-05-01 10:40:57.203902: train_loss 2.9481 
2025-05-01 10:40:57.235152: val_loss 2.9967 
2025-05-01 10:40:57.250779: PSNR 18.9782 
2025-05-01 10:40:57.266404: Epoch time: 1402.48 s 
2025-05-01 10:40:58.590682:  
2025-05-01 10:40:58.606297: Epoch 641 
2025-05-01 10:40:58.621921: Current learning rate: 5.92e-05 
2025-05-01 11:04:11.041572: train_loss 3.107 
2025-05-01 11:04:11.088451: val_loss 2.653 
2025-05-01 11:04:11.104081: PSNR 20.0816 
2025-05-01 11:04:11.119703: Epoch time: 1392.47 s 
2025-05-01 11:04:12.639157:  
2025-05-01 11:04:12.670411: Epoch 642 
2025-05-01 11:04:12.686033: Current learning rate: 5.89e-05 
2025-05-01 11:27:36.153922: train_loss 3.1011 
2025-05-01 11:27:36.200800: val_loss 2.5646 
2025-05-01 11:27:36.216448: PSNR 20.1494 
2025-05-01 11:27:36.232054: Epoch time: 1403.51 s 
2025-05-01 11:27:37.128613:  
2025-05-01 11:27:37.159864: Epoch 643 
2025-05-01 11:27:37.175492: Current learning rate: 5.86e-05 
2025-05-01 11:50:55.138606: train_loss 3.1907 
2025-05-01 11:50:55.154242: val_loss 2.6449 
2025-05-01 11:50:55.185494: PSNR 19.4089 
2025-05-01 11:50:55.185494: Epoch time: 1398.01 s 
2025-05-01 11:50:56.091755:  
2025-05-01 11:50:56.107377: Epoch 644 
2025-05-01 11:50:56.123001: Current learning rate: 5.83e-05 
2025-05-01 12:14:18.009156: train_loss 2.9319 
2025-05-01 12:14:18.024781: val_loss 2.7241 
2025-05-01 12:14:18.040409: PSNR 19.8969 
2025-05-01 12:14:18.063877: Epoch time: 1401.92 s 
2025-05-01 12:14:19.256140:  
2025-05-01 12:14:19.258811: Epoch 645 
2025-05-01 12:14:19.274453: Current learning rate: 5.8e-05 
2025-05-01 12:37:41.145195: train_loss 3.0211 
2025-05-01 12:37:41.160829: val_loss 2.6947 
2025-05-01 12:37:41.176446: PSNR 19.8588 
2025-05-01 12:37:41.192071: Epoch time: 1401.89 s 
2025-05-01 12:37:42.252164:  
2025-05-01 12:37:42.267792: Epoch 646 
2025-05-01 12:37:42.283415: Current learning rate: 5.77e-05 
2025-05-01 13:01:05.695063: train_loss 3.101 
2025-05-01 13:01:05.710689: val_loss 2.7026 
2025-05-01 13:01:05.726319: PSNR 19.8055 
2025-05-01 13:01:05.741944: Epoch time: 1403.44 s 
2025-05-01 13:01:07.133152:  
2025-05-01 13:01:07.164425: Epoch 647 
2025-05-01 13:01:07.180032: Current learning rate: 5.74e-05 
2025-05-01 13:24:30.052028: train_loss 3.0212 
2025-05-01 13:24:30.083282: val_loss 2.6944 
2025-05-01 13:24:30.098916: PSNR 19.6031 
2025-05-01 13:24:30.114532: Epoch time: 1402.92 s 
2025-05-01 13:24:31.102422:  
2025-05-01 13:24:31.118054: Epoch 648 
2025-05-01 13:24:31.149303: Current learning rate: 5.72e-05 
2025-05-01 13:47:45.324556: train_loss 3.0251 
2025-05-01 13:47:45.340195: val_loss 2.6889 
2025-05-01 13:47:45.355816: PSNR 20.3862 
2025-05-01 13:47:45.371429: Epoch time: 1394.22 s 
2025-05-01 13:47:46.749199:  
2025-05-01 13:47:46.764818: Epoch 649 
2025-05-01 13:47:46.780443: Current learning rate: 5.69e-05 
2025-05-01 14:11:05.618982: train_loss 3.0445 
2025-05-01 14:11:05.634605: val_loss 2.7977 
2025-05-01 14:11:05.665860: PSNR 19.4034 
2025-05-01 14:11:05.681484: Epoch time: 1398.89 s 
2025-05-01 14:11:12.078449:  
2025-05-01 14:11:12.094078: Epoch 650 
2025-05-01 14:11:12.109704: Current learning rate: 5.66e-05 
2025-05-01 14:34:32.918118: train_loss 3.0884 
2025-05-01 14:34:32.949367: val_loss 2.4469 
2025-05-01 14:34:32.980622: PSNR 20.7443 
2025-05-01 14:34:32.996248: Epoch time: 1400.84 s 
2025-05-01 14:34:33.011879: Yayy! New best EMA pseudo PSNR: 19.768 
2025-05-01 14:34:40.829670:  
2025-05-01 14:34:40.860922: Epoch 651 
2025-05-01 14:34:40.876553: Current learning rate: 5.63e-05 
2025-05-01 14:58:02.122771: train_loss 3.0424 
2025-05-01 14:58:02.138414: val_loss 2.716 
2025-05-01 14:58:02.154017: PSNR 19.6467 
2025-05-01 14:58:02.169647: Epoch time: 1401.29 s 
2025-05-01 14:58:03.150446:  
2025-05-01 14:58:03.168560: Epoch 652 
2025-05-01 14:58:03.184214: Current learning rate: 5.6e-05 
2025-05-01 15:21:14.222678: train_loss 3.0027 
2025-05-01 15:21:14.253929: val_loss 2.7804 
2025-05-01 15:21:14.269563: PSNR 19.8573 
2025-05-01 15:21:14.285181: Epoch time: 1391.09 s 
2025-05-01 15:21:15.169377:  
2025-05-01 15:21:15.200642: Epoch 653 
2025-05-01 15:21:15.216254: Current learning rate: 5.57e-05 
2025-05-01 15:44:38.368145: train_loss 3.1075 
2025-05-01 15:44:38.399395: val_loss 2.5501 
2025-05-01 15:44:38.415026: PSNR 19.5535 
2025-05-01 15:44:38.430649: Epoch time: 1403.2 s 
2025-05-01 15:44:39.721101:  
2025-05-01 15:44:39.752357: Epoch 654 
2025-05-01 15:44:39.767987: Current learning rate: 5.54e-05 
2025-05-01 16:07:59.789173: train_loss 3.0703 
2025-05-01 16:07:59.804917: val_loss 2.7157 
2025-05-01 16:07:59.820567: PSNR 19.9189 
2025-05-01 16:07:59.851808: Epoch time: 1400.07 s 
2025-05-01 16:08:00.880196:  
2025-05-01 16:08:00.895843: Epoch 655 
2025-05-01 16:08:00.911450: Current learning rate: 5.52e-05 
2025-05-01 16:31:18.779332: train_loss 3.0753 
2025-05-01 16:31:18.810582: val_loss 2.7079 
2025-05-01 16:31:18.841838: PSNR 19.8506 
2025-05-01 16:31:18.857462: Epoch time: 1397.9 s 
2025-05-01 16:31:18.873098: Yayy! New best EMA pseudo PSNR: 19.771 
2025-05-01 16:31:27.683067:  
2025-05-01 16:31:27.698684: Epoch 656 
2025-05-01 16:31:27.729924: Current learning rate: 5.49e-05 
2025-05-01 16:54:40.750421: train_loss 3.0821 
2025-05-01 16:54:40.766053: val_loss 2.7643 
2025-05-01 16:54:40.797303: PSNR 19.2224 
2025-05-01 16:54:40.812927: Epoch time: 1393.08 s 
2025-05-01 16:54:42.119155:  
2025-05-01 16:54:42.150413: Epoch 657 
2025-05-01 16:54:42.150413: Current learning rate: 5.46e-05 
2025-05-01 17:17:59.257271: train_loss 3.0323 
2025-05-01 17:17:59.272896: val_loss 2.7132 
2025-05-01 17:17:59.304151: PSNR 19.2616 
2025-05-01 17:17:59.304151: Epoch time: 1397.14 s 
2025-05-01 17:18:00.619337:  
2025-05-01 17:18:00.634963: Epoch 658 
2025-05-01 17:18:00.650596: Current learning rate: 5.43e-05 
2025-05-01 17:41:17.652379: train_loss 3.0825 
2025-05-01 17:41:17.668003: val_loss 2.5525 
2025-05-01 17:41:17.683632: PSNR 20.4299 
2025-05-01 17:41:17.699258: Epoch time: 1397.03 s 
2025-05-01 17:41:18.865330:  
2025-05-01 17:41:18.873770: Epoch 659 
2025-05-01 17:41:18.889420: Current learning rate: 5.4e-05 
2025-05-01 18:04:38.833738: train_loss 3.156 
2025-05-01 18:04:38.864990: val_loss 2.6833 
2025-05-01 18:04:38.880616: PSNR 19.3553 
2025-05-01 18:04:38.896240: Epoch time: 1399.98 s 
2025-05-01 18:04:39.756269:  
2025-05-01 18:04:39.771895: Epoch 660 
2025-05-01 18:04:39.787517: Current learning rate: 5.38e-05 
2025-05-01 18:28:01.713847: train_loss 2.968 
2025-05-01 18:28:01.729478: val_loss 2.5197 
2025-05-01 18:28:01.760730: PSNR 20.529 
2025-05-01 18:28:01.776354: Epoch time: 1401.96 s 
2025-05-01 18:28:01.807610: Yayy! New best EMA pseudo PSNR: 19.7896 
2025-05-01 18:28:10.207820:  
2025-05-01 18:28:10.239061: Epoch 661 
2025-05-01 18:28:10.254688: Current learning rate: 5.35e-05 
2025-05-01 18:51:35.296858: train_loss 2.9772 
2025-05-01 18:51:35.328108: val_loss 2.7777 
2025-05-01 18:51:35.343736: PSNR 19.4785 
2025-05-01 18:51:35.359366: Epoch time: 1405.09 s 
2025-05-01 18:51:36.768455:  
2025-05-01 18:51:36.784073: Epoch 662 
2025-05-01 18:51:36.802654: Current learning rate: 5.32e-05 
2025-05-01 19:15:03.014023: train_loss 2.9643 
2025-05-01 19:15:03.029645: val_loss 2.788 
2025-05-01 19:15:03.045275: PSNR 20.3342 
2025-05-01 19:15:03.060898: Epoch time: 1406.26 s 
2025-05-01 19:15:03.092163: Yayy! New best EMA pseudo PSNR: 19.8161 
2025-05-01 19:15:11.282593:  
2025-05-01 19:15:11.313837: Epoch 663 
2025-05-01 19:15:11.329462: Current learning rate: 5.29e-05 
2025-05-01 19:38:33.792172: train_loss 3.0321 
2025-05-01 19:38:33.807800: val_loss 2.8758 
2025-05-01 19:38:33.823421: PSNR 19.1001 
2025-05-01 19:38:33.854674: Epoch time: 1402.53 s 
2025-05-01 19:38:34.937125:  
2025-05-01 19:38:34.952756: Epoch 664 
2025-05-01 19:38:34.968377: Current learning rate: 5.26e-05 
2025-05-01 20:02:00.137026: train_loss 3.0251 
2025-05-01 20:02:00.152649: val_loss 2.8026 
2025-05-01 20:02:00.177860: PSNR 19.3126 
2025-05-01 20:02:00.193518: Epoch time: 1405.2 s 
2025-05-01 20:02:01.630091:  
2025-05-01 20:02:01.661369: Epoch 665 
2025-05-01 20:02:01.679093: Current learning rate: 5.24e-05 
2025-05-01 20:25:30.591615: train_loss 3.0447 
2025-05-01 20:25:30.607245: val_loss 2.8124 
2025-05-01 20:25:30.622866: PSNR 19.3868 
2025-05-01 20:25:30.654118: Epoch time: 1408.96 s 
2025-05-01 20:25:31.937971:  
2025-05-01 20:25:31.953597: Epoch 666 
2025-05-01 20:25:31.969232: Current learning rate: 5.21e-05 
2025-05-01 20:48:55.592169: train_loss 3.0785 
2025-05-01 20:48:55.623418: val_loss 2.6251 
2025-05-01 20:48:55.639043: PSNR 19.9008 
2025-05-01 20:48:55.654669: Epoch time: 1403.67 s 
2025-05-01 20:48:56.747909:  
2025-05-01 20:48:56.763525: Epoch 667 
2025-05-01 20:48:56.779152: Current learning rate: 5.18e-05 
2025-05-01 21:12:20.053683: train_loss 3.0309 
2025-05-01 21:12:20.084932: val_loss 2.7635 
2025-05-01 21:12:20.100566: PSNR 20.4425 
2025-05-01 21:12:20.116190: Epoch time: 1403.32 s 
2025-05-01 21:12:21.586924:  
2025-05-01 21:12:21.602549: Epoch 668 
2025-05-01 21:12:21.633801: Current learning rate: 5.15e-05 
2025-05-01 21:35:46.340709: train_loss 3.0775 
2025-05-01 21:35:46.356337: val_loss 2.7435 
2025-05-01 21:35:46.371969: PSNR 19.931 
2025-05-01 21:35:46.387588: Epoch time: 1404.77 s 
2025-05-01 21:35:47.471130:  
2025-05-01 21:35:47.486745: Epoch 669 
2025-05-01 21:35:47.502372: Current learning rate: 5.13e-05 
2025-05-01 21:59:14.323161: train_loss 3.1147 
2025-05-01 21:59:14.338794: val_loss 2.8416 
2025-05-01 21:59:14.354419: PSNR 19.1528 
2025-05-01 21:59:14.370036: Epoch time: 1406.87 s 
2025-05-01 21:59:15.738591:  
2025-05-01 21:59:15.769851: Epoch 670 
2025-05-01 21:59:15.785467: Current learning rate: 5.1e-05 
2025-05-01 22:22:37.212468: train_loss 3.0106 
2025-05-01 22:22:37.243728: val_loss 2.8037 
2025-05-01 22:22:37.274969: PSNR 19.4221 
2025-05-01 22:22:37.290602: Epoch time: 1401.47 s 
2025-05-01 22:22:38.643546:  
2025-05-01 22:22:38.659172: Epoch 671 
2025-05-01 22:22:38.690423: Current learning rate: 5.07e-05 
2025-05-01 22:46:09.475166: train_loss 2.9943 
2025-05-01 22:46:09.498622: val_loss 2.7972 
2025-05-01 22:46:09.515286: PSNR 19.1546 
2025-05-01 22:46:09.530913: Epoch time: 1410.83 s 
2025-05-01 22:46:10.480452:  
2025-05-01 22:46:10.496079: Epoch 672 
2025-05-01 22:46:10.511704: Current learning rate: 5.04e-05 
2025-05-01 23:09:40.214286: train_loss 3.0295 
2025-05-01 23:09:40.229907: val_loss 2.5646 
2025-05-01 23:09:40.245533: PSNR 20.279 
2025-05-01 23:09:40.261157: Epoch time: 1409.73 s 
2025-05-01 23:09:41.535839:  
2025-05-01 23:09:41.551478: Epoch 673 
2025-05-01 23:09:41.582715: Current learning rate: 5.02e-05 
2025-05-01 23:33:01.998830: train_loss 2.9305 
2025-05-01 23:33:02.030086: val_loss 2.7601 
2025-05-01 23:33:02.045710: PSNR 19.1879 
2025-05-01 23:33:02.061331: Epoch time: 1400.46 s 
2025-05-01 23:33:03.082674:  
2025-05-01 23:33:03.113925: Epoch 674 
2025-05-01 23:33:03.129553: Current learning rate: 4.99e-05 
2025-05-01 23:56:35.724198: train_loss 2.9044 
2025-05-01 23:56:35.755445: val_loss 2.8074 
2025-05-01 23:56:35.771092: PSNR 19.8773 
2025-05-01 23:56:35.786708: Epoch time: 1412.64 s 
2025-05-01 23:56:37.245903:  
2025-05-01 23:56:37.277176: Epoch 675 
2025-05-01 23:56:37.292803: Current learning rate: 4.96e-05 
2025-05-02 00:20:02.827574: train_loss 2.7302 
2025-05-02 00:20:02.843216: val_loss 2.7148 
2025-05-02 00:20:02.874466: PSNR 19.6608 
2025-05-02 00:20:02.890093: Epoch time: 1405.6 s 
2025-05-02 00:20:03.888721:  
2025-05-02 00:20:03.919973: Epoch 676 
2025-05-02 00:20:03.935596: Current learning rate: 4.93e-05 
2025-05-02 00:43:22.569525: train_loss 2.8008 
2025-05-02 00:43:22.600777: val_loss 2.5929 
2025-05-02 00:43:22.616408: PSNR 20.2166 
2025-05-02 00:43:22.616408: Epoch time: 1398.68 s 
2025-05-02 00:43:23.743759:  
2025-05-02 00:43:23.759378: Epoch 677 
2025-05-02 00:43:23.759378: Current learning rate: 4.91e-05 
2025-05-02 01:06:52.611583: train_loss 2.7789 
2025-05-02 01:06:52.642836: val_loss 2.5554 
2025-05-02 01:06:52.658467: PSNR 20.0512 
2025-05-02 01:06:52.658467: Epoch time: 1408.88 s 
2025-05-02 01:06:54.007916:  
2025-05-02 01:06:54.023542: Epoch 678 
2025-05-02 01:06:54.044717: Current learning rate: 4.88e-05 
2025-05-02 01:30:14.094207: train_loss 2.7454 
2025-05-02 01:30:14.109831: val_loss 2.8762 
2025-05-02 01:30:14.125458: PSNR 18.9292 
2025-05-02 01:30:14.141087: Epoch time: 1400.1 s 
2025-05-02 01:30:15.628334:  
2025-05-02 01:30:15.643951: Epoch 679 
2025-05-02 01:30:15.659577: Current learning rate: 4.85e-05 
2025-05-02 01:53:43.374960: train_loss 2.766 
2025-05-02 01:53:43.406211: val_loss 2.689 
2025-05-02 01:53:43.421842: PSNR 19.1764 
2025-05-02 01:53:43.453088: Epoch time: 1407.76 s 
2025-05-02 01:53:45.012544:  
2025-05-02 01:53:45.043787: Epoch 680 
2025-05-02 01:53:45.059422: Current learning rate: 4.83e-05 
2025-05-02 02:17:05.205115: train_loss 2.6493 
2025-05-02 02:17:05.220751: val_loss 2.7287 
2025-05-02 02:17:05.236382: PSNR 19.4537 
2025-05-02 02:17:05.251998: Epoch time: 1400.19 s 
2025-05-02 02:17:06.402667:  
2025-05-02 02:17:06.418293: Epoch 681 
2025-05-02 02:17:06.449563: Current learning rate: 4.8e-05 
2025-05-02 02:40:35.078014: train_loss 2.697 
2025-05-02 02:40:35.093646: val_loss 2.7836 
2025-05-02 02:40:35.109275: PSNR 19.6096 
2025-05-02 02:40:35.140529: Epoch time: 1408.68 s 
2025-05-02 02:40:36.561032:  
2025-05-02 02:40:36.576665: Epoch 682 
2025-05-02 02:40:36.592289: Current learning rate: 4.77e-05 
2025-05-02 03:03:56.150594: train_loss 2.7831 
2025-05-02 03:03:56.166221: val_loss 2.9208 
2025-05-02 03:03:56.181849: PSNR 18.569 
2025-05-02 03:03:56.208537: Epoch time: 1399.59 s 
2025-05-02 03:03:57.702054:  
2025-05-02 03:03:57.733301: Epoch 683 
2025-05-02 03:03:57.748927: Current learning rate: 4.75e-05 
2025-05-02 03:27:29.011157: train_loss 2.7529 
2025-05-02 03:27:29.042402: val_loss 2.6244 
2025-05-02 03:27:29.042402: PSNR 19.9777 
2025-05-02 03:27:29.058028: Epoch time: 1411.32 s 
2025-05-02 03:27:30.301493:  
2025-05-02 03:27:30.317119: Epoch 684 
2025-05-02 03:27:30.348369: Current learning rate: 4.72e-05 
2025-05-02 03:50:57.764350: train_loss 2.7618 
2025-05-02 03:50:57.795603: val_loss 2.8732 
2025-05-02 03:50:57.811237: PSNR 18.8845 
2025-05-02 03:50:57.811237: Epoch time: 1407.46 s 
2025-05-02 03:50:58.919693:  
2025-05-02 03:50:58.950955: Epoch 685 
2025-05-02 03:50:58.966570: Current learning rate: 4.69e-05 
2025-05-02 04:14:19.040694: train_loss 2.6831 
2025-05-02 04:14:19.056313: val_loss 2.9025 
2025-05-02 04:14:19.088301: PSNR 18.9123 
2025-05-02 04:14:19.090347: Epoch time: 1400.12 s 
2025-05-02 04:14:20.447049:  
2025-05-02 04:14:20.462672: Epoch 686 
2025-05-02 04:14:20.478303: Current learning rate: 4.67e-05 
2025-05-02 04:37:46.814609: train_loss 2.7673 
2025-05-02 04:37:46.845860: val_loss 2.5349 
2025-05-02 04:37:46.845860: PSNR 20.2192 
2025-05-02 04:37:46.861485: Epoch time: 1406.38 s 
2025-05-02 04:37:47.748608:  
2025-05-02 04:37:47.764224: Epoch 687 
2025-05-02 04:37:47.789357: Current learning rate: 4.64e-05 
2025-05-02 05:01:16.591224: train_loss 2.8137 
2025-05-02 05:01:16.606853: val_loss 2.7886 
2025-05-02 05:01:16.638113: PSNR 19.1229 
2025-05-02 05:01:16.638113: Epoch time: 1408.86 s 
2025-05-02 05:01:17.691436:  
2025-05-02 05:01:17.707062: Epoch 688 
2025-05-02 05:01:17.722688: Current learning rate: 4.61e-05 
2025-05-02 05:24:45.702213: train_loss 2.749 
2025-05-02 05:24:45.733466: val_loss 2.7318 
2025-05-02 05:24:45.764710: PSNR 19.4419 
2025-05-02 05:24:45.780337: Epoch time: 1408.01 s 
2025-05-02 05:24:47.216777:  
2025-05-02 05:24:47.248031: Epoch 689 
2025-05-02 05:24:47.263658: Current learning rate: 4.59e-05 
2025-05-02 05:48:12.364957: train_loss 2.834 
2025-05-02 05:48:12.380577: val_loss 2.6723 
2025-05-02 05:48:12.396209: PSNR 19.0772 
2025-05-02 05:48:12.411825: Epoch time: 1405.16 s 
2025-05-02 05:48:13.499528:  
2025-05-02 05:48:13.504672: Epoch 690 
2025-05-02 05:48:13.520313: Current learning rate: 4.56e-05 
2025-05-02 06:11:39.022590: train_loss 2.6171 
2025-05-02 06:11:39.038223: val_loss 2.7697 
2025-05-02 06:11:39.053847: PSNR 19.4462 
2025-05-02 06:11:39.085094: Epoch time: 1405.53 s 
2025-05-02 06:11:40.947260:  
2025-05-02 06:11:40.978510: Epoch 691 
2025-05-02 06:11:40.994134: Current learning rate: 4.53e-05 
2025-05-02 06:35:12.483742: train_loss 2.8482 
2025-05-02 06:35:12.514995: val_loss 2.6339 
2025-05-02 06:35:12.530626: PSNR 19.861 
2025-05-02 06:35:12.548628: Epoch time: 1411.54 s 
2025-05-02 06:35:13.998026:  
2025-05-02 06:35:14.029267: Epoch 692 
2025-05-02 06:35:14.044894: Current learning rate: 4.51e-05 
2025-05-02 06:58:45.037879: train_loss 2.6669 
2025-05-02 06:58:45.053506: val_loss 2.8084 
2025-05-02 06:58:45.084759: PSNR 19.2638 
2025-05-02 06:58:45.100401: Epoch time: 1411.04 s 
2025-05-02 06:58:46.687683:  
2025-05-02 06:58:46.718927: Epoch 693 
2025-05-02 06:58:46.718927: Current learning rate: 4.48e-05 
2025-05-02 07:22:13.817617: train_loss 2.7321 
2025-05-02 07:22:13.833252: val_loss 2.5537 
2025-05-02 07:22:13.864496: PSNR 20.4189 
2025-05-02 07:22:13.880127: Epoch time: 1407.13 s 
2025-05-02 07:22:14.904799:  
2025-05-02 07:22:14.936033: Epoch 694 
2025-05-02 07:22:14.951661: Current learning rate: 4.45e-05 
2025-05-02 07:45:44.931671: train_loss 2.7925 
2025-05-02 07:45:44.947299: val_loss 2.8003 
2025-05-02 07:45:44.986552: PSNR 19.2871 
2025-05-02 07:45:44.987569: Epoch time: 1410.03 s 
2025-05-02 07:45:46.191477:  
2025-05-02 07:45:46.207101: Epoch 695 
2025-05-02 07:45:46.222726: Current learning rate: 4.43e-05 
2025-05-02 08:09:17.416441: train_loss 2.7176 
2025-05-02 08:09:17.432083: val_loss 2.7234 
2025-05-02 08:09:17.468113: PSNR 19.2764 
2025-05-02 08:09:17.472258: Epoch time: 1411.24 s 
2025-05-02 08:09:18.696283:  
2025-05-02 08:09:18.711892: Epoch 696 
2025-05-02 08:09:18.727517: Current learning rate: 4.4e-05 
2025-05-02 08:32:38.838281: train_loss 2.6918 
2025-05-02 08:32:38.853920: val_loss 2.8886 
2025-05-02 08:32:38.869535: PSNR 18.6893 
2025-05-02 08:32:38.885159: Epoch time: 1400.16 s 
2025-05-02 08:32:40.401596:  
2025-05-02 08:32:40.417232: Epoch 697 
2025-05-02 08:32:40.432854: Current learning rate: 4.38e-05 
2025-05-02 08:56:06.946986: train_loss 2.6694 
2025-05-02 08:56:06.962614: val_loss 2.6615 
2025-05-02 08:56:06.993863: PSNR 19.8852 
2025-05-02 08:56:07.009492: Epoch time: 1406.56 s 
2025-05-02 08:56:08.112372:  
2025-05-02 08:56:08.128010: Epoch 698 
2025-05-02 08:56:08.143630: Current learning rate: 4.35e-05 
2025-05-02 09:19:34.831211: train_loss 2.764 
2025-05-02 09:19:34.862464: val_loss 2.5324 
2025-05-02 09:19:34.878092: PSNR 19.7315 
2025-05-02 09:19:34.893717: Epoch time: 1406.73 s 
2025-05-02 09:19:35.865092:  
2025-05-02 09:19:35.896343: Epoch 699 
2025-05-02 09:19:35.911970: Current learning rate: 4.32e-05 
2025-05-02 09:43:02.044531: train_loss 2.7221 
2025-05-02 09:43:02.060158: val_loss 2.772 
2025-05-02 09:43:02.075788: PSNR 19.4952 
2025-05-02 09:43:02.091414: Epoch time: 1406.18 s 
2025-05-02 09:43:10.940278:  
2025-05-02 09:43:10.971513: Epoch 700 
2025-05-02 09:43:11.002765: Current learning rate: 4.3e-05 
2025-05-02 10:06:39.512868: train_loss 2.7239 
2025-05-02 10:06:39.544133: val_loss 2.8964 
2025-05-02 10:06:39.559759: PSNR 19.1729 
2025-05-02 10:06:39.575384: Epoch time: 1408.57 s 
2025-05-02 10:06:40.836428:  
2025-05-02 10:06:40.876771: Epoch 701 
2025-05-02 10:06:40.876771: Current learning rate: 4.27e-05 
2025-05-02 10:30:10.105819: train_loss 2.7515 
2025-05-02 10:30:10.121449: val_loss 2.6158 
2025-05-02 10:30:10.137073: PSNR 20.0368 
2025-05-02 10:30:10.152698: Epoch time: 1409.29 s 
2025-05-02 10:30:11.255294:  
2025-05-02 10:30:11.270921: Epoch 702 
2025-05-02 10:30:11.286551: Current learning rate: 4.25e-05 
2025-05-02 10:53:37.538939: train_loss 2.7196 
2025-05-02 10:53:37.570190: val_loss 2.6575 
2025-05-02 10:53:37.585819: PSNR 20.1726 
2025-05-02 10:53:37.601442: Epoch time: 1406.28 s 
2025-05-02 10:53:38.986138:  
2025-05-02 10:53:39.033015: Epoch 703 
2025-05-02 10:53:39.048643: Current learning rate: 4.22e-05 
2025-05-02 11:17:03.936497: train_loss 2.7797 
2025-05-02 11:17:03.983375: val_loss 2.9962 
2025-05-02 11:17:03.999017: PSNR 18.4096 
2025-05-02 11:17:04.030255: Epoch time: 1404.95 s 
2025-05-02 11:17:05.570636:  
2025-05-02 11:17:05.586259: Epoch 704 
2025-05-02 11:17:05.617511: Current learning rate: 4.19e-05 
2025-05-02 11:40:28.958639: train_loss 2.7131 
2025-05-02 11:40:28.989894: val_loss 2.828 
2025-05-02 11:40:29.005524: PSNR 19.336 
2025-05-02 11:40:29.021154: Epoch time: 1403.4 s 
2025-05-02 11:40:30.520181:  
2025-05-02 11:40:30.551414: Epoch 705 
2025-05-02 11:40:30.582666: Current learning rate: 4.17e-05 
2025-05-02 12:03:50.077002: train_loss 2.7071 
2025-05-02 12:03:50.108254: val_loss 2.6914 
2025-05-02 12:03:50.123884: PSNR 19.4733 
2025-05-02 12:03:50.139509: Epoch time: 1399.56 s 
2025-05-02 12:03:51.211157:  
2025-05-02 12:03:51.242407: Epoch 706 
2025-05-02 12:03:51.258040: Current learning rate: 4.14e-05 
2025-05-02 12:27:16.102244: train_loss 2.7609 
2025-05-02 12:27:16.133489: val_loss 2.6485 
2025-05-02 12:27:16.164745: PSNR 19.7359 
2025-05-02 12:27:16.180372: Epoch time: 1404.89 s 
2025-05-02 12:27:17.941873:  
2025-05-02 12:27:17.973117: Epoch 707 
2025-05-02 12:27:17.988738: Current learning rate: 4.12e-05 
2025-05-02 12:50:36.338924: train_loss 2.7537 
2025-05-02 12:50:36.370172: val_loss 2.7703 
2025-05-02 12:50:36.370172: PSNR 19.3205 
2025-05-02 12:50:36.385798: Epoch time: 1398.4 s 
2025-05-02 12:50:37.381823:  
2025-05-02 12:50:37.397458: Epoch 708 
2025-05-02 12:50:37.413078: Current learning rate: 4.09e-05 
2025-05-02 13:14:00.709652: train_loss 2.7234 
2025-05-02 13:14:00.725283: val_loss 2.584 
2025-05-02 13:14:00.740908: PSNR 19.9123 
2025-05-02 13:14:00.740908: Epoch time: 1403.33 s 
2025-05-02 13:14:02.167077:  
2025-05-02 13:14:02.182702: Epoch 709 
2025-05-02 13:14:02.198328: Current learning rate: 4.07e-05 
