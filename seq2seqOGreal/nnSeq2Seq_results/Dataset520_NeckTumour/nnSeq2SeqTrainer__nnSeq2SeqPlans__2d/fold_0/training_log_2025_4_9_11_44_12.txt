
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-09 11:44:18.444521: unpacking dataset... 
2025-04-09 11:44:52.626116: unpacking done... 
2025-04-09 11:44:52.645150: do_dummy_2d_data_aug: False 
2025-04-09 11:44:52.659151: Using splits from existing split file: C:\Users\P095789\OneDrive - Amsterdam UMC\Documenten\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-09 11:44:52.682060: The split file contains 5 splits. 
2025-04-09 11:44:52.695059: Desired fold for training: 0 
2025-04-09 11:44:52.708885: This split has 4 training and 1 validation cases. 
2025-04-09 11:44:53.673142: Unable to plot network architecture: 
2025-04-09 11:44:53.684140: No module named 'hiddenlayer' 
2025-04-09 11:44:53.750879:  
2025-04-09 11:44:53.763011: Epoch 0 
2025-04-09 11:44:53.774014: Current learning rate: 1e-07 
2025-04-09 11:50:30.227279: train_loss 10.5115 
2025-04-09 11:50:30.246364: val_loss 3.6604 
2025-04-09 11:50:30.261019: PSNR 13.5314 
2025-04-09 11:50:30.276025: Epoch time: 336.47 s 
2025-04-09 11:50:30.289018: Yayy! New best EMA pseudo PSNR: 13.5314 
2025-04-09 11:50:35.646116:  
2025-04-09 11:50:35.659119: Epoch 1 
2025-04-09 11:50:35.672045: Current learning rate: 2.01e-05 
2025-04-09 12:02:51.619498: train_loss 8.9528 
2025-04-09 12:02:51.655775: val_loss 3.7732 
2025-04-09 12:02:51.683013: PSNR 15.2036 
2025-04-09 12:02:51.698525: Epoch time: 735.98 s 
2025-04-09 12:02:51.714164: Yayy! New best EMA pseudo PSNR: 13.6986 
2025-04-09 12:02:57.526344:  
2025-04-09 12:02:57.541973: Epoch 2 
2025-04-09 12:02:57.557596: Current learning rate: 4.01e-05 
2025-04-09 12:15:19.605665: train_loss 9.3474 
2025-04-09 12:15:19.636914: val_loss 3.4264 
2025-04-09 12:15:19.653252: PSNR 16.3664 
2025-04-09 12:15:19.668856: Epoch time: 742.09 s 
2025-04-09 12:15:19.684490: Yayy! New best EMA pseudo PSNR: 13.9654 
2025-04-09 12:15:26.699412:  
2025-04-09 12:15:26.715038: Epoch 3 
2025-04-09 12:15:26.730664: Current learning rate: 6.01e-05 
2025-04-09 12:28:02.209086: train_loss 9.5122 
2025-04-09 12:28:02.224711: val_loss 3.4233 
2025-04-09 12:28:02.247062: PSNR 16.5874 
2025-04-09 12:28:02.264739: Epoch time: 755.51 s 
2025-04-09 12:28:02.264739: Yayy! New best EMA pseudo PSNR: 14.2276 
2025-04-09 12:28:08.275046:  
2025-04-09 12:28:08.290673: Epoch 4 
2025-04-09 12:28:08.306299: Current learning rate: 8.01e-05 
2025-04-09 12:40:40.852023: train_loss 8.961 
2025-04-09 12:40:40.877981: val_loss 2.9345 
2025-04-09 12:40:40.893615: PSNR 16.7987 
2025-04-09 12:40:40.925449: Epoch time: 752.58 s 
2025-04-09 12:40:40.925449: Yayy! New best EMA pseudo PSNR: 14.4847 
2025-04-09 12:40:48.398730:  
2025-04-09 12:40:48.429995: Epoch 5 
2025-04-09 12:40:48.445618: Current learning rate: 0.0001001 
2025-04-09 12:53:20.698327: train_loss 8.4382 
2025-04-09 12:53:20.713945: val_loss 2.7257 
2025-04-09 12:53:20.737558: PSNR 17.0163 
2025-04-09 12:53:20.739657: Epoch time: 752.3 s 
2025-04-09 12:53:20.755293: Yayy! New best EMA pseudo PSNR: 14.7378 
2025-04-09 12:53:26.491257:  
2025-04-09 12:53:26.506882: Epoch 6 
2025-04-09 12:53:26.522508: Current learning rate: 0.00012 
2025-04-09 13:05:52.254250: train_loss 7.2164 
2025-04-09 13:05:52.269880: val_loss 2.5312 
2025-04-09 13:05:52.285500: PSNR 17.7985 
2025-04-09 13:05:52.301124: Epoch time: 745.76 s 
2025-04-09 13:05:52.301124: Yayy! New best EMA pseudo PSNR: 15.0439 
2025-04-09 13:05:57.515944:  
2025-04-09 13:05:57.531571: Epoch 7 
2025-04-09 13:05:57.547195: Current learning rate: 0.00014 
2025-04-09 13:18:29.782789: train_loss 6.6562 
2025-04-09 13:29:23.783510: val_loss 2.5073 
2025-04-09 13:29:23.799142: PSNR 17.9265 
2025-04-09 13:29:23.814764: Epoch time: 752.27 s 
2025-04-09 13:29:23.830393: Yayy! New best EMA pseudo PSNR: 15.3322 
2025-04-09 13:29:32.861641:  
2025-04-09 13:29:32.877263: Epoch 8 
2025-04-09 13:29:32.892889: Current learning rate: 0.00016 
2025-04-09 13:42:12.175053: train_loss 6.2644 
2025-04-09 13:42:12.190678: val_loss 2.71 
2025-04-09 13:42:12.206304: PSNR 17.5677 
2025-04-09 13:42:12.237558: Epoch time: 759.33 s 
2025-04-09 13:42:12.253180: Yayy! New best EMA pseudo PSNR: 15.5557 
2025-04-09 13:42:19.893790:  
2025-04-09 13:42:19.909419: Epoch 9 
2025-04-09 13:42:19.925042: Current learning rate: 0.00018 
2025-04-09 13:55:04.808941: train_loss 5.9504 
2025-04-09 13:55:04.812004: val_loss 2.5032 
2025-04-09 13:55:04.827641: PSNR 18.2471 
2025-04-09 13:55:04.843272: Epoch time: 764.92 s 
2025-04-09 13:55:04.858890: Yayy! New best EMA pseudo PSNR: 15.8249 
2025-04-09 13:55:11.622750:  
2025-04-09 13:55:11.638374: Epoch 10 
2025-04-09 13:55:11.654002: Current learning rate: 0.0002 
2025-04-09 14:07:47.618970: train_loss 6.0338 
2025-04-09 14:07:47.634601: val_loss 2.3861 
2025-04-09 14:07:47.659451: PSNR 18.8379 
2025-04-09 14:07:47.659451: Epoch time: 756.01 s 
2025-04-09 14:07:47.675086: Yayy! New best EMA pseudo PSNR: 16.1262 
2025-04-09 14:07:53.603726:  
2025-04-09 14:07:53.619348: Epoch 11 
2025-04-09 14:07:53.619348: Current learning rate: 0.0002 
2025-04-09 14:20:47.865910: train_loss 5.8523 
2025-04-09 14:20:47.897582: val_loss 2.6494 
2025-04-09 14:20:47.913215: PSNR 17.6669 
2025-04-09 14:20:47.927729: Epoch time: 774.26 s 
2025-04-09 14:20:47.943381: Yayy! New best EMA pseudo PSNR: 16.2802 
2025-04-09 14:20:54.179508:  
2025-04-09 14:20:54.194509: Epoch 12 
2025-04-09 14:20:54.206510: Current learning rate: 0.0002 
2025-04-09 14:33:47.922882: train_loss 5.6338 
2025-04-09 14:33:47.949353: val_loss 2.6783 
2025-04-09 14:33:47.966323: PSNR 18.1118 
2025-04-09 14:33:47.978327: Epoch time: 773.76 s 
2025-04-09 14:33:47.989326: Yayy! New best EMA pseudo PSNR: 16.4634 
2025-04-09 14:33:53.602009:  
2025-04-09 14:33:53.614011: Epoch 13 
2025-04-09 14:33:53.626383: Current learning rate: 0.0002 
2025-04-09 14:46:45.004907: train_loss 5.5444 
2025-04-09 14:46:45.017907: val_loss 2.5577 
2025-04-09 14:46:45.043981: PSNR 18.4336 
2025-04-09 14:46:45.059926: Epoch time: 771.41 s 
2025-04-09 14:46:45.074930: Yayy! New best EMA pseudo PSNR: 16.6604 
2025-04-09 14:46:51.101800:  
2025-04-09 14:46:51.114837: Epoch 14 
2025-04-09 14:46:51.128696: Current learning rate: 0.0002 
2025-04-09 14:59:52.898307: train_loss 5.5656 
2025-04-09 14:59:52.913928: val_loss 2.5624 
2025-04-09 14:59:52.943603: PSNR 18.3763 
2025-04-09 14:59:52.961249: Epoch time: 781.8 s 
2025-04-09 14:59:52.976889: Yayy! New best EMA pseudo PSNR: 16.832 
2025-04-09 14:59:59.788930:  
2025-04-09 14:59:59.788930: Epoch 15 
2025-04-09 14:59:59.804560: Current learning rate: 0.0002 
2025-04-09 15:13:01.367786: train_loss 5.4346 
2025-04-09 15:13:01.367786: val_loss 2.6662 
2025-04-09 15:13:01.402625: PSNR 18.0433 
2025-04-09 15:13:01.404642: Epoch time: 781.59 s 
2025-04-09 15:13:01.430286: Yayy! New best EMA pseudo PSNR: 16.9531 
2025-04-09 15:13:08.164681:  
2025-04-09 15:13:08.180302: Epoch 16 
2025-04-09 15:13:08.180302: Current learning rate: 0.0002 
2025-04-09 15:26:07.649697: train_loss 5.4101 
2025-04-09 15:26:07.680948: val_loss 2.5888 
2025-04-09 15:26:07.696573: PSNR 18.3045 
2025-04-09 15:26:07.712199: Epoch time: 779.49 s 
2025-04-09 15:26:07.712199: Yayy! New best EMA pseudo PSNR: 17.0883 
2025-04-09 15:26:13.852809:  
2025-04-09 15:26:13.868435: Epoch 17 
2025-04-09 15:26:13.884060: Current learning rate: 0.0002 
2025-04-09 15:39:14.103406: train_loss 5.4873 
2025-04-09 15:39:14.134660: val_loss 2.5911 
2025-04-09 15:39:14.153710: PSNR 18.3776 
2025-04-09 15:39:14.153710: Epoch time: 780.25 s 
2025-04-09 15:39:14.169347: Yayy! New best EMA pseudo PSNR: 17.2172 
2025-04-09 15:39:21.462800:  
2025-04-09 15:39:21.478426: Epoch 18 
2025-04-09 15:39:21.494054: Current learning rate: 0.0002 
2025-04-09 15:52:17.979331: train_loss 5.3786 
2025-04-09 16:42:48.043875: val_loss 2.5536 
2025-04-09 16:42:48.059506: PSNR 18.4671 
2025-04-09 16:42:48.075128: Epoch time: 776.52 s 
2025-04-09 16:42:48.090756: Yayy! New best EMA pseudo PSNR: 17.3422 
2025-04-09 16:42:56.981369:  
2025-04-09 16:42:57.012626: Epoch 19 
2025-04-09 16:42:57.028259: Current learning rate: 0.0002 
2025-04-09 16:56:33.481963: train_loss 5.3598 
2025-04-09 16:56:33.497586: val_loss 2.5644 
2025-04-09 16:56:33.528839: PSNR 18.1658 
2025-04-09 16:56:33.544464: Epoch time: 816.5 s 
2025-04-09 16:56:33.560090: Yayy! New best EMA pseudo PSNR: 17.4245 
2025-04-09 16:56:40.263373:  
2025-04-09 16:56:40.278894: Epoch 20 
2025-04-09 16:56:40.294538: Current learning rate: 0.0002 
2025-04-09 17:09:51.551124: train_loss 5.4028 
2025-04-09 17:09:51.566752: val_loss 2.4992 
2025-04-09 17:09:51.594471: PSNR 18.623 
2025-04-09 17:09:51.594471: Epoch time: 791.3 s 
2025-04-09 17:09:51.610104: Yayy! New best EMA pseudo PSNR: 17.5444 
2025-04-09 17:09:57.488708:  
2025-04-09 17:09:57.504336: Epoch 21 
2025-04-09 17:09:57.519961: Current learning rate: 0.0001999 
2025-04-09 17:22:52.364594: train_loss 5.3524 
2025-04-09 17:22:52.380219: val_loss 2.5714 
2025-04-09 17:22:52.395844: PSNR 18.524 
2025-04-09 17:22:52.411469: Epoch time: 774.88 s 
2025-04-09 17:22:52.427096: Yayy! New best EMA pseudo PSNR: 17.6424 
2025-04-09 17:22:57.958469:  
2025-04-09 17:22:57.974104: Epoch 22 
2025-04-09 17:22:57.974104: Current learning rate: 0.0001999 
2025-04-09 17:35:52.129764: train_loss 5.3421 
2025-04-09 17:35:52.145389: val_loss 2.5357 
2025-04-09 17:35:52.161013: PSNR 19.1265 
2025-04-09 17:35:52.176643: Epoch time: 774.17 s 
2025-04-09 17:35:52.192265: Yayy! New best EMA pseudo PSNR: 17.7908 
2025-04-09 17:35:58.145761:  
2025-04-09 17:35:58.161392: Epoch 23 
2025-04-09 17:35:58.177013: Current learning rate: 0.0001999 
2025-04-09 17:48:55.513919: train_loss 5.3139 
2025-04-09 17:48:55.545169: val_loss 2.5035 
2025-04-09 17:48:55.566688: PSNR 18.9512 
2025-04-09 17:48:55.584358: Epoch time: 777.37 s 
2025-04-09 17:48:55.599984: Yayy! New best EMA pseudo PSNR: 17.9068 
2025-04-09 17:49:01.968763:  
2025-04-09 17:49:02.000011: Epoch 24 
2025-04-09 17:49:02.015636: Current learning rate: 0.0001999 
2025-04-09 18:02:01.428833: train_loss 5.2039 
2025-04-09 18:02:01.444463: val_loss 2.4344 
2025-04-09 18:02:01.474308: PSNR 19.0321 
2025-04-09 18:02:01.489943: Epoch time: 779.46 s 
2025-04-09 18:02:01.505571: Yayy! New best EMA pseudo PSNR: 18.0193 
2025-04-09 18:02:08.112182:  
2025-04-09 18:02:08.127807: Epoch 25 
2025-04-09 18:02:08.143442: Current learning rate: 0.0001999 
2025-04-09 18:15:03.927796: train_loss 5.2752 
2025-04-09 18:15:03.943421: val_loss 2.6852 
2025-04-09 18:15:03.970445: PSNR 18.3412 
2025-04-09 18:15:03.972649: Epoch time: 775.83 s 
2025-04-09 18:15:03.988284: Yayy! New best EMA pseudo PSNR: 18.0515 
2025-04-09 18:15:10.440068:  
2025-04-09 18:15:10.455692: Epoch 26 
2025-04-09 18:15:10.471323: Current learning rate: 0.0001999 
2025-04-09 18:28:06.627779: train_loss 5.309 
2025-04-09 18:28:06.659039: val_loss 2.5437 
2025-04-09 18:28:06.688048: PSNR 18.5475 
2025-04-09 18:28:06.689053: Epoch time: 776.19 s 
2025-04-09 18:28:06.704691: Yayy! New best EMA pseudo PSNR: 18.1011 
2025-04-09 18:28:13.889194:  
2025-04-09 18:28:13.889194: Epoch 27 
2025-04-09 18:28:13.904819: Current learning rate: 0.0001999 
2025-04-09 18:41:07.679001: train_loss 5.3183 
2025-04-09 18:41:07.694625: val_loss 2.2867 
2025-04-09 18:41:07.718549: PSNR 19.6775 
2025-04-09 18:41:07.721674: Epoch time: 773.81 s 
2025-04-09 18:41:07.737311: Yayy! New best EMA pseudo PSNR: 18.2588 
2025-04-09 18:41:14.148841:  
2025-04-09 18:41:14.164468: Epoch 28 
2025-04-09 18:41:14.180093: Current learning rate: 0.0001998 
2025-04-09 18:54:09.110579: train_loss 5.3559 
2025-04-09 18:54:09.126211: val_loss 2.6093 
2025-04-09 18:54:09.145529: PSNR 18.6107 
2025-04-09 18:54:09.161163: Epoch time: 774.96 s 
2025-04-09 18:54:09.161163: Yayy! New best EMA pseudo PSNR: 18.294 
2025-04-09 18:54:16.318455:  
2025-04-09 18:54:16.334081: Epoch 29 
2025-04-09 18:54:16.349708: Current learning rate: 0.0001998 
2025-04-09 19:07:13.165888: train_loss 5.2983 
2025-04-09 19:07:13.197577: val_loss 2.3973 
2025-04-09 19:07:13.213497: PSNR 19.0024 
2025-04-09 19:07:13.229135: Epoch time: 776.85 s 
2025-04-09 19:07:13.244760: Yayy! New best EMA pseudo PSNR: 18.3648 
2025-04-09 19:07:19.186187:  
2025-04-09 19:07:19.201816: Epoch 30 
2025-04-09 19:07:19.217436: Current learning rate: 0.0001998 
2025-04-09 19:20:15.284472: train_loss 5.3425 
2025-04-09 19:20:15.300096: val_loss 2.6264 
2025-04-09 19:20:15.330132: PSNR 18.6165 
2025-04-09 19:20:15.346776: Epoch time: 776.1 s 
2025-04-09 19:20:15.362405: Yayy! New best EMA pseudo PSNR: 18.39 
2025-04-09 19:20:21.390799:  
2025-04-09 19:20:21.406425: Epoch 31 
2025-04-09 19:20:21.422050: Current learning rate: 0.0001998 
2025-04-09 19:33:17.715385: train_loss 5.2121 
2025-04-09 19:33:17.731011: val_loss 2.3411 
2025-04-09 19:33:17.756268: PSNR 19.094 
2025-04-09 19:33:17.772908: Epoch time: 776.32 s 
2025-04-09 19:33:17.788535: Yayy! New best EMA pseudo PSNR: 18.4604 
2025-04-09 19:33:23.306598:  
2025-04-09 19:33:23.322217: Epoch 32 
2025-04-09 19:33:23.337839: Current learning rate: 0.0001998 
2025-04-09 19:46:14.253333: train_loss 5.2246 
2025-04-09 19:46:14.284583: val_loss 2.5877 
2025-04-09 19:46:14.308775: PSNR 18.6661 
2025-04-09 19:46:14.311844: Epoch time: 770.95 s 
2025-04-09 19:46:14.327480: Yayy! New best EMA pseudo PSNR: 18.481 
2025-04-09 19:46:20.433874:  
2025-04-09 19:46:20.449501: Epoch 33 
2025-04-09 19:46:20.466502: Current learning rate: 0.0001997 
2025-04-09 19:59:12.425267: train_loss 5.2003 
2025-04-09 19:59:12.440895: val_loss 2.5154 
2025-04-09 19:59:12.459404: PSNR 18.6256 
2025-04-09 19:59:12.475041: Epoch time: 771.99 s 
2025-04-09 19:59:12.506289: Yayy! New best EMA pseudo PSNR: 18.4954 
2025-04-09 19:59:18.814028:  
2025-04-09 19:59:18.829655: Epoch 34 
2025-04-09 19:59:18.860918: Current learning rate: 0.0001997 
2025-04-09 20:12:12.179730: train_loss 5.3028 
2025-04-09 20:12:12.195354: val_loss 2.4811 
2025-04-09 20:12:12.219265: PSNR 18.8228 
2025-04-09 20:12:12.222448: Epoch time: 773.37 s 
2025-04-09 20:12:12.238084: Yayy! New best EMA pseudo PSNR: 18.5282 
2025-04-09 20:12:18.145475:  
2025-04-09 20:12:18.161094: Epoch 35 
2025-04-09 20:12:18.161094: Current learning rate: 0.0001997 
2025-04-09 20:25:10.466239: train_loss 5.2331 
2025-04-09 20:25:10.466239: val_loss 2.5527 
2025-04-09 20:25:10.493846: PSNR 18.6464 
2025-04-09 20:25:10.509482: Epoch time: 772.32 s 
2025-04-09 20:25:10.525107: Yayy! New best EMA pseudo PSNR: 18.54 
2025-04-09 20:25:16.016399:  
2025-04-09 20:25:16.032027: Epoch 36 
2025-04-09 20:25:16.063272: Current learning rate: 0.0001997 
2025-04-09 20:38:08.187050: train_loss 5.0835 
2025-04-09 20:38:08.202674: val_loss 2.5395 
2025-04-09 20:38:08.230336: PSNR 19.4606 
2025-04-09 20:38:08.245970: Epoch time: 772.17 s 
2025-04-09 20:38:08.261597: Yayy! New best EMA pseudo PSNR: 18.632 
2025-04-09 20:38:14.497602:  
2025-04-09 20:38:14.513224: Epoch 37 
2025-04-09 20:38:14.528848: Current learning rate: 0.0001996 
2025-04-09 20:51:09.659127: train_loss 5.134 
2025-04-09 20:51:09.671045: val_loss 2.6785 
2025-04-09 20:51:09.686680: PSNR 18.814 
2025-04-09 20:51:09.702307: Epoch time: 775.18 s 
2025-04-09 20:51:09.717935: Yayy! New best EMA pseudo PSNR: 18.6502 
2025-04-09 20:51:15.551652:  
2025-04-09 20:51:15.567276: Epoch 38 
2025-04-09 20:51:15.582901: Current learning rate: 0.0001996 
2025-04-09 21:04:08.386021: train_loss 5.2762 
2025-04-09 21:04:08.401649: val_loss 2.5717 
2025-04-09 21:04:08.429194: PSNR 18.8356 
2025-04-09 21:04:08.429194: Epoch time: 772.85 s 
2025-04-09 21:04:08.444834: Yayy! New best EMA pseudo PSNR: 18.6688 
2025-04-09 21:04:14.935137:  
2025-04-09 21:04:14.950763: Epoch 39 
2025-04-09 21:04:14.966384: Current learning rate: 0.0001996 
2025-04-09 21:17:14.721497: train_loss 5.1354 
2025-04-09 21:17:14.737120: val_loss 2.5765 
2025-04-09 21:17:14.762491: PSNR 18.9538 
2025-04-09 21:17:14.766605: Epoch time: 779.79 s 
2025-04-09 21:17:14.782241: Yayy! New best EMA pseudo PSNR: 18.6973 
2025-04-09 21:17:20.876997:  
2025-04-09 21:17:20.892618: Epoch 40 
2025-04-09 21:17:20.892618: Current learning rate: 0.0001996 
2025-04-09 21:30:15.383222: train_loss 5.2245 
2025-04-09 21:30:15.398846: val_loss 2.6396 
2025-04-09 21:30:15.418222: PSNR 18.874 
2025-04-09 21:30:15.433857: Epoch time: 774.52 s 
2025-04-09 21:30:15.449484: Yayy! New best EMA pseudo PSNR: 18.715 
2025-04-09 21:30:21.411894:  
2025-04-09 21:30:21.427519: Epoch 41 
2025-04-09 21:30:21.443142: Current learning rate: 0.0001995 
2025-04-09 21:43:16.995742: train_loss 5.1647 
2025-04-09 21:43:17.011367: val_loss 2.814 
2025-04-09 21:43:17.032801: PSNR 18.0979 
2025-04-09 21:43:17.048441: Epoch time: 775.58 s 
2025-04-09 21:43:18.017189:  
2025-04-09 21:43:18.032813: Epoch 42 
2025-04-09 21:43:18.048439: Current learning rate: 0.0001995 
2025-04-09 21:56:11.250829: train_loss 5.1149 
2025-04-09 21:56:11.266455: val_loss 2.5314 
2025-04-09 21:56:11.297151: PSNR 18.6675 
2025-04-09 21:56:11.312784: Epoch time: 773.25 s 
2025-04-09 21:56:12.098259:  
2025-04-09 21:56:12.129509: Epoch 43 
2025-04-09 21:56:12.145139: Current learning rate: 0.0001995 
2025-04-09 22:09:06.146539: train_loss 5.2002 
2025-04-09 22:09:06.162154: val_loss 2.4554 
2025-04-09 22:09:06.188500: PSNR 18.9785 
2025-04-09 22:09:06.204135: Epoch time: 774.05 s 
2025-04-09 22:09:07.124403:  
2025-04-09 22:09:07.140033: Epoch 44 
2025-04-09 22:09:07.155654: Current learning rate: 0.0001994 
2025-04-09 22:22:03.200726: train_loss 5.2241 
2025-04-09 22:22:03.231978: val_loss 2.5823 
2025-04-09 22:22:03.253676: PSNR 18.5194 
2025-04-09 22:22:03.269313: Epoch time: 776.08 s 
2025-04-09 22:22:04.372361:  
2025-04-09 22:22:04.403612: Epoch 45 
2025-04-09 22:22:04.403612: Current learning rate: 0.0001994 
2025-04-09 22:34:57.759896: train_loss 5.1178 
2025-04-09 22:34:57.775524: val_loss 2.5563 
2025-04-09 22:34:57.791782: PSNR 18.9594 
2025-04-09 22:34:57.799370: Epoch time: 773.39 s 
2025-04-09 22:34:58.873753:  
2025-04-09 22:34:58.889376: Epoch 46 
2025-04-09 22:34:58.905005: Current learning rate: 0.0001994 
2025-04-09 22:47:50.877570: train_loss 5.2098 
2025-04-09 22:47:50.893196: val_loss 2.6353 
2025-04-09 22:47:50.908825: PSNR 18.2136 
2025-04-09 22:47:50.924449: Epoch time: 772.0 s 
2025-04-09 22:47:51.973971:  
2025-04-09 22:47:51.973971: Epoch 47 
2025-04-09 22:47:51.989599: Current learning rate: 0.0001993 
2025-04-09 23:00:47.013947: train_loss 5.1525 
2025-04-09 23:00:47.029572: val_loss 2.5924 
2025-04-09 23:00:47.048088: PSNR 18.4963 
2025-04-09 23:00:47.055701: Epoch time: 775.06 s 
2025-04-09 23:00:47.805711:  
2025-04-09 23:00:47.805711: Epoch 48 
2025-04-09 23:00:47.821339: Current learning rate: 0.0001993 
2025-04-09 23:13:41.899069: train_loss 5.208 
2025-04-09 23:13:41.914696: val_loss 2.5982 
2025-04-09 23:13:41.928455: PSNR 18.6657 
2025-04-09 23:13:41.932620: Epoch time: 774.11 s 
2025-04-09 23:13:42.899592:  
2025-04-09 23:13:42.930844: Epoch 49 
2025-04-09 23:13:42.946468: Current learning rate: 0.0001992 
2025-04-09 23:26:33.492694: train_loss 5.0232 
2025-04-09 23:26:33.508325: val_loss 2.6339 
2025-04-09 23:26:33.537629: PSNR 18.8025 
2025-04-09 23:26:33.568886: Epoch time: 770.59 s 
2025-04-09 23:26:39.698467:  
2025-04-09 23:26:39.729717: Epoch 50 
2025-04-09 23:26:39.745343: Current learning rate: 0.0001992 
2025-04-09 23:39:34.479352: train_loss 5.0327 
2025-04-09 23:39:34.494975: val_loss 2.7162 
2025-04-09 23:39:34.517722: PSNR 18.5885 
2025-04-09 23:39:34.533359: Epoch time: 774.78 s 
2025-04-09 23:39:35.368371:  
2025-04-09 23:39:35.384000: Epoch 51 
2025-04-09 23:39:35.415248: Current learning rate: 0.0001992 
2025-04-09 23:52:31.420923: train_loss 4.9849 
2025-04-09 23:52:31.436547: val_loss 2.7305 
2025-04-09 23:52:31.454197: PSNR 18.6042 
2025-04-09 23:52:31.469833: Epoch time: 776.05 s 
2025-04-09 23:52:32.367040:  
2025-04-09 23:52:32.382668: Epoch 52 
2025-04-09 23:52:32.398292: Current learning rate: 0.0001991 
2025-04-10 00:05:26.780368: train_loss 5.0509 
2025-04-10 00:05:26.841015: val_loss 2.4791 
2025-04-10 00:05:26.856642: PSNR 19.8037 
2025-04-10 00:05:26.872269: Epoch time: 774.41 s 
2025-04-10 00:05:26.887892: Yayy! New best EMA pseudo PSNR: 18.7597 
2025-04-10 00:05:34.549425:  
2025-04-10 00:05:34.565053: Epoch 53 
2025-04-10 00:05:34.596304: Current learning rate: 0.0001991 
2025-04-10 00:18:30.482788: train_loss 5.1442 
2025-04-10 00:18:30.498414: val_loss 2.3256 
2025-04-10 00:18:30.515967: PSNR 19.6158 
2025-04-10 00:18:30.517994: Epoch time: 775.93 s 
2025-04-10 00:18:30.533629: Yayy! New best EMA pseudo PSNR: 18.8453 
2025-04-10 00:18:37.307837:  
2025-04-10 00:18:37.323462: Epoch 54 
2025-04-10 00:18:37.339086: Current learning rate: 0.000199 
2025-04-10 00:31:30.813757: train_loss 5.0649 
2025-04-10 00:31:30.845007: val_loss 2.6353 
2025-04-10 00:31:30.862287: PSNR 18.5687 
2025-04-10 00:31:30.868546: Epoch time: 773.51 s 
2025-04-10 00:31:31.753015:  
2025-04-10 00:31:31.768642: Epoch 55 
2025-04-10 00:31:31.784268: Current learning rate: 0.000199 
2025-04-10 00:44:27.079616: train_loss 4.9787 
2025-04-10 00:44:27.126489: val_loss 2.676 
2025-04-10 00:44:27.143805: PSNR 18.5651 
2025-04-10 00:44:27.145847: Epoch time: 775.33 s 
2025-04-10 00:44:28.409808:  
2025-04-10 00:44:28.441061: Epoch 56 
2025-04-10 00:44:28.472310: Current learning rate: 0.0001989 
2025-04-10 00:57:21.799228: train_loss 5.0378 
2025-04-10 00:57:21.830476: val_loss 2.7854 
2025-04-10 00:57:21.845161: PSNR 18.5524 
2025-04-10 00:57:21.847566: Epoch time: 773.39 s 
2025-04-10 00:57:22.675700:  
2025-04-10 00:57:22.706954: Epoch 57 
2025-04-10 00:57:22.706954: Current learning rate: 0.0001989 
2025-04-10 01:10:19.143591: train_loss 5.0645 
2025-04-10 01:10:19.159216: val_loss 2.6133 
2025-04-10 01:10:19.181484: PSNR 18.6632 
2025-04-10 01:10:19.186787: Epoch time: 776.47 s 
2025-04-10 01:10:20.017869:  
2025-04-10 01:10:20.049126: Epoch 58 
2025-04-10 01:10:20.064752: Current learning rate: 0.0001989 
2025-04-10 01:23:12.570203: train_loss 5.0297 
2025-04-10 01:23:12.601447: val_loss 2.6787 
2025-04-10 01:23:12.619935: PSNR 18.4646 
2025-04-10 01:23:12.635576: Epoch time: 772.55 s 
2025-04-10 01:23:13.645164:  
2025-04-10 01:23:13.676413: Epoch 59 
2025-04-10 01:23:13.692038: Current learning rate: 0.0001988 
2025-04-10 01:36:06.475383: train_loss 5.1415 
2025-04-10 01:36:06.491010: val_loss 2.5881 
2025-04-10 01:36:06.512353: PSNR 18.4974 
2025-04-10 01:36:06.516452: Epoch time: 772.83 s 
2025-04-10 01:36:07.461976:  
2025-04-10 01:36:07.477593: Epoch 60 
2025-04-10 01:36:07.493220: Current learning rate: 0.0001988 
2025-04-10 01:49:01.869214: train_loss 5.0497 
2025-04-10 01:49:01.884844: val_loss 2.8291 
2025-04-10 01:49:01.914869: PSNR 17.7169 
2025-04-10 01:49:01.930506: Epoch time: 774.42 s 
2025-04-10 01:49:02.815540:  
2025-04-10 01:49:02.846792: Epoch 61 
2025-04-10 01:49:02.862419: Current learning rate: 0.0001987 
2025-04-10 02:01:58.989899: train_loss 5.0789 
2025-04-10 02:01:59.026138: val_loss 2.5908 
2025-04-10 02:01:59.041765: PSNR 18.5757 
2025-04-10 02:01:59.057392: Epoch time: 776.17 s 
2025-04-10 02:01:59.885040:  
2025-04-10 02:01:59.916291: Epoch 62 
2025-04-10 02:01:59.931920: Current learning rate: 0.0001987 
2025-04-10 02:14:51.832646: train_loss 4.9823 
2025-04-10 02:14:51.848790: val_loss 2.679 
2025-04-10 02:14:51.864422: PSNR 18.5243 
2025-04-10 02:14:51.880048: Epoch time: 771.95 s 
2025-04-10 02:14:52.775017:  
2025-04-10 02:14:52.790642: Epoch 63 
2025-04-10 02:14:52.806267: Current learning rate: 0.0001986 
2025-04-10 02:27:47.124877: train_loss 4.9538 
2025-04-10 02:27:47.171755: val_loss 2.4575 
2025-04-10 02:27:47.186999: PSNR 19.123 
2025-04-10 02:27:47.190138: Epoch time: 774.35 s 
2025-04-10 02:27:48.429168:  
2025-04-10 02:27:48.444794: Epoch 64 
2025-04-10 02:27:48.460417: Current learning rate: 0.0001986 
2025-04-10 02:40:42.010432: train_loss 5.0263 
2025-04-10 02:40:42.026055: val_loss 2.9108 
2025-04-10 02:40:42.047643: PSNR 17.8896 
2025-04-10 02:40:42.063279: Epoch time: 773.58 s 
2025-04-10 02:40:42.902547:  
2025-04-10 02:40:42.933793: Epoch 65 
2025-04-10 02:40:42.949422: Current learning rate: 0.0001985 
2025-04-10 02:53:34.678737: train_loss 4.9421 
2025-04-10 02:53:34.694363: val_loss 2.3556 
2025-04-10 02:53:34.725173: PSNR 19.5252 
2025-04-10 02:53:34.740809: Epoch time: 771.78 s 
2025-04-10 02:53:35.656520:  
2025-04-10 02:53:35.687771: Epoch 66 
2025-04-10 02:53:35.703396: Current learning rate: 0.0001984 
2025-04-10 03:06:26.180038: train_loss 5.0369 
2025-04-10 03:06:26.226901: val_loss 2.6717 
2025-04-10 03:06:26.240867: PSNR 18.4381 
2025-04-10 03:06:26.245074: Epoch time: 770.52 s 
2025-04-10 03:06:27.318440:  
2025-04-10 03:06:27.318440: Epoch 67 
2025-04-10 03:06:27.334066: Current learning rate: 0.0001984 
2025-04-10 03:19:20.831438: train_loss 5.0773 
2025-04-10 03:19:20.847065: val_loss 2.721 
2025-04-10 03:19:20.861423: PSNR 18.1475 
2025-04-10 03:19:20.867691: Epoch time: 773.53 s 
2025-04-10 03:19:21.875620: Epoch 68 
2025-04-10 03:19:21.890822: Current learning rate: 0.0001983 
2025-04-10 03:32:13.494208: train_loss 5.0755 
2025-04-10 03:32:13.527992: val_loss 2.3238 
2025-04-10 03:32:13.534396: PSNR 19.9432 
2025-04-10 03:32:13.550033: Epoch time: 771.65 s 
2025-04-10 03:32:14.546651:  
2025-04-10 03:32:14.577896: Epoch 69 
2025-04-10 03:32:14.592777: Current learning rate: 0.0001983 
2025-04-10 03:45:11.548910: train_loss 5.0501 
2025-04-10 03:45:11.564535: val_loss 2.5983 
2025-04-10 03:45:11.588376: PSNR 19.2957 
2025-04-10 03:45:11.588376: Epoch time: 777.02 s 
2025-04-10 03:45:12.784830:  
2025-04-10 03:45:12.816080: Epoch 70 
2025-04-10 03:45:12.831704: Current learning rate: 0.0001982 
2025-04-10 03:58:07.412740: train_loss 4.9805 
2025-04-10 03:58:07.443992: val_loss 2.5753 
2025-04-10 03:58:07.458680: PSNR 19.1069 
2025-04-10 03:58:07.474316: Epoch time: 774.64 s 
2025-04-10 03:58:08.390250:  
2025-04-10 03:58:08.421500: Epoch 71 
2025-04-10 03:58:08.421500: Current learning rate: 0.0001982 
2025-04-10 04:11:01.448406: train_loss 4.9486 
2025-04-10 04:11:01.479656: val_loss 2.5744 
2025-04-10 04:11:01.500221: PSNR 18.8373 
2025-04-10 04:11:01.516868: Epoch time: 773.06 s 
2025-04-10 04:11:02.495317:  
2025-04-10 04:11:02.526565: Epoch 72 
2025-04-10 04:11:02.557818: Current learning rate: 0.0001981 
2025-04-10 04:23:55.902799: train_loss 4.9142 
2025-04-10 04:23:55.934053: val_loss 2.5516 
2025-04-10 04:23:55.954537: PSNR 19.7327 
2025-04-10 04:23:55.970174: Epoch time: 773.41 s 
2025-04-10 04:23:55.985796: Yayy! New best EMA pseudo PSNR: 18.9116 
2025-04-10 04:24:02.401257:  
2025-04-10 04:24:02.432506: Epoch 73 
2025-04-10 04:24:02.448133: Current learning rate: 0.000198 
2025-04-10 04:36:58.053530: train_loss 4.9457 
2025-04-10 04:36:58.069153: val_loss 2.6112 
2025-04-10 04:36:58.091685: PSNR 18.8729 
2025-04-10 04:36:58.107321: Epoch time: 775.65 s 
2025-04-10 04:36:58.960265:  
2025-04-10 04:36:58.991515: Epoch 74 
2025-04-10 04:36:58.991515: Current learning rate: 0.000198 
2025-04-10 04:49:54.491543: train_loss 4.9675 
2025-04-10 04:49:54.522796: val_loss 2.4335 
2025-04-10 04:49:54.540274: PSNR 19.2193 
2025-04-10 04:49:54.540274: Epoch time: 775.53 s 
2025-04-10 04:49:54.571537: Yayy! New best EMA pseudo PSNR: 18.9389 
2025-04-10 04:50:00.683772:  
2025-04-10 04:50:00.683772: Epoch 75 
2025-04-10 04:50:00.699398: Current learning rate: 0.0001979 
2025-04-10 05:02:56.033196: train_loss 5.0326 
2025-04-10 05:02:56.053642: val_loss 2.5703 
2025-04-10 05:02:56.073571: PSNR 18.6834 
2025-04-10 05:02:56.089195: Epoch time: 775.37 s 
2025-04-10 05:02:56.906923:  
2025-04-10 05:02:56.906923: Epoch 76 
2025-04-10 05:02:56.938174: Current learning rate: 0.0001978 
2025-04-10 05:15:52.606313: train_loss 4.9164 
2025-04-10 05:15:52.621938: val_loss 2.3877 
2025-04-10 05:15:52.637565: PSNR 19.6496 
2025-04-10 05:15:52.653188: Epoch time: 775.7 s 
2025-04-10 05:15:52.668814: Yayy! New best EMA pseudo PSNR: 18.987 
2025-04-10 05:15:59.306785:  
2025-04-10 05:15:59.322409: Epoch 77 
2025-04-10 05:15:59.338037: Current learning rate: 0.0001978 
2025-04-10 05:28:55.777639: train_loss 5.0618 
2025-04-10 05:28:55.793264: val_loss 2.6295 
2025-04-10 05:28:55.819204: PSNR 18.8215 
2025-04-10 05:28:55.834840: Epoch time: 776.47 s 
2025-04-10 05:28:56.912115:  
2025-04-10 05:28:56.943992: Epoch 78 
2025-04-10 05:28:56.959621: Current learning rate: 0.0001977 
2025-04-10 05:41:52.613555: train_loss 5.028 
2025-04-10 05:41:52.629176: val_loss 2.4262 
2025-04-10 05:41:52.650358: PSNR 18.9693 
2025-04-10 05:41:52.652385: Epoch time: 775.7 s 
2025-04-10 05:41:53.534088:  
2025-04-10 05:41:53.541418: Epoch 79 
2025-04-10 05:41:53.572676: Current learning rate: 0.0001976 
2025-04-10 05:54:52.142588: train_loss 4.9394 
2025-04-10 05:54:52.158214: val_loss 2.6081 
2025-04-10 05:54:52.177382: PSNR 18.9357 
2025-04-10 05:54:52.193016: Epoch time: 778.63 s 
2025-04-10 05:54:53.130520:  
2025-04-10 05:54:53.146142: Epoch 80 
2025-04-10 05:54:53.161771: Current learning rate: 0.0001976 
2025-04-10 06:07:48.268564: train_loss 5.0201 
2025-04-10 06:07:48.299813: val_loss 2.6595 
2025-04-10 06:07:48.316532: PSNR 18.3864 
2025-04-10 06:07:48.332170: Epoch time: 775.15 s 
2025-04-10 06:07:49.380208:  
2025-04-10 06:07:49.400392: Epoch 81 
2025-04-10 06:07:49.416015: Current learning rate: 0.0001975 
2025-04-10 06:20:41.785138: train_loss 4.8618 
2025-04-10 06:20:41.816388: val_loss 2.5083 
2025-04-10 06:20:41.836483: PSNR 19.1473 
2025-04-10 06:20:41.836483: Epoch time: 772.41 s 
2025-04-10 06:20:42.758372:  
2025-04-10 06:20:42.774004: Epoch 82 
2025-04-10 06:20:42.789622: Current learning rate: 0.0001974 
2025-04-10 06:33:39.457613: train_loss 4.9358 
2025-04-10 06:33:39.473237: val_loss 2.4243 
2025-04-10 06:33:39.495171: PSNR 19.9253 
2025-04-10 06:33:39.510807: Epoch time: 776.71 s 
2025-04-10 06:33:39.526434: Yayy! New best EMA pseudo PSNR: 19.0319 
2025-04-10 06:33:45.635224:  
2025-04-10 06:33:45.650855: Epoch 83 
2025-04-10 06:33:45.666479: Current learning rate: 0.0001974 
2025-04-10 06:46:41.580140: train_loss 4.8988 
2025-04-10 06:46:41.611394: val_loss 2.655 
2025-04-10 06:46:41.628218: PSNR 19.0279 
2025-04-10 06:46:41.631368: Epoch time: 775.94 s 
2025-04-10 06:46:42.621400:  
2025-04-10 06:46:42.637028: Epoch 84 
2025-04-10 06:46:42.650038: Current learning rate: 0.0001973 
2025-04-10 06:59:34.698147: train_loss 4.914 
2025-04-10 06:59:34.713771: val_loss 2.3493 
2025-04-10 06:59:34.729398: PSNR 19.5577 
2025-04-10 06:59:34.745023: Epoch time: 772.08 s 
2025-04-10 06:59:34.760648: Yayy! New best EMA pseudo PSNR: 19.0841 
2025-04-10 06:59:40.299439:  
2025-04-10 06:59:40.315063: Epoch 85 
2025-04-10 06:59:40.315063: Current learning rate: 0.0001972 
2025-04-10 07:12:36.522992: train_loss 4.8826 
2025-04-10 07:12:36.538615: val_loss 2.4334 
2025-04-10 07:12:36.562189: PSNR 19.4429 
2025-04-10 07:12:36.577827: Epoch time: 776.22 s 
2025-04-10 07:12:36.593451: Yayy! New best EMA pseudo PSNR: 19.12 
2025-04-10 07:12:42.267825:  
2025-04-10 07:12:42.292606: Epoch 86 
2025-04-10 07:12:42.308241: Current learning rate: 0.0001971 
2025-04-10 07:25:41.217192: train_loss 4.9662 
2025-04-10 07:25:41.232826: val_loss 2.4205 
2025-04-10 07:25:41.248454: PSNR 19.1361 
2025-04-10 07:25:41.264079: Epoch time: 778.95 s 
2025-04-10 07:25:41.295326: Yayy! New best EMA pseudo PSNR: 19.1216 
2025-04-10 07:25:47.079922:  
2025-04-10 07:25:47.095548: Epoch 87 
2025-04-10 07:25:47.111174: Current learning rate: 0.0001971 
2025-04-10 07:38:44.356778: train_loss 4.9372 
2025-04-10 07:38:44.388029: val_loss 2.4931 
2025-04-10 07:38:44.409173: PSNR 18.9534 
2025-04-10 07:38:44.410177: Epoch time: 777.29 s 
2025-04-10 07:38:45.285186:  
2025-04-10 07:38:45.300813: Epoch 88 
2025-04-10 07:38:45.316442: Current learning rate: 0.000197 
2025-04-10 07:51:38.427704: train_loss 4.8946 
2025-04-10 07:51:38.458963: val_loss 2.655 
2025-04-10 07:51:38.474586: PSNR 18.9607 
2025-04-10 07:51:38.490214: Epoch time: 773.14 s 
2025-04-10 07:51:39.741216:  
2025-04-10 07:51:39.756843: Epoch 89 
2025-04-10 07:51:39.772465: Current learning rate: 0.0001969 
2025-04-10 08:04:29.935399: train_loss 4.9618 
2025-04-10 08:04:29.951023: val_loss 2.5865 
2025-04-10 08:04:29.966649: PSNR 18.8128 
2025-04-10 08:04:29.982274: Epoch time: 770.21 s 
2025-04-10 08:04:30.779154:  
2025-04-10 08:04:30.810399: Epoch 90 
2025-04-10 08:04:30.826024: Current learning rate: 0.0001968 
2025-04-10 08:17:24.436724: train_loss 4.889 
2025-04-10 08:17:24.452349: val_loss 2.679 
2025-04-10 08:17:24.483322: PSNR 18.418 
2025-04-10 08:17:24.484328: Epoch time: 773.66 s 
2025-04-10 08:17:25.374968:  
2025-04-10 08:17:25.374968: Epoch 91 
2025-04-10 08:17:25.406217: Current learning rate: 0.0001967 
2025-04-10 08:30:18.344995: train_loss 4.8755 
2025-04-10 08:30:18.344995: val_loss 2.6758 
2025-04-10 08:30:18.376259: PSNR 18.8124 
2025-04-10 08:30:18.391870: Epoch time: 772.99 s 
2025-04-10 08:30:19.270185:  
2025-04-10 08:30:19.285809: Epoch 92 
2025-04-10 08:30:19.301433: Current learning rate: 0.0001967 
2025-04-10 08:43:13.834887: train_loss 4.9727 
2025-04-10 08:43:13.881763: val_loss 2.5618 
2025-04-10 08:43:13.896837: PSNR 18.5985 
2025-04-10 08:43:13.912470: Epoch time: 774.56 s 
2025-04-10 08:43:14.812500:  
2025-04-10 08:43:14.828126: Epoch 93 
2025-04-10 08:43:14.843750: Current learning rate: 0.0001966 
2025-04-10 08:56:09.046432: train_loss 4.9011 
2025-04-10 08:56:09.077680: val_loss 2.5348 
2025-04-10 08:56:09.099221: PSNR 19.1025 
2025-04-10 08:56:09.114858: Epoch time: 774.23 s 
2025-04-10 08:56:09.929569:  
2025-04-10 08:56:09.960826: Epoch 94 
2025-04-10 08:56:09.960826: Current learning rate: 0.0001965 
2025-04-10 09:09:04.254321: train_loss 4.8469 
2025-04-10 09:09:04.285569: val_loss 2.5905 
2025-04-10 09:09:04.309065: PSNR 19.2636 
2025-04-10 09:09:04.311090: Epoch time: 774.32 s 
2025-04-10 09:09:05.176921:  
2025-04-10 09:09:05.192547: Epoch 95 
2025-04-10 09:09:05.208173: Current learning rate: 0.0001964 
2025-04-10 09:22:00.981974: train_loss 4.8408 
2025-04-10 09:22:00.997602: val_loss 2.7896 
2025-04-10 09:22:01.012188: PSNR 18.2388 
2025-04-10 09:22:01.016458: Epoch time: 775.81 s 
2025-04-10 09:22:01.828969:  
2025-04-10 09:22:01.844591: Epoch 96 
2025-04-10 09:22:01.860218: Current learning rate: 0.0001963 
2025-04-10 09:34:57.858921: train_loss 4.8938 
2025-04-10 09:34:57.874542: val_loss 2.5265 
2025-04-10 09:34:57.902961: PSNR 19.0971 
2025-04-10 09:34:57.903967: Epoch time: 776.05 s 
2025-04-10 09:34:58.785584:  
2025-04-10 09:34:58.785584: Epoch 97 
2025-04-10 09:34:58.801219: Current learning rate: 0.0001963 
2025-04-10 09:47:51.306115: train_loss 4.7787 
2025-04-10 09:47:51.337363: val_loss 2.6609 
2025-04-10 09:47:51.352990: PSNR 18.923 
2025-04-10 09:47:51.368614: Epoch time: 772.54 s 
2025-04-10 09:47:52.659204:  
2025-04-10 09:47:52.674825: Epoch 98 
2025-04-10 09:47:52.690452: Current learning rate: 0.0001962 
2025-04-10 10:00:47.723987: train_loss 4.8892 
2025-04-10 10:00:47.739616: val_loss 2.6285 
2025-04-10 10:00:47.755828: PSNR 18.5428 
2025-04-10 10:00:47.771462: Epoch time: 775.06 s 
2025-04-10 10:00:48.618357:  
2025-04-10 10:00:48.633980: Epoch 99 
2025-04-10 10:00:48.649607: Current learning rate: 0.0001961 
2025-04-10 10:13:46.553257: train_loss 4.8113 
2025-04-10 10:13:46.553257: val_loss 2.4076 
2025-04-10 10:13:46.578842: PSNR 20.1162 
2025-04-10 10:13:46.594480: Epoch time: 777.93 s 
2025-04-10 10:13:53.041599:  
2025-04-10 10:13:53.057227: Epoch 100 
2025-04-10 10:13:53.072852: Current learning rate: 0.000196 
2025-04-10 10:26:44.298966: train_loss 4.9971 
2025-04-10 10:26:44.330796: val_loss 2.7006 
2025-04-10 10:26:44.355523: PSNR 18.6471 
2025-04-10 10:26:44.371160: Epoch time: 771.26 s 
2025-04-10 10:26:45.449981:  
2025-04-10 10:26:45.465607: Epoch 101 
2025-04-10 10:26:45.481232: Current learning rate: 0.0001959 
2025-04-10 10:39:38.263729: train_loss 4.8319 
2025-04-10 10:39:38.279356: val_loss 2.647 
2025-04-10 10:39:38.294981: PSNR 18.5181 
2025-04-10 10:39:38.310614: Epoch time: 772.81 s 
2025-04-10 10:39:39.237862:  
2025-04-10 10:39:39.253486: Epoch 102 
2025-04-10 10:39:39.269115: Current learning rate: 0.0001958 
2025-04-10 10:52:36.454024: train_loss 4.791 
2025-04-10 10:52:36.485271: val_loss 2.4409 
2025-04-10 10:52:36.501072: PSNR 19.5491 
2025-04-10 10:52:36.501072: Epoch time: 777.22 s 
2025-04-10 10:52:37.486176:  
2025-04-10 10:52:37.501802: Epoch 103 
2025-04-10 10:52:37.517431: Current learning rate: 0.0001957 
2025-04-10 11:05:30.184808: train_loss 4.7983 
2025-04-10 11:05:30.200432: val_loss 2.5342 
2025-04-10 11:05:30.221263: PSNR 18.9396 
2025-04-10 11:05:30.222271: Epoch time: 772.7 s 
2025-04-10 11:05:31.121920:  
2025-04-10 11:05:31.153173: Epoch 104 
2025-04-10 11:05:31.153173: Current learning rate: 0.0001956 
2025-04-10 11:18:35.776966: train_loss 4.7307 
2025-04-10 11:39:34.215699: val_loss 2.4658 
2025-04-10 11:39:34.231317: PSNR 19.8422 
2025-04-10 11:39:34.262529: Epoch time: 784.66 s 
2025-04-10 11:39:35.575054:  
2025-04-10 11:39:35.606303: Epoch 105 
2025-04-10 11:39:35.621928: Current learning rate: 0.0001955 
2025-04-10 11:53:04.872621: train_loss 4.9621 
2025-04-10 11:53:04.903871: val_loss 2.6228 
2025-04-10 11:53:04.924147: PSNR 18.696 
2025-04-10 11:53:04.939787: Epoch time: 809.31 s 
2025-04-10 11:53:06.138289:  
2025-04-10 11:53:06.153915: Epoch 106 
2025-04-10 11:53:06.173780: Current learning rate: 0.0001954 
2025-04-10 12:06:33.795233: train_loss 4.7904 
2025-04-10 12:06:33.826707: val_loss 2.794 
2025-04-10 12:06:33.842343: PSNR 18.5395 
2025-04-10 12:06:33.857860: Epoch time: 807.67 s 
2025-04-10 12:06:34.857732:  
2025-04-10 12:06:34.873352: Epoch 107 
2025-04-10 12:06:34.888979: Current learning rate: 0.0001953 
2025-04-10 12:19:43.061557: train_loss 4.6748 
2025-04-10 12:19:43.092805: val_loss 2.4416 
2025-04-10 12:19:43.108439: PSNR 19.8617 
2025-04-10 12:19:43.124060: Epoch time: 788.2 s 
2025-04-10 12:19:44.218075:  
2025-04-10 12:19:44.233591: Epoch 108 
2025-04-10 12:19:44.249231: Current learning rate: 0.0001953 
2025-04-10 12:32:46.478588: train_loss 4.9501 
2025-04-10 13:24:19.711838: val_loss 2.4323 
2025-04-10 13:24:19.727464: PSNR 19.5757 
2025-04-10 13:24:19.743090: Epoch time: 782.28 s 
2025-04-10 13:24:19.758718: Yayy! New best EMA pseudo PSNR: 19.1239 
2025-04-10 13:24:28.243151:  
2025-04-10 13:29:07.384533: Epoch 109 
2025-04-10 13:29:07.400158: Current learning rate: 0.0001952 
2025-04-10 13:42:39.402144: train_loss 4.8264 
2025-04-10 13:55:05.528937: val_loss 2.5827 
2025-04-10 13:55:05.560186: PSNR 18.6717 
2025-04-10 13:55:05.575812: Epoch time: 1091.16 s 
2025-04-10 13:55:06.872686:  
2025-04-10 13:55:06.888313: Epoch 110 
2025-04-10 13:55:06.919560: Current learning rate: 0.0001951 
2025-04-10 14:08:40.405663: train_loss 4.7577 
2025-04-10 14:08:40.421288: val_loss 2.5432 
2025-04-10 14:08:40.452539: PSNR 19.1596 
2025-04-10 14:08:40.468165: Epoch time: 813.53 s 
2025-04-10 14:08:41.608798:  
2025-04-10 14:08:41.640045: Epoch 111 
2025-04-10 14:08:41.655670: Current learning rate: 0.000195 
2025-04-10 14:22:08.094797: train_loss 4.8773 
2025-04-10 14:22:08.110426: val_loss 2.4579 
2025-04-10 14:22:08.126046: PSNR 19.9427 
2025-04-10 14:22:08.141675: Epoch time: 806.49 s 
2025-04-10 14:22:08.157298: Yayy! New best EMA pseudo PSNR: 19.1723 
2025-04-10 14:22:14.391702:  
2025-04-10 14:22:14.407327: Epoch 112 
2025-04-10 14:22:14.422952: Current learning rate: 0.0001949 
2025-04-10 14:35:24.205708: train_loss 4.877 
2025-04-10 14:35:24.221337: val_loss 2.4988 
2025-04-10 14:35:24.253975: PSNR 19.2107 
2025-04-10 14:35:24.254988: Epoch time: 789.81 s 
2025-04-10 14:35:24.270627: Yayy! New best EMA pseudo PSNR: 19.1762 
2025-04-10 14:35:30.112010:  
2025-04-10 14:35:30.127636: Epoch 113 
2025-04-10 14:35:30.143263: Current learning rate: 0.0001948 
2025-04-10 14:48:56.144745: train_loss 4.914 
2025-04-10 14:52:52.942036: val_loss 2.3675 
2025-04-10 14:52:52.957663: PSNR 19.0309 
2025-04-10 14:52:52.979924: Epoch time: 806.03 s 
2025-04-10 14:52:54.301344:  
2025-04-10 14:52:54.316971: Epoch 114 
2025-04-10 14:52:54.316971: Current learning rate: 0.0001947 
2025-04-10 15:06:15.080843: train_loss 4.6911 
2025-04-10 15:06:15.096473: val_loss 2.4989 
2025-04-10 15:06:15.109658: PSNR 20.1497 
2025-04-10 15:06:15.118053: Epoch time: 800.78 s 
2025-04-10 15:06:15.133689: Yayy! New best EMA pseudo PSNR: 19.2604 
2025-04-10 15:06:21.615405:  
2025-04-10 15:06:21.631032: Epoch 115 
2025-04-10 15:06:21.646659: Current learning rate: 0.0001946 
2025-04-10 15:19:46.882054: train_loss 4.885 
2025-04-10 15:22:04.069769: val_loss 2.7169 
2025-04-10 15:22:04.085394: PSNR 18.2407 
2025-04-10 15:22:04.101023: Epoch time: 805.27 s 
2025-04-10 15:22:05.335396:  
2025-04-10 15:22:05.351023: Epoch 116 
2025-04-10 15:22:05.366647: Current learning rate: 0.0001945 
2025-04-10 15:35:39.570983: train_loss 4.759 
2025-04-10 15:35:39.586606: val_loss 2.4908 
2025-04-10 15:35:39.602235: PSNR 19.2669 
2025-04-10 15:35:39.602235: Epoch time: 814.24 s 
2025-04-10 15:35:40.539740:  
2025-04-10 15:35:40.539740: Epoch 117 
2025-04-10 15:35:40.555361: Current learning rate: 0.0001944 
