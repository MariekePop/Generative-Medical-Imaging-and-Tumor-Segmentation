
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-17 17:24:51.948256: unpacking dataset... 
2025-04-17 17:25:04.591882: unpacking done... 
2025-04-17 17:25:04.611885: do_dummy_2d_data_aug: False 
2025-04-17 17:25:04.628882: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-17 17:25:04.656886: The split file contains 5 splits. 
2025-04-17 17:25:04.669885: Desired fold for training: 0 
2025-04-17 17:25:04.684886: This split has 4 training and 1 validation cases. 
2025-04-17 17:25:05.572886: Unable to plot network architecture: 
2025-04-17 17:25:05.585886: No module named 'hiddenlayer' 
2025-04-17 17:25:05.657887:  
2025-04-17 17:25:05.675888: Epoch 300 
2025-04-17 17:25:05.691885: Current learning rate: 0.0001609 
2025-04-17 17:32:41.268447: train_loss 2.8708 
2025-04-17 17:32:41.288451: val_loss 2.2463 
2025-04-17 17:32:41.301449: PSNR 19.0224 
2025-04-17 17:32:41.316451: Epoch time: 455.61 s 
2025-04-17 17:32:42.204452:  
2025-04-17 17:32:42.221451: Epoch 301 
2025-04-17 17:32:42.232454: Current learning rate: 0.0001607 
2025-04-17 17:46:53.359335: train_loss 3.7868 
2025-04-17 17:46:53.393371: val_loss 2.689 
2025-04-17 17:46:53.406370: PSNR 18.6122 
2025-04-17 17:46:53.417373: Epoch time: 851.16 s 
2025-04-17 17:46:54.215404:  
2025-04-17 17:46:54.232409: Epoch 302 
2025-04-17 17:46:54.248408: Current learning rate: 0.0001604 
2025-04-17 18:09:51.074261: train_loss 4.0427 
2025-04-17 18:09:51.099263: val_loss 2.618 
2025-04-17 18:09:51.115295: PSNR 19.2539 
2025-04-17 18:09:51.129295: Epoch time: 1376.86 s 
2025-04-17 18:09:52.096298:  
2025-04-17 18:09:52.109298: Epoch 303 
2025-04-17 18:09:52.122296: Current learning rate: 0.0001602 
2025-04-17 18:32:49.793836: train_loss 4.1766 
2025-04-17 18:32:49.811836: val_loss 2.3736 
2025-04-17 18:32:49.831838: PSNR 20.1202 
2025-04-17 18:32:49.850838: Epoch time: 1377.7 s 
2025-04-17 18:32:50.676874:  
2025-04-17 18:32:50.693872: Epoch 304 
2025-04-17 18:32:50.709875: Current learning rate: 0.0001599 
2025-04-17 18:55:44.511786: train_loss 4.1277 
2025-04-17 18:55:44.529792: val_loss 2.6411 
2025-04-17 18:55:44.543785: PSNR 18.7424 
2025-04-17 18:55:44.559788: Epoch time: 1373.84 s 
2025-04-17 18:55:45.347356:  
2025-04-17 18:55:45.364360: Epoch 305 
2025-04-17 18:55:45.383358: Current learning rate: 0.0001597 
2025-04-17 19:18:39.689038: train_loss 4.1254 
2025-04-17 19:18:39.708041: val_loss 2.6869 
2025-04-17 19:18:39.730036: PSNR 18.8489 
2025-04-17 19:18:39.742039: Epoch time: 1374.34 s 
2025-04-17 19:18:40.529648:  
2025-04-17 19:18:40.545647: Epoch 306 
2025-04-17 19:18:40.555647: Current learning rate: 0.0001594 
2025-04-17 19:41:36.159083: train_loss 3.8742 
2025-04-17 19:41:36.188082: val_loss 2.7209 
2025-04-17 19:41:36.203588: PSNR 18.9818 
2025-04-17 19:41:36.220594: Epoch time: 1375.63 s 
2025-04-17 19:41:37.004624:  
2025-04-17 19:41:37.027624: Epoch 307 
2025-04-17 19:41:37.041623: Current learning rate: 0.0001592 
2025-04-17 20:04:31.438657: train_loss 3.9181 
2025-04-17 20:04:31.468656: val_loss 2.6928 
2025-04-17 20:04:31.488661: PSNR 18.8982 
2025-04-17 20:04:31.510658: Epoch time: 1374.44 s 
2025-04-17 20:04:32.307681:  
2025-04-17 20:04:32.330679: Epoch 308 
2025-04-17 20:04:32.343679: Current learning rate: 0.0001589 
2025-04-17 20:27:20.585396: train_loss 3.9212 
2025-04-17 20:27:20.598394: val_loss 2.5126 
2025-04-17 20:27:20.612393: PSNR 19.6994 
2025-04-17 20:27:20.625396: Epoch time: 1368.28 s 
2025-04-17 20:27:21.400425:  
2025-04-17 20:27:21.416423: Epoch 309 
2025-04-17 20:27:21.430425: Current learning rate: 0.0001587 
2025-04-17 20:50:17.795080: train_loss 3.8689 
2025-04-17 20:50:17.818115: val_loss 2.7271 
2025-04-17 20:50:17.829114: PSNR 18.3635 
2025-04-17 20:50:17.843117: Epoch time: 1376.4 s 
2025-04-17 20:50:18.672112:  
2025-04-17 20:50:18.695115: Epoch 310 
2025-04-17 20:50:18.710111: Current learning rate: 0.0001584 
2025-04-17 21:13:12.365970: train_loss 3.8457 
2025-04-17 21:13:12.385974: val_loss 2.5872 
2025-04-17 21:13:12.397963: PSNR 19.278 
2025-04-17 21:13:12.408964: Epoch time: 1373.7 s 
2025-04-17 21:13:13.202960:  
2025-04-17 21:13:13.218964: Epoch 311 
2025-04-17 21:13:13.231962: Current learning rate: 0.0001582 
2025-04-17 21:36:01.544932: train_loss 3.9055 
2025-04-17 21:36:01.561927: val_loss 2.5948 
2025-04-17 21:36:01.582928: PSNR 19.4496 
2025-04-17 21:36:01.596924: Epoch time: 1368.35 s 
2025-04-17 21:36:02.382925:  
2025-04-17 21:36:02.394924: Epoch 312 
2025-04-17 21:36:02.406924: Current learning rate: 0.0001579 
2025-04-17 21:59:00.420292: train_loss 3.7873 
2025-04-17 21:59:00.445832: val_loss 2.6662 
2025-04-17 21:59:00.457831: PSNR 18.8059 
2025-04-17 21:59:00.467834: Epoch time: 1378.04 s 
2025-04-17 21:59:01.240867:  
2025-04-17 21:59:01.255863: Epoch 313 
2025-04-17 21:59:01.268863: Current learning rate: 0.0001576 
2025-04-17 22:21:49.846825: train_loss 3.8618 
2025-04-17 22:21:49.864827: val_loss 2.8254 
2025-04-17 22:21:49.877825: PSNR 18.4082 
2025-04-17 22:21:49.892824: Epoch time: 1368.61 s 
2025-04-17 22:21:51.122825:  
2025-04-17 22:21:51.136824: Epoch 314 
2025-04-17 22:21:51.148825: Current learning rate: 0.0001574 
2025-04-17 22:44:49.008143: train_loss 3.8668 
2025-04-17 22:44:49.028142: val_loss 2.8025 
2025-04-17 22:44:49.040144: PSNR 18.7189 
2025-04-17 22:44:49.054142: Epoch time: 1377.89 s 
2025-04-17 22:44:49.849177:  
2025-04-17 22:44:49.862177: Epoch 315 
2025-04-17 22:44:49.876174: Current learning rate: 0.0001571 
2025-04-17 23:07:49.910995: train_loss 3.7595 
2025-04-17 23:07:49.923994: val_loss 2.5396 
2025-04-17 23:07:49.938997: PSNR 19.6439 
2025-04-17 23:07:49.948994: Epoch time: 1380.06 s 
2025-04-17 23:07:50.755641:  
2025-04-17 23:07:50.770641: Epoch 316 
2025-04-17 23:07:50.781639: Current learning rate: 0.0001569 
2025-04-17 23:30:48.683398: train_loss 3.8939 
2025-04-17 23:30:48.701919: val_loss 2.6477 
2025-04-17 23:30:48.717921: PSNR 19.0688 
2025-04-17 23:30:48.733917: Epoch time: 1377.93 s 
2025-04-17 23:30:49.545919:  
2025-04-17 23:30:49.562921: Epoch 317 
2025-04-17 23:30:49.584920: Current learning rate: 0.0001566 
2025-04-17 23:53:36.930146: train_loss 3.809 
2025-04-17 23:53:36.946144: val_loss 2.6429 
2025-04-17 23:53:36.967144: PSNR 19.2992 
2025-04-17 23:53:36.981143: Epoch time: 1367.39 s 
2025-04-17 23:53:37.777143:  
2025-04-17 23:53:37.792141: Epoch 318 
2025-04-17 23:53:37.803140: Current learning rate: 0.0001563 
2025-04-18 00:16:31.414311: train_loss 4.0071 
2025-04-18 00:16:31.439312: val_loss 2.5948 
2025-04-18 00:16:31.460309: PSNR 18.9634 
2025-04-18 00:16:31.477313: Epoch time: 1373.64 s 
2025-04-18 00:16:32.495316:  
2025-04-18 00:16:32.510310: Epoch 319 
2025-04-18 00:16:32.522312: Current learning rate: 0.0001561 
2025-04-18 00:39:19.427222: train_loss 3.7893 
2025-04-18 00:39:19.444224: val_loss 2.6041 
2025-04-18 00:39:19.459224: PSNR 19.9148 
2025-04-18 00:39:19.473224: Epoch time: 1366.94 s 
2025-04-18 00:39:20.317435:  
2025-04-18 00:39:20.330434: Epoch 320 
2025-04-18 00:39:20.350437: Current learning rate: 0.0001558 
2025-04-18 01:02:14.723212: train_loss 3.8455 
2025-04-18 01:02:14.746213: val_loss 2.6925 
2025-04-18 01:02:14.762214: PSNR 19.1198 
2025-04-18 01:02:14.783212: Epoch time: 1374.41 s 
2025-04-18 01:02:15.901242:  
2025-04-18 01:02:15.914247: Epoch 321 
2025-04-18 01:02:15.928243: Current learning rate: 0.0001556 
2025-04-18 01:25:03.042331: train_loss 3.8301 
2025-04-18 01:25:03.057332: val_loss 2.5454 
2025-04-18 01:25:03.076333: PSNR 19.6657 
2025-04-18 01:25:03.095375: Epoch time: 1367.14 s 
2025-04-18 01:25:03.898371:  
2025-04-18 01:25:03.922373: Epoch 322 
2025-04-18 01:25:03.936375: Current learning rate: 0.0001553 
2025-04-18 01:47:52.995066: train_loss 3.783 
2025-04-18 01:47:53.030101: val_loss 2.6989 
2025-04-18 01:47:53.047103: PSNR 19.3559 
2025-04-18 01:47:53.064098: Epoch time: 1369.1 s 
2025-04-18 01:47:53.878103:  
2025-04-18 01:47:53.894104: Epoch 323 
2025-04-18 01:47:53.915100: Current learning rate: 0.000155 
2025-04-18 02:10:42.391992: train_loss 3.8777 
2025-04-18 02:10:42.405985: val_loss 2.6404 
2025-04-18 02:10:42.420988: PSNR 19.2784 
2025-04-18 02:10:42.431988: Epoch time: 1368.52 s 
2025-04-18 02:10:43.213989:  
2025-04-18 02:10:43.228987: Epoch 324 
2025-04-18 02:10:43.243989: Current learning rate: 0.0001548 
2025-04-18 02:33:42.514387: train_loss 3.8954 
2025-04-18 02:33:42.541386: val_loss 2.6852 
2025-04-18 02:33:42.559387: PSNR 18.9155 
2025-04-18 02:33:42.579390: Epoch time: 1379.3 s 
2025-04-18 02:33:43.786934:  
2025-04-18 02:33:43.800451: Epoch 325 
2025-04-18 02:33:43.811973: Current learning rate: 0.0001545 
2025-04-18 02:56:32.361542: train_loss 3.8611 
2025-04-18 02:56:32.388555: val_loss 2.4727 
2025-04-18 02:56:32.400545: PSNR 19.721 
2025-04-18 02:56:32.420545: Epoch time: 1368.58 s 
2025-04-18 02:56:33.595577:  
2025-04-18 02:56:33.609575: Epoch 326 
2025-04-18 02:56:33.621574: Current learning rate: 0.0001542 
2025-04-18 03:19:32.605071: train_loss 3.841 
2025-04-18 03:19:32.634072: val_loss 2.5157 
2025-04-18 03:19:32.650075: PSNR 19.5191 
2025-04-18 03:19:32.671073: Epoch time: 1379.01 s 
2025-04-18 03:19:34.023128:  
2025-04-18 03:19:34.041132: Epoch 327 
2025-04-18 03:19:34.065128: Current learning rate: 0.000154 
2025-04-18 03:42:32.263595: train_loss 3.9706 
2025-04-18 03:42:32.294595: val_loss 2.6517 
2025-04-18 03:42:32.313594: PSNR 19.08 
2025-04-18 03:42:32.333594: Epoch time: 1378.25 s 
2025-04-18 03:42:33.340819:  
2025-04-18 03:42:33.355818: Epoch 328 
2025-04-18 03:42:33.368816: Current learning rate: 0.0001537 
2025-04-18 04:05:30.787220: train_loss 3.6608 
2025-04-18 04:05:30.817791: val_loss 2.7208 
2025-04-18 04:05:30.830787: PSNR 18.9665 
2025-04-18 04:05:30.841787: Epoch time: 1377.45 s 
2025-04-18 04:05:31.646793:  
2025-04-18 04:05:31.662792: Epoch 329 
2025-04-18 04:05:31.675793: Current learning rate: 0.0001534 
2025-04-18 04:28:28.065430: train_loss 3.8445 
2025-04-18 04:28:28.083430: val_loss 2.4665 
2025-04-18 04:28:28.097432: PSNR 20.0292 
2025-04-18 04:28:28.110435: Epoch time: 1376.42 s 
2025-04-18 04:28:29.223470:  
2025-04-18 04:28:29.238472: Epoch 330 
2025-04-18 04:28:29.253474: Current learning rate: 0.0001532 
2025-04-18 04:51:28.790681: train_loss 3.8186 
2025-04-18 04:51:28.809680: val_loss 2.6965 
2025-04-18 04:51:28.833681: PSNR 19.1503 
2025-04-18 04:51:28.849680: Epoch time: 1379.57 s 
2025-04-18 04:51:29.649711:  
2025-04-18 04:51:29.674711: Epoch 331 
2025-04-18 04:51:29.695709: Current learning rate: 0.0001529 
2025-04-18 05:14:26.251045: train_loss 3.7703 
2025-04-18 05:14:26.270046: val_loss 2.6718 
2025-04-18 05:14:26.284047: PSNR 19.2162 
2025-04-18 05:14:26.296043: Epoch time: 1376.61 s 
2025-04-18 05:14:27.233040:  
2025-04-18 05:14:27.250040: Epoch 332 
2025-04-18 05:14:27.265044: Current learning rate: 0.0001526 
2025-04-18 05:37:26.751116: train_loss 3.8006 
2025-04-18 05:37:26.779115: val_loss 2.6372 
2025-04-18 05:37:26.794117: PSNR 19.2874 
2025-04-18 05:37:26.806117: Epoch time: 1379.52 s 
2025-04-18 05:37:28.152623:  
2025-04-18 05:37:28.166623: Epoch 333 
2025-04-18 05:37:28.179626: Current learning rate: 0.0001524 
2025-04-18 06:00:27.346018: train_loss 3.7433 
2025-04-18 06:00:27.377020: val_loss 2.7434 
2025-04-18 06:00:27.396021: PSNR 19.1871 
2025-04-18 06:00:27.411018: Epoch time: 1379.2 s 
2025-04-18 06:00:28.729053:  
2025-04-18 06:00:28.745054: Epoch 334 
2025-04-18 06:00:28.760057: Current learning rate: 0.0001521 
2025-04-18 06:23:26.695753: train_loss 3.6735 
2025-04-18 06:23:26.712759: val_loss 2.6767 
2025-04-18 06:23:26.727752: PSNR 19.5775 
2025-04-18 06:23:26.742752: Epoch time: 1377.97 s 
2025-04-18 06:23:27.757751:  
2025-04-18 06:23:27.776754: Epoch 335 
2025-04-18 06:23:27.790751: Current learning rate: 0.0001518 
2025-04-18 06:46:26.703930: train_loss 3.7832 
2025-04-18 06:46:26.718932: val_loss 2.7932 
2025-04-18 06:46:26.733932: PSNR 19.0977 
2025-04-18 06:46:26.747933: Epoch time: 1378.95 s 
2025-04-18 06:46:27.601592:  
2025-04-18 06:46:27.614592: Epoch 336 
2025-04-18 06:46:27.628591: Current learning rate: 0.0001516 
2025-04-18 07:09:15.299549: train_loss 3.7701 
2025-04-18 07:09:15.313061: val_loss 2.7829 
2025-04-18 07:09:15.331063: PSNR 18.8607 
2025-04-18 07:09:15.345065: Epoch time: 1367.7 s 
2025-04-18 07:09:16.123060:  
2025-04-18 07:09:16.139058: Epoch 337 
2025-04-18 07:09:16.152062: Current learning rate: 0.0001513 
2025-04-18 07:32:06.680332: train_loss 3.7664 
2025-04-18 07:32:06.702323: val_loss 2.5488 
2025-04-18 07:32:06.718327: PSNR 19.8686 
2025-04-18 07:32:06.735325: Epoch time: 1370.56 s 
2025-04-18 07:32:07.896481:  
2025-04-18 07:32:07.909480: Epoch 338 
2025-04-18 07:32:07.929482: Current learning rate: 0.000151 
2025-04-18 07:55:04.814325: train_loss 3.7851 
2025-04-18 07:55:04.844325: val_loss 2.6018 
2025-04-18 07:55:04.868325: PSNR 19.2772 
2025-04-18 07:55:04.887327: Epoch time: 1376.92 s 
2025-04-18 07:55:06.140043:  
2025-04-18 07:55:06.154046: Epoch 339 
2025-04-18 07:55:06.170048: Current learning rate: 0.0001508 
2025-04-18 08:18:03.285994: train_loss 3.8286 
2025-04-18 08:18:03.322000: val_loss 2.769 
2025-04-18 08:18:03.334996: PSNR 18.7665 
2025-04-18 08:18:03.345994: Epoch time: 1377.15 s 
2025-04-18 08:18:04.600755:  
2025-04-18 08:18:04.612752: Epoch 340 
2025-04-18 08:18:04.625751: Current learning rate: 0.0001505 
2025-04-18 08:41:01.840295: train_loss 3.7924 
2025-04-18 08:41:01.864294: val_loss 2.6079 
2025-04-18 08:41:01.877296: PSNR 19.4165 
2025-04-18 08:41:01.889295: Epoch time: 1377.24 s 
2025-04-18 08:41:03.082302:  
2025-04-18 08:41:03.099343: Epoch 341 
2025-04-18 08:41:03.113342: Current learning rate: 0.0001502 
2025-04-18 09:03:57.654719: train_loss 3.7315 
2025-04-18 09:03:57.683720: val_loss 2.646 
2025-04-18 09:03:57.705717: PSNR 19.2027 
2025-04-18 09:03:57.719718: Epoch time: 1374.58 s 
2025-04-18 09:03:58.536497:  
2025-04-18 09:03:58.560496: Epoch 342 
2025-04-18 09:03:58.578495: Current learning rate: 0.0001499 
2025-04-18 09:26:58.268863: train_loss 3.7588 
2025-04-18 09:26:58.290867: val_loss 2.636 
2025-04-18 09:26:58.307863: PSNR 19.6525 
2025-04-18 09:26:58.329863: Epoch time: 1379.74 s 
2025-04-18 09:26:59.507432:  
2025-04-18 09:26:59.519433: Epoch 343 
2025-04-18 09:26:59.532434: Current learning rate: 0.0001497 
2025-04-18 09:49:47.234489: train_loss 3.8266 
2025-04-18 09:49:47.256483: val_loss 2.6616 
2025-04-18 09:49:47.269485: PSNR 19.3653 
2025-04-18 09:49:47.283484: Epoch time: 1367.73 s 
2025-04-18 09:49:48.420560:  
2025-04-18 09:49:48.433558: Epoch 344 
2025-04-18 09:49:48.444560: Current learning rate: 0.0001494 
2025-04-18 10:12:45.051760: train_loss 3.6672 
2025-04-18 10:12:45.072299: val_loss 2.7149 
2025-04-18 10:12:45.089284: PSNR 19.0165 
2025-04-18 10:12:45.100284: Epoch time: 1376.64 s 
2025-04-18 10:12:46.352332:  
2025-04-18 10:12:46.364328: Epoch 345 
2025-04-18 10:12:46.377330: Current learning rate: 0.0001491 
2025-04-18 10:35:42.786333: train_loss 3.7194 
2025-04-18 10:35:42.812332: val_loss 2.7594 
2025-04-18 10:35:42.828333: PSNR 18.8066 
2025-04-18 10:35:42.838333: Epoch time: 1376.44 s 
2025-04-18 10:35:44.013232:  
2025-04-18 10:35:44.033231: Epoch 346 
2025-04-18 10:35:44.050232: Current learning rate: 0.0001488 
2025-04-18 10:58:41.871384: train_loss 3.7224 
2025-04-18 10:58:41.900383: val_loss 2.9014 
2025-04-18 10:58:41.913393: PSNR 18.2593 
2025-04-18 10:58:41.927387: Epoch time: 1377.86 s 
2025-04-18 10:58:43.252448:  
2025-04-18 10:58:43.265445: Epoch 347 
2025-04-18 10:58:43.277445: Current learning rate: 0.0001486 
2025-04-18 11:21:38.245948: train_loss 3.751 
2025-04-18 11:21:38.266947: val_loss 2.598 
2025-04-18 11:21:38.278951: PSNR 19.4149 
2025-04-18 11:21:38.292943: Epoch time: 1375.0 s 
2025-04-18 11:21:39.072948:  
2025-04-18 11:21:39.087946: Epoch 348 
2025-04-18 11:21:39.106982: Current learning rate: 0.0001483 
2025-04-18 11:44:26.949462: train_loss 3.7936 
2025-04-18 11:44:26.975498: val_loss 2.6305 
2025-04-18 11:44:26.990499: PSNR 19.2212 
2025-04-18 11:44:27.001498: Epoch time: 1367.88 s 
2025-04-18 11:44:27.806767:  
2025-04-18 11:44:27.818760: Epoch 349 
2025-04-18 11:44:27.829761: Current learning rate: 0.000148 
2025-04-18 12:07:26.996613: train_loss 3.7362 
2025-04-18 12:07:27.020614: val_loss 2.6198 
2025-04-18 12:07:27.039635: PSNR 20.1632 
2025-04-18 12:07:27.054613: Epoch time: 1379.19 s 
2025-04-18 12:07:34.675420:  
2025-04-18 12:07:34.692421: Epoch 350 
2025-04-18 12:07:34.707421: Current learning rate: 0.0001477 
2025-04-18 12:30:28.012464: train_loss 3.8492 
2025-04-18 12:30:28.033465: val_loss 2.5689 
2025-04-18 12:30:28.051465: PSNR 19.6175 
2025-04-18 12:30:28.067466: Epoch time: 1373.34 s 
2025-04-18 12:30:29.271499:  
2025-04-18 12:30:29.283499: Epoch 351 
2025-04-18 12:30:29.294499: Current learning rate: 0.0001475 
2025-04-18 12:53:21.547350: train_loss 3.7136 
2025-04-18 12:53:21.573349: val_loss 2.6731 
2025-04-18 12:53:21.589380: PSNR 19.5532 
2025-04-18 12:53:21.611376: Epoch time: 1372.28 s 
2025-04-18 12:53:22.800385:  
2025-04-18 12:53:22.816380: Epoch 352 
2025-04-18 12:53:22.827380: Current learning rate: 0.0001472 
