
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-05-02 21:43:24.828233: unpacking dataset... 
2025-05-02 21:43:35.570372: unpacking done... 
2025-05-02 21:43:35.585999: do_dummy_2d_data_aug: False 
2025-05-02 21:43:35.601624: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-05-02 21:43:35.617247: The split file contains 5 splits. 
2025-05-02 21:43:35.625264: Desired fold for training: 0 
2025-05-02 21:43:35.640905: This split has 4 training and 1 validation cases. 
2025-05-02 21:43:36.263110: Unable to plot network architecture: 
2025-05-02 21:43:36.294375: No module named 'hiddenlayer' 
2025-05-02 21:43:36.406038:  
2025-05-02 21:43:36.421675: Epoch 700 
2025-05-02 21:43:36.437300: Current learning rate: 4.3e-05 
2025-05-02 21:51:00.681489: train_loss 1.4114 
2025-05-02 21:51:00.712746: val_loss 2.4196 
2025-05-02 21:51:00.712746: PSNR 18.909 
2025-05-02 21:51:00.728366: Epoch time: 444.29 s 
2025-05-02 21:51:02.062209:  
2025-05-02 21:51:02.077830: Epoch 701 
2025-05-02 21:51:02.093454: Current learning rate: 4.27e-05 
2025-05-02 22:05:06.768433: train_loss 2.3313 
2025-05-02 22:05:06.799693: val_loss 2.7554 
2025-05-02 22:05:06.815312: PSNR 19.593 
2025-05-02 22:05:06.830944: Epoch time: 844.72 s 
2025-05-02 22:05:08.176409:  
2025-05-02 22:05:08.192030: Epoch 702 
2025-05-02 22:05:08.207655: Current learning rate: 4.25e-05 
2025-05-02 22:19:16.309079: train_loss 2.6709 
2025-05-02 22:19:16.371582: val_loss 2.7496 
2025-05-02 22:19:16.387226: PSNR 19.7071 
2025-05-02 22:19:16.403445: Epoch time: 848.15 s 
2025-05-02 22:19:17.943691:  
2025-05-02 22:19:17.964352: Epoch 703 
2025-05-02 22:19:17.980004: Current learning rate: 4.22e-05 
2025-05-02 22:42:37.335401: train_loss 2.6606 
2025-05-02 22:42:37.387838: val_loss 2.6639 
2025-05-02 22:42:37.403466: PSNR 19.8909 
2025-05-02 22:42:37.434711: Epoch time: 1399.41 s 
2025-05-02 22:42:38.656178:  
2025-05-02 22:42:38.671797: Epoch 704 
2025-05-02 22:42:38.703040: Current learning rate: 4.19e-05 
2025-05-02 23:05:47.893066: train_loss 2.7023 
2025-05-02 23:05:47.908693: val_loss 2.8623 
2025-05-02 23:05:47.924326: PSNR 19.2805 
2025-05-02 23:05:47.939956: Epoch time: 1389.25 s 
2025-05-02 23:05:48.949572:  
2025-05-02 23:05:48.965188: Epoch 705 
2025-05-02 23:05:48.996439: Current learning rate: 4.17e-05 
2025-05-02 23:29:03.484320: train_loss 2.7244 
2025-05-02 23:29:03.515567: val_loss 2.8244 
2025-05-02 23:29:03.531196: PSNR 19.4613 
2025-05-02 23:29:03.546825: Epoch time: 1394.55 s 
2025-05-02 23:29:04.806032:  
2025-05-02 23:29:04.837282: Epoch 706 
2025-05-02 23:29:04.852907: Current learning rate: 4.14e-05 
2025-05-02 23:52:08.241110: train_loss 2.7413 
2025-05-02 23:52:08.272361: val_loss 2.5984 
2025-05-02 23:52:08.287987: PSNR 20.2785 
2025-05-02 23:52:08.303614: Epoch time: 1383.44 s 
2025-05-02 23:52:09.325259:  
2025-05-02 23:52:09.340883: Epoch 707 
2025-05-02 23:52:09.356509: Current learning rate: 4.12e-05 
2025-05-03 00:15:24.432611: train_loss 2.761 
2025-05-03 00:15:24.448239: val_loss 2.7509 
2025-05-03 00:15:24.463864: PSNR 19.6946 
2025-05-03 00:15:24.479489: Epoch time: 1395.11 s 
2025-05-03 00:15:25.504012:  
2025-05-03 00:15:25.519631: Epoch 708 
2025-05-03 00:15:25.550881: Current learning rate: 4.09e-05 
2025-05-03 00:38:29.316435: train_loss 2.7605 
2025-05-03 00:38:29.350898: val_loss 2.6302 
2025-05-03 00:38:29.366529: PSNR 19.6738 
2025-05-03 00:38:29.382152: Epoch time: 1383.83 s 
2025-05-03 00:38:31.016982:  
2025-05-03 00:38:31.032617: Epoch 709 
2025-05-03 00:38:31.063859: Current learning rate: 4.07e-05 
2025-05-03 01:01:39.862677: train_loss 2.6725 
2025-05-03 01:01:39.893938: val_loss 2.8947 
2025-05-03 01:01:39.925179: PSNR 18.9445 
2025-05-03 01:01:39.940806: Epoch time: 1388.85 s 
2025-05-03 01:01:41.465612:  
2025-05-03 01:01:41.481230: Epoch 710 
2025-05-03 01:01:41.496858: Current learning rate: 4.04e-05 
2025-05-03 01:24:43.684410: train_loss 2.6916 
2025-05-03 01:24:43.700035: val_loss 2.8057 
2025-05-03 01:24:43.731289: PSNR 19.3174 
2025-05-03 01:24:43.731289: Epoch time: 1382.23 s 
2025-05-03 01:24:44.859509:  
2025-05-03 01:24:44.890788: Epoch 711 
2025-05-03 01:24:44.906412: Current learning rate: 4.02e-05 
2025-05-03 01:48:00.235689: train_loss 2.6997 
2025-05-03 01:48:00.252337: val_loss 2.6276 
2025-05-03 01:48:00.267962: PSNR 20.2364 
2025-05-03 01:48:00.283587: Epoch time: 1395.38 s 
2025-05-03 01:48:01.624945:  
2025-05-03 01:48:01.640579: Epoch 712 
2025-05-03 01:48:01.656200: Current learning rate: 3.99e-05 
2025-05-03 02:11:12.888069: train_loss 2.7774 
2025-05-03 02:11:12.903694: val_loss 2.7127 
2025-05-03 02:11:12.934941: PSNR 19.6425 
2025-05-03 02:11:12.956667: Epoch time: 1391.28 s 
2025-05-03 02:11:14.491068:  
2025-05-03 02:11:14.506684: Epoch 713 
2025-05-03 02:11:14.537928: Current learning rate: 3.97e-05 
2025-05-03 02:34:22.642310: train_loss 2.6486 
2025-05-03 02:34:22.657938: val_loss 2.8845 
2025-05-03 02:34:22.673576: PSNR 19.1821 
2025-05-03 02:34:22.689190: Epoch time: 1388.15 s 
2025-05-03 02:34:23.933390:  
2025-05-03 02:34:23.949006: Epoch 714 
2025-05-03 02:34:23.964628: Current learning rate: 3.94e-05 
2025-05-03 02:57:41.626450: train_loss 2.72 
2025-05-03 02:57:41.642087: val_loss 2.8202 
2025-05-03 02:57:41.657698: PSNR 19.1537 
2025-05-03 02:57:41.673321: Epoch time: 1397.71 s 
2025-05-03 02:57:42.876502:  
2025-05-03 02:57:42.892129: Epoch 715 
2025-05-03 02:57:42.907754: Current learning rate: 3.92e-05 
2025-05-03 03:20:58.752619: train_loss 2.7308 
2025-05-03 03:20:58.768261: val_loss 2.8081 
2025-05-03 03:20:58.799495: PSNR 19.1103 
2025-05-03 03:20:58.815124: Epoch time: 1395.88 s 
2025-05-03 03:21:00.118476:  
2025-05-03 03:21:00.134111: Epoch 716 
2025-05-03 03:21:00.165347: Current learning rate: 3.89e-05 
2025-05-03 03:44:03.071207: train_loss 2.6967 
2025-05-03 03:44:03.102463: val_loss 2.6333 
2025-05-03 03:44:03.118089: PSNR 19.9363 
2025-05-03 03:44:03.133713: Epoch time: 1382.95 s 
2025-05-03 03:44:04.792961:  
2025-05-03 03:44:04.824211: Epoch 717 
2025-05-03 03:44:04.839837: Current learning rate: 3.87e-05 
2025-05-03 04:07:09.265399: train_loss 2.7015 
2025-05-03 04:07:09.296667: val_loss 2.9899 
2025-05-03 04:07:09.312293: PSNR 19.1243 
2025-05-03 04:07:09.327920: Epoch time: 1384.47 s 
2025-05-03 04:07:10.978395:  
2025-05-03 04:07:11.009648: Epoch 718 
2025-05-03 04:07:11.025273: Current learning rate: 3.84e-05 
2025-05-03 04:30:21.911197: train_loss 2.7033 
2025-05-03 04:30:21.953960: val_loss 2.7149 
2025-05-03 04:30:21.969588: PSNR 19.5854 
2025-05-03 04:30:22.000839: Epoch time: 1390.93 s 
2025-05-03 04:30:23.389482:  
2025-05-03 04:30:23.405118: Epoch 719 
2025-05-03 04:30:23.436357: Current learning rate: 3.82e-05 
2025-05-03 04:53:34.087211: train_loss 2.6822 
2025-05-03 04:53:34.102839: val_loss 2.7457 
2025-05-03 04:53:34.134083: PSNR 19.8001 
2025-05-03 04:53:34.165334: Epoch time: 1390.7 s 
2025-05-03 04:53:35.858171:  
2025-05-03 04:53:35.873797: Epoch 720 
2025-05-03 04:53:35.889424: Current learning rate: 3.79e-05 
2025-05-03 05:16:47.736539: train_loss 2.7908 
2025-05-03 05:16:47.752163: val_loss 2.6282 
2025-05-03 05:16:47.767815: PSNR 19.5332 
2025-05-03 05:16:47.799047: Epoch time: 1391.89 s 
2025-05-03 05:16:49.505705:  
2025-05-03 05:16:49.536958: Epoch 721 
2025-05-03 05:16:49.552584: Current learning rate: 3.77e-05 
2025-05-03 05:40:02.343951: train_loss 2.6375 
2025-05-03 05:40:02.375213: val_loss 2.9128 
2025-05-03 05:40:02.390836: PSNR 19.1348 
2025-05-03 05:40:02.406458: Epoch time: 1392.84 s 
2025-05-03 05:40:03.978025:  
2025-05-03 05:40:04.009277: Epoch 722 
2025-05-03 05:40:04.024903: Current learning rate: 3.74e-05 
2025-05-03 06:03:07.646065: train_loss 2.8264 
2025-05-03 06:03:07.677313: val_loss 2.5731 
2025-05-03 06:03:07.692940: PSNR 20.1718 
2025-05-03 06:03:07.708564: Epoch time: 1383.67 s 
2025-05-03 06:03:09.151659:  
2025-05-03 06:03:09.167283: Epoch 723 
2025-05-03 06:03:09.189616: Current learning rate: 3.72e-05 
2025-05-03 06:26:13.805434: train_loss 2.6787 
2025-05-03 06:26:13.821058: val_loss 2.7558 
2025-05-03 06:26:13.852309: PSNR 19.4212 
2025-05-03 06:26:13.867939: Epoch time: 1384.65 s 
2025-05-03 06:26:15.684494:  
2025-05-03 06:26:15.715744: Epoch 724 
2025-05-03 06:26:15.731372: Current learning rate: 3.69e-05 
2025-05-03 06:49:26.690115: train_loss 2.7294 
2025-05-03 06:49:26.721370: val_loss 2.8815 
2025-05-03 06:49:26.736992: PSNR 19.1161 
2025-05-03 06:49:26.752621: Epoch time: 1391.01 s 
2025-05-03 06:49:28.308660:  
2025-05-03 06:49:28.339918: Epoch 725 
2025-05-03 06:49:28.355544: Current learning rate: 3.67e-05 
2025-05-03 07:12:38.318687: train_loss 2.8292 
2025-05-03 07:12:38.349937: val_loss 2.5417 
2025-05-03 07:12:38.374820: PSNR 19.745 
2025-05-03 07:12:38.390463: Epoch time: 1390.03 s 
2025-05-03 07:12:39.649476:  
2025-05-03 07:12:39.680724: Epoch 726 
2025-05-03 07:12:39.696351: Current learning rate: 3.64e-05 
2025-05-03 07:35:58.906179: train_loss 2.6083 
2025-05-03 07:35:58.937428: val_loss 2.7104 
2025-05-03 07:35:58.937428: PSNR 19.5615 
2025-05-03 07:35:58.968693: Epoch time: 1399.26 s 
2025-05-03 07:35:59.886474:  
2025-05-03 07:35:59.902098: Epoch 727 
2025-05-03 07:35:59.917725: Current learning rate: 3.62e-05 
2025-05-03 07:59:11.984315: train_loss 2.7375 
2025-05-03 07:59:12.015553: val_loss 2.7633 
2025-05-03 07:59:12.054201: PSNR 19.4186 
2025-05-03 07:59:12.069824: Epoch time: 1392.1 s 
2025-05-03 07:59:13.557562:  
2025-05-03 07:59:13.588799: Epoch 728 
2025-05-03 07:59:13.604428: Current learning rate: 3.6e-05 
2025-05-03 08:22:32.559846: train_loss 2.689 
2025-05-03 08:22:32.591107: val_loss 2.6921 
2025-05-03 08:22:32.606723: PSNR 19.783 
2025-05-03 08:22:32.622350: Epoch time: 1399.02 s 
2025-05-03 08:22:33.975110:  
2025-05-03 08:22:34.006371: Epoch 729 
2025-05-03 08:22:34.017393: Current learning rate: 3.57e-05 
2025-05-03 08:45:55.201745: train_loss 2.7005 
2025-05-03 08:45:55.217370: val_loss 2.5923 
2025-05-03 08:45:55.248624: PSNR 20.0935 
2025-05-03 08:45:55.264255: Epoch time: 1401.23 s 
2025-05-03 08:45:56.344560:  
2025-05-03 08:45:56.375814: Epoch 730 
2025-05-03 08:45:56.391437: Current learning rate: 3.55e-05 
2025-05-03 09:09:05.929776: train_loss 2.7437 
2025-05-03 09:09:05.961026: val_loss 2.8396 
2025-05-03 09:09:05.976656: PSNR 19.0159 
2025-05-03 09:09:05.992273: Epoch time: 1389.59 s 
2025-05-03 09:09:07.480156:  
2025-05-03 09:09:07.511405: Epoch 731 
2025-05-03 09:09:07.511405: Current learning rate: 3.52e-05 
2025-05-03 09:32:25.952062: train_loss 2.7042 
2025-05-03 09:32:25.967689: val_loss 2.6117 
2025-05-03 09:32:25.983320: PSNR 20.0187 
2025-05-03 09:32:25.998945: Epoch time: 1398.47 s 
2025-05-03 09:32:27.615383:  
2025-05-03 09:32:27.631001: Epoch 732 
2025-05-03 09:32:27.646625: Current learning rate: 3.5e-05 
2025-05-03 09:55:49.487051: train_loss 2.6982 
2025-05-03 09:55:49.502693: val_loss 2.9788 
2025-05-03 09:55:49.518322: PSNR 18.7438 
2025-05-03 09:55:49.533944: Epoch time: 1401.89 s 
2025-05-03 09:55:50.777499:  
2025-05-03 09:55:50.808751: Epoch 733 
2025-05-03 09:55:50.824373: Current learning rate: 3.47e-05 
2025-05-03 10:19:06.959848: train_loss 2.6134 
2025-05-03 10:19:06.975490: val_loss 2.7855 
2025-05-03 10:19:06.991121: PSNR 19.8378 
2025-05-03 10:19:07.006743: Epoch time: 1396.18 s 
2025-05-03 10:19:08.423753:  
2025-05-03 10:19:08.455001: Epoch 734 
2025-05-03 10:19:08.486249: Current learning rate: 3.45e-05 
2025-05-03 10:42:18.918529: train_loss 2.8383 
2025-05-03 10:42:18.934151: val_loss 2.5459 
2025-05-03 10:42:18.965407: PSNR 19.1231 
2025-05-03 10:42:18.981032: Epoch time: 1390.51 s 
2025-05-03 10:42:20.569034:  
2025-05-03 10:42:20.584672: Epoch 735 
2025-05-03 10:42:20.615913: Current learning rate: 3.43e-05 
2025-05-03 11:05:26.500945: train_loss 2.6474 
2025-05-03 11:05:26.532198: val_loss 2.6626 
2025-05-03 11:05:26.547828: PSNR 19.7011 
2025-05-03 11:05:26.563452: Epoch time: 1385.93 s 
2025-05-03 11:05:27.994654:  
2025-05-03 11:05:28.025903: Epoch 736 
2025-05-03 11:05:28.057164: Current learning rate: 3.4e-05 
2025-05-03 11:28:37.756707: train_loss 2.7445 
2025-05-03 11:28:37.772333: val_loss 2.6117 
2025-05-03 11:28:37.787963: PSNR 19.9868 
2025-05-03 11:28:37.819210: Epoch time: 1389.78 s 
2025-05-03 11:28:39.159345:  
2025-05-03 11:28:39.174963: Epoch 737 
2025-05-03 11:28:39.190587: Current learning rate: 3.38e-05 
2025-05-03 11:51:57.723513: train_loss 2.6454 
2025-05-03 11:51:57.739140: val_loss 2.7047 
2025-05-03 11:51:57.770398: PSNR 20.1062 
2025-05-03 11:51:57.786023: Epoch time: 1398.58 s 
2025-05-03 11:51:59.245286:  
2025-05-03 11:51:59.260899: Epoch 738 
2025-05-03 11:51:59.276524: Current learning rate: 3.36e-05 
2025-05-03 12:15:15.881018: train_loss 2.7452 
2025-05-03 12:15:15.896644: val_loss 2.6599 
2025-05-03 12:15:15.912268: PSNR 19.7356 
2025-05-03 12:15:15.927900: Epoch time: 1396.65 s 
2025-05-03 12:15:17.633996:  
2025-05-03 12:15:17.665246: Epoch 739 
2025-05-03 12:15:17.680877: Current learning rate: 3.33e-05 
2025-05-03 12:38:29.363838: train_loss 2.6774 
2025-05-03 12:38:29.379472: val_loss 2.7255 
2025-05-03 12:38:29.395088: PSNR 19.8062 
2025-05-03 12:38:29.395088: Epoch time: 1391.75 s 
2025-05-03 12:38:30.678938:  
2025-05-03 12:38:30.694602: Epoch 740 
2025-05-03 12:38:30.710223: Current learning rate: 3.31e-05 
2025-05-03 13:01:46.116874: train_loss 2.6998 
2025-05-03 13:01:46.132498: val_loss 3.0098 
2025-05-03 13:01:46.163750: PSNR 19.0046 
2025-05-03 13:01:46.179380: Epoch time: 1395.44 s 
2025-05-03 13:01:47.573859:  
2025-05-03 13:01:47.589485: Epoch 741 
2025-05-03 13:01:47.605108: Current learning rate: 3.29e-05 
2025-05-03 13:25:10.082217: train_loss 2.6537 
2025-05-03 13:25:10.113467: val_loss 2.7529 
2025-05-03 13:25:10.129097: PSNR 19.5973 
2025-05-03 13:25:10.144722: Epoch time: 1402.51 s 
2025-05-03 13:25:11.779694:  
2025-05-03 13:25:11.810940: Epoch 742 
2025-05-03 13:25:11.826567: Current learning rate: 3.26e-05 
2025-05-03 13:48:21.916258: train_loss 2.5918 
2025-05-03 13:48:21.963135: val_loss 2.905 
2025-05-03 13:48:21.978776: PSNR 19.2826 
2025-05-03 13:48:21.994383: Epoch time: 1390.14 s 
2025-05-03 13:48:23.363084:  
2025-05-03 13:48:23.394340: Epoch 743 
2025-05-03 13:48:23.409960: Current learning rate: 3.24e-05 
2025-05-03 14:11:28.231599: train_loss 2.7061 
2025-05-03 14:11:28.262851: val_loss 2.6832 
2025-05-03 14:11:28.294106: PSNR 20.0634 
2025-05-03 14:11:28.309733: Epoch time: 1384.87 s 
2025-05-03 14:11:29.830696:  
2025-05-03 14:11:29.861946: Epoch 744 
2025-05-03 14:11:29.877573: Current learning rate: 3.22e-05 
2025-05-03 14:34:34.362856: train_loss 2.9011 
2025-05-03 14:34:34.378484: val_loss 2.4497 
2025-05-03 14:34:34.403182: PSNR 19.4753 
2025-05-03 14:34:34.418828: Epoch time: 1384.53 s 
2025-05-03 14:34:35.479238:  
2025-05-03 14:34:35.494865: Epoch 745 
2025-05-03 14:34:35.510490: Current learning rate: 3.19e-05 
2025-05-03 14:57:40.093089: train_loss 2.67 
2025-05-03 14:57:40.124339: val_loss 2.8418 
2025-05-03 14:57:40.139970: PSNR 19.0557 
2025-05-03 14:57:40.155594: Epoch time: 1384.61 s 
2025-05-03 14:57:41.415431:  
2025-05-03 14:57:41.431056: Epoch 746 
2025-05-03 14:57:41.446685: Current learning rate: 3.17e-05 
2025-05-03 15:20:47.238000: train_loss 2.6321 
2025-05-03 15:20:47.269252: val_loss 2.7893 
2025-05-03 15:20:47.284877: PSNR 19.2131 
2025-05-03 15:20:47.300500: Epoch time: 1385.84 s 
2025-05-03 15:20:48.715931:  
2025-05-03 15:20:48.731554: Epoch 747 
2025-05-03 15:20:48.747179: Current learning rate: 3.15e-05 
2025-05-03 15:44:06.498910: train_loss 2.6706 
2025-05-03 15:44:06.514535: val_loss 2.8978 
2025-05-03 15:44:06.530164: PSNR 19.1112 
2025-05-03 15:44:06.545789: Epoch time: 1397.8 s 
2025-05-03 15:44:07.555051:  
2025-05-03 15:44:07.586302: Epoch 748 
2025-05-03 15:44:07.601928: Current learning rate: 3.12e-05 
2025-05-03 16:07:26.247158: train_loss 2.7412 
2025-05-03 16:07:26.278412: val_loss 2.9178 
2025-05-03 16:07:26.294037: PSNR 18.673 
2025-05-03 16:07:26.309666: Epoch time: 1398.69 s 
2025-05-03 16:07:27.553206:  
2025-05-03 16:07:27.568822: Epoch 749 
2025-05-03 16:07:27.600074: Current learning rate: 3.1e-05 
2025-05-03 16:30:44.865402: train_loss 2.6668 
2025-05-03 16:30:44.881025: val_loss 2.6538 
2025-05-03 16:30:44.896654: PSNR 19.9623 
2025-05-03 16:30:44.912279: Epoch time: 1397.33 s 
2025-05-03 16:30:53.243516:  
2025-05-03 16:30:53.259135: Epoch 750 
2025-05-03 16:30:53.290392: Current learning rate: 3.08e-05 
2025-05-03 16:54:11.751706: train_loss 2.6855 
2025-05-03 16:54:11.767339: val_loss 2.8977 
2025-05-03 16:54:11.782957: PSNR 19.2765 
2025-05-03 16:54:11.798584: Epoch time: 1398.51 s 
2025-05-03 16:54:13.106216:  
2025-05-03 16:54:13.137453: Epoch 751 
2025-05-03 16:54:13.137453: Current learning rate: 3.06e-05 
2025-05-03 17:17:22.467056: train_loss 2.7735 
2025-05-03 17:17:22.498310: val_loss 2.6365 
2025-05-03 17:17:22.513934: PSNR 19.907 
2025-05-03 17:17:22.529557: Epoch time: 1389.36 s 
2025-05-03 17:17:24.007063:  
2025-05-03 17:17:24.038307: Epoch 752 
2025-05-03 17:17:24.053936: Current learning rate: 3.03e-05 
2025-05-03 17:40:33.847522: train_loss 2.7197 
2025-05-03 17:40:33.863147: val_loss 2.7227 
2025-05-03 17:40:33.894408: PSNR 19.2494 
2025-05-03 17:40:33.910031: Epoch time: 1389.84 s 
2025-05-03 17:40:35.656227:  
2025-05-03 17:40:35.687465: Epoch 753 
2025-05-03 17:40:35.703094: Current learning rate: 3.01e-05 
2025-05-03 18:03:49.256514: train_loss 2.7704 
2025-05-03 18:03:49.287766: val_loss 2.6724 
2025-05-03 18:03:49.303395: PSNR 19.5163 
2025-05-03 18:03:49.319021: Epoch time: 1393.6 s 
2025-05-03 18:03:50.374791:  
2025-05-03 18:03:50.406042: Epoch 754 
2025-05-03 18:03:50.421668: Current learning rate: 2.99e-05 
2025-05-03 18:26:53.680062: train_loss 2.7024 
2025-05-03 18:26:53.695686: val_loss 2.7743 
2025-05-03 18:26:53.711312: PSNR 19.1153 
2025-05-03 18:26:53.726938: Epoch time: 1383.31 s 
2025-05-03 18:26:54.671045:  
2025-05-03 18:26:54.686662: Epoch 755 
2025-05-03 18:26:54.702287: Current learning rate: 2.97e-05 
2025-05-03 18:50:04.750144: train_loss 2.6863 
2025-05-03 18:50:04.765769: val_loss 2.8173 
2025-05-03 18:50:04.781393: PSNR 19.5535 
2025-05-03 18:50:04.797018: Epoch time: 1390.09 s 
2025-05-03 18:50:05.979345:  
2025-05-03 18:50:06.010590: Epoch 756 
2025-05-03 18:50:06.041840: Current learning rate: 2.94e-05 
2025-05-03 19:13:11.374751: train_loss 2.6974 
2025-05-03 19:13:11.390395: val_loss 2.6828 
2025-05-03 19:13:11.421644: PSNR 19.8372 
2025-05-03 19:13:11.437272: Epoch time: 1385.41 s 
2025-05-03 19:13:12.833740:  
2025-05-03 19:13:12.864995: Epoch 757 
2025-05-03 19:13:12.896242: Current learning rate: 2.92e-05 
2025-05-03 19:36:30.355849: train_loss 2.7569 
2025-05-03 19:36:30.371487: val_loss 2.4491 
2025-05-03 19:36:30.387100: PSNR 20.2465 
2025-05-03 19:36:30.418364: Epoch time: 1397.52 s 
2025-05-03 19:36:31.739988:  
2025-05-03 19:36:31.771225: Epoch 758 
2025-05-03 19:36:31.771225: Current learning rate: 2.9e-05 
2025-05-03 19:59:50.479622: train_loss 2.6521 
2025-05-03 19:59:50.510872: val_loss 2.9123 
2025-05-03 19:59:50.542130: PSNR 19.2249 
2025-05-03 19:59:50.557753: Epoch time: 1398.76 s 
2025-05-03 19:59:51.857226:  
2025-05-03 19:59:51.904096: Epoch 759 
2025-05-03 19:59:51.919723: Current learning rate: 2.88e-05 
2025-05-03 20:23:07.959174: train_loss 2.7369 
2025-05-03 20:23:07.990425: val_loss 2.8263 
2025-05-03 20:23:08.021680: PSNR 18.6447 
2025-05-03 20:23:08.037305: Epoch time: 1396.1 s 
2025-05-03 20:23:09.280823:  
2025-05-03 20:23:09.312074: Epoch 760 
2025-05-03 20:23:09.327699: Current learning rate: 2.85e-05 
2025-05-03 20:46:29.271325: train_loss 2.6778 
2025-05-03 20:46:29.302576: val_loss 2.6857 
2025-05-03 20:46:29.318205: PSNR 19.9389 
2025-05-03 20:46:29.333829: Epoch time: 1399.99 s 
2025-05-03 20:46:30.687304:  
2025-05-03 20:46:30.702929: Epoch 761 
2025-05-03 20:46:30.718555: Current learning rate: 2.83e-05 
2025-05-03 21:09:46.591927: train_loss 2.755 
2025-05-03 21:09:46.623188: val_loss 2.5389 
2025-05-03 21:09:46.638804: PSNR 19.883 
2025-05-03 21:09:46.654429: Epoch time: 1395.92 s 
2025-05-03 21:09:47.804531:  
2025-05-03 21:09:47.820179: Epoch 762 
2025-05-03 21:09:47.851408: Current learning rate: 2.81e-05 
2025-05-03 21:32:57.703849: train_loss 2.6218 
2025-05-03 21:32:57.719476: val_loss 2.7478 
2025-05-03 21:32:57.750728: PSNR 19.6554 
2025-05-03 21:32:57.750728: Epoch time: 1389.91 s 
2025-05-03 21:32:59.041080:  
2025-05-03 21:32:59.072331: Epoch 763 
2025-05-03 21:32:59.072331: Current learning rate: 2.79e-05 
2025-05-03 21:56:01.317805: train_loss 2.6607 
2025-05-03 21:56:01.349055: val_loss 2.7688 
2025-05-03 21:56:01.364684: PSNR 19.5895 
2025-05-03 21:56:01.380312: Epoch time: 1382.28 s 
2025-05-03 21:56:02.851621:  
2025-05-03 21:56:02.882858: Epoch 764 
2025-05-03 21:56:02.898488: Current learning rate: 2.77e-05 
2025-05-03 22:19:20.171378: train_loss 2.5877 
2025-05-03 22:19:20.187001: val_loss 2.704 
2025-05-03 22:19:20.218251: PSNR 20.0294 
2025-05-03 22:19:20.218251: Epoch time: 1397.34 s 
2025-05-03 22:19:21.533551:  
2025-05-03 22:19:21.549175: Epoch 765 
2025-05-03 22:19:21.580423: Current learning rate: 2.74e-05 
2025-05-03 22:42:24.321608: train_loss 2.6634 
2025-05-03 22:42:24.346517: val_loss 2.692 
2025-05-03 22:42:24.362160: PSNR 19.6558 
2025-05-03 22:42:24.377785: Epoch time: 1382.79 s 
2025-05-03 22:42:25.825937:  
2025-05-03 22:42:25.857187: Epoch 766 
2025-05-03 22:42:25.872816: Current learning rate: 2.72e-05 
2025-05-03 23:05:28.693753: train_loss 2.7164 
2025-05-03 23:05:28.709378: val_loss 2.7861 
2025-05-03 23:05:28.725014: PSNR 19.524 
2025-05-03 23:05:28.740628: Epoch time: 1382.88 s 
2025-05-03 23:05:29.968374:  
2025-05-03 23:05:29.999625: Epoch 767 
2025-05-03 23:05:30.030875: Current learning rate: 2.7e-05 
2025-05-03 23:28:51.674051: train_loss 2.7465 
2025-05-03 23:28:51.705302: val_loss 2.704 
2025-05-03 23:28:51.720929: PSNR 19.8259 
2025-05-03 23:28:51.736552: Epoch time: 1401.71 s 
2025-05-03 23:28:53.111404:  
2025-05-03 23:28:53.127030: Epoch 768 
2025-05-03 23:28:53.158282: Current learning rate: 2.68e-05 
2025-05-03 23:52:04.524953: train_loss 2.6798 
2025-05-03 23:52:04.549800: val_loss 2.7118 
2025-05-03 23:52:04.565450: PSNR 19.8188 
2025-05-03 23:52:04.581075: Epoch time: 1391.41 s 
2025-05-03 23:52:06.152914:  
2025-05-03 23:52:06.184164: Epoch 769 
2025-05-03 23:52:06.215416: Current learning rate: 2.66e-05 
2025-05-04 00:15:22.386195: train_loss 2.6928 
2025-05-04 00:15:22.402443: val_loss 2.7279 
2025-05-04 00:15:22.418093: PSNR 19.8444 
2025-05-04 00:15:22.449344: Epoch time: 1396.23 s 
2025-05-04 00:15:23.386841:  
2025-05-04 00:15:23.421916: Epoch 770 
2025-05-04 00:15:23.427236: Current learning rate: 2.64e-05 
2025-05-04 00:38:32.705521: train_loss 2.7038 
2025-05-04 00:38:32.726449: val_loss 2.7571 
2025-05-04 00:38:32.742081: PSNR 19.8036 
2025-05-04 00:38:32.757703: Epoch time: 1389.32 s 
2025-05-04 00:38:34.360641:  
2025-05-04 00:38:34.391891: Epoch 771 
2025-05-04 00:38:34.407516: Current learning rate: 2.62e-05 
2025-05-04 01:01:37.498112: train_loss 2.638 
2025-05-04 01:01:37.513741: val_loss 2.625 
2025-05-04 01:01:37.544991: PSNR 20.4829 
2025-05-04 01:01:37.581122: Epoch time: 1383.14 s 
2025-05-04 01:01:38.950316:  
2025-05-04 01:01:38.981567: Epoch 772 
2025-05-04 01:01:38.997195: Current learning rate: 2.59e-05 
2025-05-04 01:24:58.413395: train_loss 2.6613 
2025-05-04 01:24:58.444642: val_loss 2.5346 
2025-05-04 01:24:58.460264: PSNR 20.3039 
2025-05-04 01:24:58.475890: Epoch time: 1399.46 s 
2025-05-04 01:25:00.013237:  
2025-05-04 01:25:00.028860: Epoch 773 
2025-05-04 01:25:00.069939: Current learning rate: 2.57e-05 
2025-05-04 01:48:16.510674: train_loss 2.6988 
2025-05-04 01:48:16.526303: val_loss 2.8461 
2025-05-04 01:48:16.541929: PSNR 19.7581 
2025-05-04 01:48:16.557550: Epoch time: 1396.51 s 
2025-05-04 01:48:17.612598:  
2025-05-04 01:48:17.628234: Epoch 774 
2025-05-04 01:48:17.643852: Current learning rate: 2.55e-05 
2025-05-04 02:11:21.790867: train_loss 2.6473 
2025-05-04 02:11:21.806492: val_loss 2.6841 
2025-05-04 02:11:21.822116: PSNR 20.4201 
2025-05-04 02:11:21.853369: Epoch time: 1384.18 s 
2025-05-04 02:11:21.853369: Yayy! New best EMA pseudo PSNR: 19.8637 
2025-05-04 02:11:27.917657:  
2025-05-04 02:11:27.948899: Epoch 775 
2025-05-04 02:11:27.964526: Current learning rate: 2.53e-05 
2025-05-04 02:34:48.582574: train_loss 2.6202 
2025-05-04 02:34:48.629450: val_loss 2.8467 
2025-05-04 02:34:48.645080: PSNR 19.5899 
2025-05-04 02:34:48.660704: Epoch time: 1400.68 s 
2025-05-04 02:34:50.201180:  
2025-05-04 02:34:50.232434: Epoch 776 
2025-05-04 02:34:50.248062: Current learning rate: 2.51e-05 
2025-05-04 02:58:10.550418: train_loss 2.6713 
2025-05-04 02:58:10.581673: val_loss 2.8098 
2025-05-04 02:58:10.597318: PSNR 19.4161 
2025-05-04 02:58:10.612926: Epoch time: 1400.35 s 
2025-05-04 02:58:12.161896:  
2025-05-04 02:58:12.183416: Epoch 777 
2025-05-04 02:58:12.186584: Current learning rate: 2.49e-05 
2025-05-04 03:21:16.637379: train_loss 2.6108 
2025-05-04 03:21:16.653003: val_loss 2.7978 
2025-05-04 03:21:16.668633: PSNR 19.4557 
2025-05-04 03:21:16.704737: Epoch time: 1384.49 s 
2025-05-04 03:21:17.637776:  
2025-05-04 03:21:17.653402: Epoch 778 
2025-05-04 03:21:17.669024: Current learning rate: 2.47e-05 
2025-05-04 03:44:39.706020: train_loss 2.5962 
2025-05-04 03:44:39.737270: val_loss 2.8793 
2025-05-04 03:44:39.752903: PSNR 19.3627 
2025-05-04 03:44:39.768532: Epoch time: 1402.07 s 
2025-05-04 03:44:41.058586:  
2025-05-04 03:44:41.089838: Epoch 779 
2025-05-04 03:44:41.105464: Current learning rate: 2.45e-05 
2025-05-04 04:08:01.212274: train_loss 2.6397 
2025-05-04 04:08:01.243523: val_loss 2.6605 
2025-05-04 04:08:01.259150: PSNR 20.0707 
2025-05-04 04:08:01.274778: Epoch time: 1400.15 s 
2025-05-04 04:08:02.339991:  
2025-05-04 04:08:02.371243: Epoch 780 
2025-05-04 04:08:02.386869: Current learning rate: 2.43e-05 
2025-05-04 04:31:13.698208: train_loss 2.6681 
2025-05-04 04:31:13.729458: val_loss 2.6603 
2025-05-04 04:31:13.745087: PSNR 20.1214 
2025-05-04 04:31:13.760711: Epoch time: 1391.36 s 
2025-05-04 04:31:15.082532:  
2025-05-04 04:31:15.113784: Epoch 781 
2025-05-04 04:31:15.129409: Current learning rate: 2.41e-05 
2025-05-04 04:54:20.176638: train_loss 2.6289 
2025-05-04 04:54:20.192264: val_loss 3.0084 
2025-05-04 04:54:20.223521: PSNR 18.3077 
2025-05-04 04:54:20.239147: Epoch time: 1385.09 s 
2025-05-04 04:54:21.416099:  
2025-05-04 04:54:21.447351: Epoch 782 
2025-05-04 04:54:21.462978: Current learning rate: 2.39e-05 
2025-05-04 05:17:34.260480: train_loss 2.6367 
2025-05-04 05:17:34.276104: val_loss 2.7297 
2025-05-04 05:17:34.307353: PSNR 19.8343 
2025-05-04 05:17:34.322978: Epoch time: 1392.84 s 
2025-05-04 05:17:35.661808:  
2025-05-04 05:17:35.677434: Epoch 783 
2025-05-04 05:17:35.708689: Current learning rate: 2.37e-05 
2025-05-04 05:40:56.582423: train_loss 2.7409 
2025-05-04 05:40:56.613673: val_loss 2.7587 
2025-05-04 05:40:56.629299: PSNR 19.5471 
2025-05-04 05:40:56.644927: Epoch time: 1400.92 s 
2025-05-04 05:40:58.043843:  
2025-05-04 05:40:58.059463: Epoch 784 
2025-05-04 05:40:58.090715: Current learning rate: 2.35e-05 
2025-05-04 06:04:10.981455: train_loss 2.621 
2025-05-04 06:04:10.997079: val_loss 2.5748 
2025-05-04 06:04:11.028327: PSNR 20.6984 
2025-05-04 06:04:11.043952: Epoch time: 1392.95 s 
2025-05-04 06:04:12.065640:  
2025-05-04 06:04:12.096883: Epoch 785 
2025-05-04 06:04:12.112510: Current learning rate: 2.33e-05 
2025-05-04 06:27:23.354625: train_loss 2.713 
2025-05-04 06:27:23.385340: val_loss 2.58 
2025-05-04 06:27:23.400965: PSNR 20.4324 
2025-05-04 06:27:23.416592: Epoch time: 1391.3 s 
2025-05-04 06:27:24.722626:  
2025-05-04 06:27:24.738260: Epoch 786 
2025-05-04 06:27:24.769508: Current learning rate: 2.31e-05 
2025-05-04 06:50:44.346712: train_loss 2.6724 
2025-05-04 06:50:44.377963: val_loss 2.7837 
2025-05-04 06:50:44.377963: PSNR 19.8124 
2025-05-04 06:50:44.409216: Epoch time: 1399.62 s 
2025-05-04 06:50:45.472326:  
2025-05-04 06:50:45.487960: Epoch 787 
2025-05-04 06:50:45.503581: Current learning rate: 2.29e-05 
2025-05-04 07:13:49.489205: train_loss 2.5916 
2025-05-04 07:13:49.520457: val_loss 2.9042 
2025-05-04 07:13:49.536095: PSNR 19.4276 
2025-05-04 07:13:49.567342: Epoch time: 1384.03 s 
2025-05-04 07:13:50.688447:  
2025-05-04 07:13:50.710443: Epoch 788 
2025-05-04 07:13:50.726068: Current learning rate: 2.27e-05 
2025-05-04 07:36:55.393381: train_loss 2.6428 
2025-05-04 07:36:55.424634: val_loss 2.7814 
2025-05-04 07:36:55.424634: PSNR 19.4164 
2025-05-04 07:36:55.455888: Epoch time: 1384.72 s 
2025-05-04 07:36:56.496328:  
2025-05-04 07:36:56.496328: Epoch 789 
2025-05-04 07:36:56.511951: Current learning rate: 2.25e-05 
2025-05-04 08:00:00.998150: train_loss 2.6515 
2025-05-04 08:00:01.045025: val_loss 2.4773 
2025-05-04 08:00:01.076280: PSNR 20.4963 
2025-05-04 08:00:01.091907: Epoch time: 1384.52 s 
2025-05-04 08:00:02.625315:  
2025-05-04 08:00:02.640941: Epoch 790 
2025-05-04 08:00:02.656569: Current learning rate: 2.23e-05 
2025-05-04 08:23:25.288230: train_loss 2.6053 
2025-05-04 08:23:25.319481: val_loss 2.6765 
2025-05-04 08:23:25.319481: PSNR 19.9204 
2025-05-04 08:23:25.350748: Epoch time: 1402.68 s 
2025-05-04 08:23:26.699810:  
2025-05-04 08:23:26.731047: Epoch 791 
2025-05-04 08:23:26.746675: Current learning rate: 2.21e-05 
2025-05-04 08:46:31.689305: train_loss 2.7092 
2025-05-04 08:46:31.704930: val_loss 2.6772 
2025-05-04 08:46:31.720557: PSNR 20.0486 
2025-05-04 08:46:31.751810: Epoch time: 1385.01 s 
2025-05-04 08:46:33.242228:  
2025-05-04 08:46:33.257846: Epoch 792 
2025-05-04 08:46:33.273482: Current learning rate: 2.19e-05 
2025-05-04 09:09:49.257659: train_loss 2.7228 
2025-05-04 09:09:49.288910: val_loss 2.6476 
2025-05-04 09:09:49.304541: PSNR 19.1461 
2025-05-04 09:09:49.320166: Epoch time: 1396.03 s 
2025-05-04 09:09:50.440170:  
2025-05-04 09:09:50.455791: Epoch 793 
2025-05-04 09:09:50.471417: Current learning rate: 2.17e-05 
2025-05-04 09:32:54.297179: train_loss 2.6489 
2025-05-04 09:32:54.328428: val_loss 2.5449 
2025-05-04 09:32:54.344057: PSNR 20.324 
2025-05-04 09:32:54.359680: Epoch time: 1383.87 s 
2025-05-04 09:32:55.390929:  
2025-05-04 09:32:55.422203: Epoch 794 
2025-05-04 09:32:55.437811: Current learning rate: 2.15e-05 
2025-05-04 09:56:15.433357: train_loss 2.7336 
2025-05-04 09:56:15.448994: val_loss 2.6497 
2025-05-04 09:56:15.472716: PSNR 19.9185 
2025-05-04 09:56:15.489403: Epoch time: 1400.04 s 
2025-05-04 09:56:16.692732:  
2025-05-04 09:56:16.723983: Epoch 795 
2025-05-04 09:56:16.723983: Current learning rate: 2.13e-05 
2025-05-04 10:19:34.189092: train_loss 2.5971 
2025-05-04 10:19:34.220342: val_loss 2.5036 
2025-05-04 10:19:34.244887: PSNR 20.9728 
2025-05-04 10:19:34.244887: Epoch time: 1397.5 s 
2025-05-04 10:19:34.276152: Yayy! New best EMA pseudo PSNR: 19.9575 
2025-05-04 10:19:42.374953:  
2025-05-04 10:19:42.390630: Epoch 796 
2025-05-04 10:19:42.427060: Current learning rate: 2.11e-05 
2025-05-04 10:43:01.406042: train_loss 2.7186 
2025-05-04 10:43:01.437293: val_loss 2.6481 
2025-05-04 10:43:01.468559: PSNR 19.918 
2025-05-04 10:43:01.468559: Epoch time: 1399.03 s 
2025-05-04 10:43:02.743882:  
2025-05-04 10:43:02.759516: Epoch 797 
2025-05-04 10:43:02.790756: Current learning rate: 2.09e-05 
2025-05-04 11:06:06.783926: train_loss 2.6397 
2025-05-04 11:06:06.799553: val_loss 2.5776 
2025-05-04 11:06:06.830811: PSNR 20.3307 
2025-05-04 11:06:06.846433: Epoch time: 1384.04 s 
2025-05-04 11:06:06.862058: Yayy! New best EMA pseudo PSNR: 19.9912 
2025-05-04 11:06:15.955171:  
2025-05-04 11:06:15.970796: Epoch 798 
2025-05-04 11:06:15.986423: Current learning rate: 2.07e-05 
2025-05-04 11:29:25.676782: train_loss 2.7368 
2025-05-04 11:29:25.708028: val_loss 2.9192 
2025-05-04 11:29:25.723660: PSNR 18.8496 
2025-05-04 11:29:25.739285: Epoch time: 1389.74 s 
2025-05-04 11:29:27.376557:  
2025-05-04 11:29:27.392171: Epoch 799 
2025-05-04 11:29:27.407794: Current learning rate: 2.05e-05 
2025-05-04 11:52:38.840887: train_loss 2.6898 
2025-05-04 11:52:38.856514: val_loss 2.5723 
2025-05-04 11:52:38.872145: PSNR 19.8763 
2025-05-04 11:52:38.887771: Epoch time: 1391.48 s 
2025-05-04 11:52:47.035213:  
2025-05-04 11:52:47.066461: Epoch 800 
2025-05-04 11:52:47.083086: Current learning rate: 2.03e-05 
2025-05-04 12:16:06.587361: train_loss 2.6235 
2025-05-04 12:16:06.618612: val_loss 2.7303 
2025-05-04 12:16:06.634239: PSNR 19.7208 
2025-05-04 12:16:06.634239: Epoch time: 1399.57 s 
2025-05-04 12:16:07.769032:  
2025-05-04 12:16:07.784651: Epoch 801 
2025-05-04 12:16:07.815899: Current learning rate: 2.01e-05 
2025-05-04 12:39:29.263354: train_loss 2.6511 
2025-05-04 12:39:29.294607: val_loss 2.9705 
2025-05-04 12:39:29.310235: PSNR 18.7887 
2025-05-04 12:39:29.341484: Epoch time: 1401.51 s 
2025-05-04 12:39:30.430389:  
2025-05-04 12:39:30.446029: Epoch 802 
2025-05-04 12:39:30.477272: Current learning rate: 1.99e-05 
2025-05-04 13:02:49.574319: train_loss 2.6278 
2025-05-04 13:02:49.605568: val_loss 2.7462 
2025-05-04 13:02:49.621199: PSNR 19.1146 
2025-05-04 13:02:49.636825: Epoch time: 1399.14 s 
2025-05-04 13:02:51.099852:  
2025-05-04 13:02:51.131101: Epoch 803 
2025-05-04 13:02:51.146729: Current learning rate: 1.97e-05 
2025-05-04 13:26:13.216078: train_loss 2.6131 
2025-05-04 13:26:13.278577: val_loss 2.9106 
2025-05-04 13:26:13.294201: PSNR 19.3572 
2025-05-04 13:26:13.309828: Epoch time: 1402.12 s 
2025-05-04 13:26:14.506239:  
2025-05-04 13:26:14.537490: Epoch 804 
2025-05-04 13:26:14.553119: Current learning rate: 1.96e-05 
2025-05-04 13:49:26.973931: train_loss 2.6253 
2025-05-04 13:49:26.989564: val_loss 2.8287 
2025-05-04 13:49:27.020810: PSNR 19.7503 
2025-05-04 13:49:27.036437: Epoch time: 1392.47 s 
2025-05-04 13:49:28.405068:  
2025-05-04 13:49:28.420679: Epoch 805 
2025-05-04 13:49:28.451927: Current learning rate: 1.94e-05 
2025-05-04 14:12:49.889024: train_loss 2.6189 
2025-05-04 14:12:49.920271: val_loss 2.7928 
2025-05-04 14:12:49.935896: PSNR 19.5291 
2025-05-04 14:12:49.951521: Epoch time: 1401.5 s 
2025-05-04 14:12:51.179461:  
2025-05-04 14:12:51.210719: Epoch 806 
2025-05-04 14:12:51.226343: Current learning rate: 1.92e-05 
2025-05-04 14:35:56.232061: train_loss 2.6396 
2025-05-04 14:35:56.289215: val_loss 2.7829 
2025-05-04 14:35:56.304848: PSNR 19.1548 
2025-05-04 14:35:56.320472: Epoch time: 1385.05 s 
2025-05-04 14:35:57.787507:  
2025-05-04 14:35:57.818752: Epoch 807 
2025-05-04 14:35:57.834388: Current learning rate: 1.9e-05 
2025-05-04 14:59:17.295265: train_loss 2.6869 
2025-05-04 14:59:17.310893: val_loss 2.5251 
2025-05-04 14:59:17.342139: PSNR 20.153 
2025-05-04 14:59:17.357777: Epoch time: 1399.51 s 
2025-05-04 14:59:18.782351:  
2025-05-04 14:59:18.813607: Epoch 808 
2025-05-04 14:59:18.829235: Current learning rate: 1.88e-05 
2025-05-04 15:22:22.983359: train_loss 2.558 
2025-05-04 15:22:23.009879: val_loss 2.9577 
2025-05-04 15:22:23.025514: PSNR 19.474 
2025-05-04 15:22:23.041135: Epoch time: 1384.22 s 
2025-05-04 15:22:23.997030:  
2025-05-04 15:22:24.012656: Epoch 809 
2025-05-04 15:22:24.028281: Current learning rate: 1.86e-05 
2025-05-04 15:45:29.733032: train_loss 2.6339 
2025-05-04 15:45:29.748664: val_loss 2.6332 
2025-05-04 15:45:29.764289: PSNR 19.8395 
2025-05-04 15:45:29.779912: Epoch time: 1385.74 s 
2025-05-04 15:45:30.673764:  
2025-05-04 15:45:30.689389: Epoch 810 
2025-05-04 15:45:30.705018: Current learning rate: 1.85e-05 
2025-05-04 16:08:34.342437: train_loss 2.6559 
2025-05-04 16:08:34.373687: val_loss 2.6562 
2025-05-04 16:08:34.389315: PSNR 20.027 
2025-05-04 16:08:34.389315: Epoch time: 1383.68 s 
2025-05-04 16:08:35.365928:  
2025-05-04 16:08:35.381559: Epoch 811 
2025-05-04 16:08:35.397191: Current learning rate: 1.83e-05 
2025-05-04 16:31:46.954926: train_loss 2.6008 
2025-05-04 16:31:46.986172: val_loss 2.8498 
2025-05-04 16:31:47.001803: PSNR 19.2595 
2025-05-04 16:31:47.017426: Epoch time: 1391.59 s 
2025-05-04 16:31:48.476098:  
2025-05-04 16:31:48.491711: Epoch 812 
2025-05-04 16:31:48.507340: Current learning rate: 1.81e-05 
2025-05-04 16:55:01.183009: train_loss 2.6617 
2025-05-04 16:55:01.198638: val_loss 2.6016 
2025-05-04 16:55:01.214262: PSNR 20.2276 
2025-05-04 16:55:01.229889: Epoch time: 1392.71 s 
2025-05-04 16:55:02.349778:  
2025-05-04 16:55:02.365403: Epoch 813 
2025-05-04 16:55:02.381050: Current learning rate: 1.79e-05 
2025-05-04 17:18:20.654336: train_loss 2.6648 
2025-05-04 17:18:20.669966: val_loss 2.8506 
2025-05-04 17:18:20.685594: PSNR 19.1743 
2025-05-04 17:18:20.716838: Epoch time: 1398.32 s 
2025-05-04 17:18:21.725976:  
2025-05-04 17:18:21.757227: Epoch 814 
2025-05-04 17:18:21.772860: Current learning rate: 1.77e-05 
2025-05-04 17:41:32.835224: train_loss 2.6213 
2025-05-04 17:41:32.852905: val_loss 2.6426 
2025-05-04 17:41:32.868533: PSNR 20.3118 
2025-05-04 17:41:32.884161: Epoch time: 1391.11 s 
2025-05-04 17:41:34.326769:  
2025-05-04 17:41:34.358031: Epoch 815 
2025-05-04 17:41:34.358031: Current learning rate: 1.76e-05 
2025-05-04 18:04:38.417217: train_loss 2.6729 
2025-05-04 18:04:38.442699: val_loss 2.59 
2025-05-04 18:04:38.458342: PSNR 19.8819 
2025-05-04 18:04:38.458342: Epoch time: 1384.09 s 
2025-05-04 18:04:39.547650:  
2025-05-04 18:04:39.563276: Epoch 816 
2025-05-04 18:04:39.594528: Current learning rate: 1.74e-05 
2025-05-04 18:27:59.493290: train_loss 2.6133 
2025-05-04 18:27:59.508917: val_loss 2.657 
2025-05-04 18:27:59.524549: PSNR 20.0664 
2025-05-04 18:27:59.540169: Epoch time: 1399.96 s 
2025-05-04 18:28:00.689727:  
2025-05-04 18:28:00.720979: Epoch 817 
2025-05-04 18:28:00.736612: Current learning rate: 1.72e-05 
2025-05-04 18:51:15.941226: train_loss 2.613 
2025-05-04 18:51:15.956854: val_loss 2.8452 
2025-05-04 18:51:15.972486: PSNR 19.4773 
2025-05-04 18:51:15.988106: Epoch time: 1395.25 s 
2025-05-04 18:51:17.190729:  
2025-05-04 18:51:17.206366: Epoch 818 
2025-05-04 18:51:17.221976: Current learning rate: 1.7e-05 
2025-05-04 19:14:37.345598: train_loss 2.6412 
2025-05-04 19:14:37.361227: val_loss 3.0764 
2025-05-04 19:14:37.376866: PSNR 18.7443 
2025-05-04 19:14:37.392495: Epoch time: 1400.15 s 
2025-05-04 19:14:38.370848:  
2025-05-04 19:14:38.386492: Epoch 819 
2025-05-04 19:14:38.417720: Current learning rate: 1.68e-05 
2025-05-04 19:37:55.754053: train_loss 2.6122 
2025-05-04 19:37:55.769680: val_loss 2.8714 
2025-05-04 19:37:55.785307: PSNR 19.312 
2025-05-04 19:37:55.816553: Epoch time: 1397.38 s 
2025-05-04 19:37:56.857522:  
2025-05-04 19:37:56.873166: Epoch 820 
2025-05-04 19:37:56.888794: Current learning rate: 1.67e-05 
2025-05-04 20:01:10.024409: train_loss 2.5462 
2025-05-04 20:01:10.040039: val_loss 2.6825 
2025-05-04 20:01:10.055674: PSNR 20.0132 
2025-05-04 20:01:10.071296: Epoch time: 1393.17 s 
2025-05-04 20:01:11.404954:  
2025-05-04 20:01:11.420578: Epoch 821 
2025-05-04 20:01:11.451820: Current learning rate: 1.65e-05 
2025-05-04 20:24:29.380767: train_loss 2.6078 
2025-05-04 20:24:29.420329: val_loss 2.8229 
2025-05-04 20:24:29.435959: PSNR 19.6977 
2025-05-04 20:24:29.451585: Epoch time: 1397.98 s 
2025-05-04 20:24:30.639723:  
2025-05-04 20:24:30.655348: Epoch 822 
2025-05-04 20:24:30.670981: Current learning rate: 1.63e-05 
2025-05-04 20:47:34.162361: train_loss 2.6404 
2025-05-04 20:47:34.193614: val_loss 2.8672 
2025-05-04 20:47:34.224871: PSNR 19.8101 
2025-05-04 20:47:34.240494: Epoch time: 1383.52 s 
2025-05-04 20:47:35.421362:  
2025-05-04 20:47:35.452619: Epoch 823 
2025-05-04 20:47:35.468236: Current learning rate: 1.62e-05 
2025-05-04 21:10:52.619071: train_loss 2.613 
2025-05-04 21:10:52.634697: val_loss 2.6152 
2025-05-04 21:10:52.650328: PSNR 19.9395 
2025-05-04 21:10:52.665954: Epoch time: 1397.2 s 
2025-05-04 21:10:53.565579:  
2025-05-04 21:10:53.581204: Epoch 824 
2025-05-04 21:10:53.596831: Current learning rate: 1.6e-05 
2025-05-04 21:34:11.059018: train_loss 2.5917 
2025-05-04 21:34:11.074649: val_loss 3.1565 
2025-05-04 21:34:11.090277: PSNR 18.4795 
2025-05-04 21:34:11.105898: Epoch time: 1397.49 s 
2025-05-04 21:34:12.168403:  
2025-05-04 21:34:12.184025: Epoch 825 
2025-05-04 21:34:12.199651: Current learning rate: 1.58e-05 
2025-05-04 21:57:22.008729: train_loss 2.62 
2025-05-04 21:57:22.039980: val_loss 2.8748 
2025-05-04 21:57:22.055610: PSNR 19.7143 
2025-05-04 21:57:22.086867: Epoch time: 1389.86 s 
2025-05-04 21:57:23.690955:  
2025-05-04 21:57:23.722205: Epoch 826 
2025-05-04 21:57:23.737834: Current learning rate: 1.56e-05 
2025-05-04 22:20:40.464920: train_loss 2.6118 
2025-05-04 22:20:40.496171: val_loss 2.5373 
2025-05-04 22:20:40.511800: PSNR 20.1814 
2025-05-04 22:20:40.511800: Epoch time: 1396.77 s 
2025-05-04 22:20:41.802241:  
2025-05-04 22:20:41.817868: Epoch 827 
2025-05-04 22:20:41.833504: Current learning rate: 1.55e-05 
2025-05-04 22:44:02.601406: train_loss 2.5844 
2025-05-04 22:44:02.632664: val_loss 2.8485 
2025-05-04 22:44:02.663915: PSNR 19.3353 
2025-05-04 22:44:02.679537: Epoch time: 1400.81 s 
2025-05-04 22:44:03.656492:  
2025-05-04 22:44:03.672118: Epoch 828 
2025-05-04 22:44:03.703374: Current learning rate: 1.53e-05 
2025-05-04 23:07:23.284141: train_loss 2.6864 
2025-05-04 23:07:23.315392: val_loss 2.5392 
2025-05-04 23:07:23.331023: PSNR 20.4096 
2025-05-04 23:07:23.346648: Epoch time: 1399.63 s 
2025-05-04 23:07:24.739923:  
2025-05-04 23:07:24.755546: Epoch 829 
2025-05-04 23:07:24.786801: Current learning rate: 1.51e-05 
2025-05-04 23:30:27.662119: train_loss 2.6315 
2025-05-04 23:30:27.662119: val_loss 2.7376 
2025-05-04 23:30:27.693374: PSNR 20.0921 
2025-05-04 23:30:27.693374: Epoch time: 1382.92 s 
2025-05-04 23:30:28.596040:  
2025-05-04 23:30:28.627303: Epoch 830 
2025-05-04 23:30:28.642920: Current learning rate: 1.5e-05 
2025-05-04 23:53:38.690364: train_loss 2.666 
2025-05-04 23:53:38.721614: val_loss 2.6749 
2025-05-04 23:53:38.737242: PSNR 20.1716 
2025-05-04 23:53:38.752866: Epoch time: 1390.09 s 
2025-05-04 23:53:40.055031:  
2025-05-04 23:53:40.070679: Epoch 831 
2025-05-04 23:53:40.086283: Current learning rate: 1.48e-05 
2025-05-05 00:16:51.436952: train_loss 2.615 
2025-05-05 00:16:51.452578: val_loss 3.2284 
2025-05-05 00:16:51.483833: PSNR 18.4772 
2025-05-05 00:16:51.499456: Epoch time: 1391.4 s 
2025-05-05 00:16:52.374332:  
2025-05-05 00:16:52.405582: Epoch 832 
2025-05-05 00:16:52.421216: Current learning rate: 1.46e-05 
2025-05-05 00:40:13.569221: train_loss 2.5878 
2025-05-05 00:40:13.600483: val_loss 2.8692 
2025-05-05 00:40:13.616100: PSNR 19.3339 
2025-05-05 00:40:13.631726: Epoch time: 1401.19 s 
2025-05-05 00:40:14.628105:  
2025-05-05 00:40:14.659375: Epoch 833 
2025-05-05 00:40:14.674989: Current learning rate: 1.45e-05 
2025-05-05 01:03:31.400206: train_loss 2.5785 
2025-05-05 01:03:31.431459: val_loss 2.5607 
2025-05-05 01:03:31.447090: PSNR 20.0632 
2025-05-05 01:03:31.462714: Epoch time: 1396.77 s 
2025-05-05 01:03:32.400215:  
2025-05-05 01:03:32.415847: Epoch 834 
2025-05-05 01:03:32.453484: Current learning rate: 1.43e-05 
2025-05-05 01:26:41.410648: train_loss 2.5824 
2025-05-05 01:26:41.441899: val_loss 2.7929 
2025-05-05 01:26:41.457529: PSNR 19.6811 
2025-05-05 01:26:41.473149: Epoch time: 1389.01 s 
2025-05-05 01:26:43.003557:  
2025-05-05 01:26:43.034803: Epoch 835 
2025-05-05 01:26:43.050432: Current learning rate: 1.42e-05 
2025-05-05 01:50:02.663234: train_loss 2.6091 
2025-05-05 01:50:02.678865: val_loss 2.9196 
2025-05-05 01:50:02.694484: PSNR 19.0235 
2025-05-05 01:50:02.710119: Epoch time: 1399.66 s 
2025-05-05 01:50:04.042955:  
2025-05-05 01:50:04.058576: Epoch 836 
2025-05-05 01:50:04.074207: Current learning rate: 1.4e-05 
2025-05-05 02:13:08.987617: train_loss 2.6093 
2025-05-05 02:13:09.018867: val_loss 2.8208 
2025-05-05 02:13:09.034494: PSNR 19.4784 
2025-05-05 02:13:09.050126: Epoch time: 1384.96 s 
2025-05-05 02:13:10.082768:  
2025-05-05 02:13:10.104787: Epoch 837 
2025-05-05 02:13:10.120399: Current learning rate: 1.38e-05 
2025-05-05 02:36:22.384736: train_loss 2.5799 
2025-05-05 02:36:22.446418: val_loss 2.7769 
2025-05-05 02:36:22.477668: PSNR 19.5363 
2025-05-05 02:36:22.477668: Epoch time: 1392.3 s 
2025-05-05 02:36:23.892879:  
2025-05-05 02:36:23.908504: Epoch 838 
2025-05-05 02:36:23.932190: Current learning rate: 1.37e-05 
2025-05-05 02:59:44.667173: train_loss 2.6456 
2025-05-05 02:59:44.682803: val_loss 2.7373 
2025-05-05 02:59:44.698431: PSNR 19.9685 
2025-05-05 02:59:44.714053: Epoch time: 1400.79 s 
2025-05-05 02:59:45.938946:  
2025-05-05 02:59:45.970196: Epoch 839 
2025-05-05 02:59:45.970196: Current learning rate: 1.35e-05 
2025-05-05 03:23:02.605425: train_loss 2.664 
2025-05-05 03:23:02.631081: val_loss 2.7235 
2025-05-05 03:23:02.646706: PSNR 19.486 
2025-05-05 03:23:02.677957: Epoch time: 1396.65 s 
2025-05-05 03:23:03.811415:  
2025-05-05 03:23:03.827040: Epoch 840 
2025-05-05 03:23:03.842670: Current learning rate: 1.34e-05 
2025-05-05 03:46:21.411693: train_loss 2.6743 
2025-05-05 03:46:21.442943: val_loss 2.5927 
2025-05-05 03:46:21.458567: PSNR 19.9196 
2025-05-05 03:46:21.474195: Epoch time: 1397.62 s 
2025-05-05 03:46:22.758954:  
2025-05-05 03:46:22.774579: Epoch 841 
2025-05-05 03:46:22.790207: Current learning rate: 1.32e-05 
2025-05-05 04:09:26.111751: train_loss 2.5946 
2025-05-05 04:09:26.127384: val_loss 2.6551 
2025-05-05 04:09:26.158629: PSNR 20.0901 
2025-05-05 04:09:26.174264: Epoch time: 1383.35 s 
2025-05-05 04:09:27.546231:  
2025-05-05 04:09:27.561855: Epoch 842 
2025-05-05 04:09:27.593111: Current learning rate: 1.3e-05 
2025-05-05 04:32:48.566204: train_loss 2.6046 
2025-05-05 04:32:48.608080: val_loss 2.7266 
2025-05-05 04:32:48.623703: PSNR 19.5406 
2025-05-05 04:32:48.639334: Epoch time: 1401.04 s 
2025-05-05 04:32:49.644222:  
2025-05-05 04:32:49.675488: Epoch 843 
2025-05-05 04:32:49.675488: Current learning rate: 1.29e-05 
2025-05-05 04:55:53.445706: train_loss 2.6572 
2025-05-05 04:55:53.476960: val_loss 2.7739 
2025-05-05 04:55:53.492591: PSNR 19.5534 
2025-05-05 04:55:53.492591: Epoch time: 1383.8 s 
2025-05-05 04:55:54.512832:  
2025-05-05 04:55:54.528459: Epoch 844 
2025-05-05 04:55:54.544092: Current learning rate: 1.27e-05 
2025-05-05 05:19:15.571041: train_loss 2.5788 
2025-05-05 05:19:15.602295: val_loss 2.8347 
2025-05-05 05:19:15.617934: PSNR 19.1361 
2025-05-05 05:19:15.633547: Epoch time: 1401.06 s 
2025-05-05 05:19:16.580204:  
2025-05-05 05:19:16.595829: Epoch 845 
2025-05-05 05:19:16.611477: Current learning rate: 1.26e-05 
2025-05-05 05:42:36.102109: train_loss 2.5764 
2025-05-05 05:42:36.117737: val_loss 2.7016 
2025-05-05 05:42:36.133367: PSNR 19.843 
2025-05-05 05:42:36.133367: Epoch time: 1399.54 s 
2025-05-05 05:42:37.148993:  
2025-05-05 05:42:37.164618: Epoch 846 
2025-05-05 05:42:37.195869: Current learning rate: 1.24e-05 
2025-05-05 06:05:40.710785: train_loss 2.5778 
2025-05-05 06:05:40.742043: val_loss 2.7861 
2025-05-05 06:05:40.757673: PSNR 20.1255 
2025-05-05 06:05:40.757673: Epoch time: 1383.56 s 
2025-05-05 06:05:41.695173:  
2025-05-05 06:05:41.710815: Epoch 847 
2025-05-05 06:05:41.726423: Current learning rate: 1.23e-05 
2025-05-05 06:28:59.653994: train_loss 2.63 
2025-05-05 06:28:59.689805: val_loss 2.9475 
2025-05-05 06:28:59.721055: PSNR 19.1554 
2025-05-05 06:28:59.721055: Epoch time: 1397.96 s 
2025-05-05 06:29:00.902292:  
2025-05-05 06:29:00.917945: Epoch 848 
2025-05-05 06:29:00.933542: Current learning rate: 1.21e-05 
2025-05-05 06:52:04.099728: train_loss 2.6989 
2025-05-05 06:52:04.115355: val_loss 2.5783 
2025-05-05 06:52:04.130984: PSNR 20.0312 
2025-05-05 06:52:04.146610: Epoch time: 1383.21 s 
2025-05-05 06:52:05.283676:  
2025-05-05 06:52:05.314926: Epoch 849 
2025-05-05 06:52:05.314926: Current learning rate: 1.2e-05 
2025-05-05 07:15:27.363210: train_loss 2.5517 
2025-05-05 07:15:27.378834: val_loss 2.7657 
2025-05-05 07:15:27.398478: PSNR 19.874 
2025-05-05 07:15:27.414124: Epoch time: 1402.08 s 
2025-05-05 07:15:34.536391:  
2025-05-05 07:15:34.567641: Epoch 850 
2025-05-05 07:15:34.583273: Current learning rate: 1.18e-05 
2025-05-05 07:38:53.216290: train_loss 2.631 
2025-05-05 07:38:53.240888: val_loss 2.6455 
2025-05-05 07:38:53.256523: PSNR 19.9649 
2025-05-05 07:38:53.272146: Epoch time: 1398.68 s 
2025-05-05 07:38:54.527364:  
2025-05-05 07:38:54.542989: Epoch 851 
2025-05-05 07:38:54.558615: Current learning rate: 1.17e-05 
2025-05-05 08:01:56.960473: train_loss 2.6238 
2025-05-05 08:01:56.991721: val_loss 2.8162 
2025-05-05 08:01:56.991721: PSNR 19.8144 
2025-05-05 08:01:57.015636: Epoch time: 1382.43 s 
2025-05-05 08:01:58.086663:  
2025-05-05 08:01:58.086663: Epoch 852 
2025-05-05 08:01:58.102286: Current learning rate: 1.15e-05 
2025-05-05 08:24:52.326811: train_loss 2.5885 
2025-05-05 08:24:52.342445: val_loss 3.0794 
2025-05-05 08:24:52.358068: PSNR 19.1944 
2025-05-05 08:24:52.373692: Epoch time: 1374.26 s 
2025-05-05 08:24:53.549864:  
2025-05-05 08:24:53.581116: Epoch 853 
2025-05-05 08:24:53.596745: Current learning rate: 1.14e-05 
2025-05-05 08:48:06.250335: train_loss 2.62 
2025-05-05 08:48:06.250335: val_loss 2.7153 
2025-05-05 08:48:06.281586: PSNR 19.5886 
2025-05-05 08:48:06.297213: Epoch time: 1392.7 s 
2025-05-05 08:48:07.109714:  
2025-05-05 08:48:07.125336: Epoch 854 
2025-05-05 08:48:07.156591: Current learning rate: 1.12e-05 
2025-05-05 09:11:09.216695: train_loss 2.5763 
2025-05-05 09:11:09.247950: val_loss 2.7836 
2025-05-05 09:11:09.279194: PSNR 20.0984 
2025-05-05 09:11:09.294821: Epoch time: 1382.11 s 
2025-05-05 09:11:10.359696:  
2025-05-05 09:11:10.390945: Epoch 855 
2025-05-05 09:11:10.406573: Current learning rate: 1.11e-05 
2025-05-05 09:34:21.209489: train_loss 2.6191 
2025-05-05 09:34:21.209489: val_loss 2.7864 
2025-05-05 09:34:21.225117: PSNR 19.4004 
2025-05-05 09:34:21.240740: Epoch time: 1390.85 s 
2025-05-05 09:34:22.249846:  
2025-05-05 09:34:22.281096: Epoch 856 
2025-05-05 09:34:22.296720: Current learning rate: 1.1e-05 
2025-05-05 09:57:16.566809: train_loss 2.5638 
2025-05-05 09:57:16.598058: val_loss 2.7494 
2025-05-05 09:57:16.613690: PSNR 19.7531 
2025-05-05 09:57:16.613690: Epoch time: 1374.32 s 
2025-05-05 09:57:17.654614:  
2025-05-05 09:57:17.670237: Epoch 857 
2025-05-05 09:57:17.685861: Current learning rate: 1.08e-05 
2025-05-05 10:20:19.179231: train_loss 2.5709 
2025-05-05 10:20:19.194860: val_loss 2.7585 
2025-05-05 10:20:19.226107: PSNR 19.4279 
2025-05-05 10:20:19.226107: Epoch time: 1381.54 s 
2025-05-05 10:20:20.198504:  
2025-05-05 10:20:20.214136: Epoch 858 
2025-05-05 10:20:20.229754: Current learning rate: 1.07e-05 
2025-05-05 10:43:28.068367: train_loss 2.6445 
2025-05-05 10:43:28.099622: val_loss 2.8037 
2025-05-05 10:43:28.099622: PSNR 19.2965 
2025-05-05 10:43:28.115242: Epoch time: 1387.87 s 
2025-05-05 10:43:29.037115:  
2025-05-05 10:43:29.052742: Epoch 859 
2025-05-05 10:43:29.083992: Current learning rate: 1.05e-05 
2025-05-05 11:06:39.831575: train_loss 2.6915 
2025-05-05 11:06:39.847198: val_loss 2.7546 
2025-05-05 11:06:39.862827: PSNR 19.0841 
2025-05-05 11:06:39.878450: Epoch time: 1390.79 s 
2025-05-05 11:06:40.732044:  
2025-05-05 11:06:40.747670: Epoch 860 
2025-05-05 11:06:40.763297: Current learning rate: 1.04e-05 
2025-05-05 11:29:50.858770: train_loss 2.5539 
2025-05-05 11:29:50.890019: val_loss 2.7959 
2025-05-05 11:29:50.905645: PSNR 19.1563 
2025-05-05 11:29:50.926796: Epoch time: 1390.13 s 
2025-05-05 11:29:51.858794:  
2025-05-05 11:29:51.874415: Epoch 861 
2025-05-05 11:29:51.890041: Current learning rate: 1.03e-05 
2025-05-05 11:52:52.300508: train_loss 2.7114 
2025-05-05 11:52:52.331761: val_loss 2.6217 
2025-05-05 11:52:52.331761: PSNR 19.958 
2025-05-05 11:52:52.363009: Epoch time: 1380.44 s 
2025-05-05 11:52:53.398020:  
2025-05-05 11:52:53.418925: Epoch 862 
2025-05-05 11:52:53.434550: Current learning rate: 1.01e-05 
2025-05-05 12:16:00.826348: train_loss 2.6496 
2025-05-05 12:16:00.841977: val_loss 2.6238 
2025-05-05 12:16:00.873226: PSNR 20.2388 
2025-05-05 12:16:00.888851: Epoch time: 1387.45 s 
2025-05-05 12:16:01.898814:  
2025-05-05 12:16:01.930062: Epoch 863 
2025-05-05 12:16:01.945692: Current learning rate: 1e-05 
2025-05-05 12:38:56.606700: train_loss 2.6369 
2025-05-05 12:38:56.622329: val_loss 2.7969 
2025-05-05 12:38:56.637949: PSNR 19.5997 
2025-05-05 12:38:56.653575: Epoch time: 1374.71 s 
2025-05-05 12:38:57.637940:  
2025-05-05 12:38:57.653565: Epoch 864 
2025-05-05 12:38:57.669190: Current learning rate: 9.8e-06 
2025-05-05 13:02:05.736295: train_loss 2.5946 
2025-05-05 13:02:05.767534: val_loss 2.7597 
2025-05-05 13:02:05.783163: PSNR 20.0708 
2025-05-05 13:02:05.783163: Epoch time: 1388.11 s 
2025-05-05 13:02:06.695708:  
2025-05-05 13:02:06.711329: Epoch 865 
2025-05-05 13:02:06.711329: Current learning rate: 9.7e-06 
2025-05-05 13:25:16.141447: train_loss 2.5859 
2025-05-05 13:25:16.167579: val_loss 2.6523 
2025-05-05 13:25:16.183205: PSNR 20.0859 
2025-05-05 13:25:16.198830: Epoch time: 1389.45 s 
2025-05-05 13:25:17.046910:  
2025-05-05 13:25:17.078161: Epoch 866 
2025-05-05 13:25:17.078161: Current learning rate: 9.6e-06 
2025-05-05 13:48:33.278946: train_loss 2.67 
2025-05-05 13:48:33.294573: val_loss 2.6377 
2025-05-05 13:48:33.310198: PSNR 19.8611 
2025-05-05 13:48:33.325838: Epoch time: 1396.23 s 
2025-05-05 13:48:34.601295:  
2025-05-05 13:48:34.616921: Epoch 867 
2025-05-05 13:48:34.632545: Current learning rate: 9.4e-06 
2025-05-05 14:11:57.427508: train_loss 2.5715 
2025-05-05 14:11:57.443139: val_loss 2.7283 
2025-05-05 14:11:57.458760: PSNR 20.0892 
2025-05-05 14:11:57.474383: Epoch time: 1402.84 s 
2025-05-05 14:11:58.568327:  
2025-05-05 14:11:58.583954: Epoch 868 
2025-05-05 14:11:58.599580: Current learning rate: 9.3e-06 
2025-05-05 14:35:06.853524: train_loss 2.5954 
2025-05-05 14:35:06.900398: val_loss 2.7592 
2025-05-05 14:35:06.900398: PSNR 20.3177 
2025-05-05 14:35:06.916023: Epoch time: 1388.29 s 
2025-05-05 14:35:07.972785:  
2025-05-05 14:35:07.988410: Epoch 869 
2025-05-05 14:35:08.004036: Current learning rate: 9.2e-06 
2025-05-05 14:58:12.394015: train_loss 2.6507 
2025-05-05 14:58:12.425264: val_loss 2.6129 
2025-05-05 14:58:12.425264: PSNR 20.0434 
2025-05-05 14:58:12.440889: Epoch time: 1384.42 s 
2025-05-05 14:58:13.387180:  
2025-05-05 14:58:13.418431: Epoch 870 
2025-05-05 14:58:13.418431: Current learning rate: 9e-06 
2025-05-05 15:21:22.665687: train_loss 2.5981 
2025-05-05 15:21:22.681314: val_loss 2.7651 
2025-05-05 15:21:22.696940: PSNR 19.7134 
2025-05-05 15:21:22.712568: Epoch time: 1389.28 s 
2025-05-05 15:21:23.900068:  
2025-05-05 15:21:23.915694: Epoch 871 
2025-05-05 15:21:23.915694: Current learning rate: 8.9e-06 
2025-05-05 15:44:26.708404: train_loss 2.6358 
2025-05-05 15:44:26.739654: val_loss 2.6726 
2025-05-05 15:44:26.755279: PSNR 19.8082 
2025-05-05 15:44:26.770906: Epoch time: 1382.82 s 
2025-05-05 15:44:27.914828:  
2025-05-05 15:44:27.946079: Epoch 872 
2025-05-05 15:44:27.946079: Current learning rate: 8.8e-06 
2025-05-05 16:07:45.176266: train_loss 2.6115 
2025-05-05 16:07:45.208018: val_loss 2.8483 
2025-05-05 16:07:45.216482: PSNR 19.5216 
2025-05-05 16:07:45.232119: Epoch time: 1397.26 s 
2025-05-05 16:07:46.122744:  
2025-05-05 16:07:46.138369: Epoch 873 
2025-05-05 16:07:46.153997: Current learning rate: 8.7e-06 
2025-05-05 16:30:48.719783: train_loss 2.5463 
2025-05-05 16:30:48.751035: val_loss 2.8309 
2025-05-05 16:30:48.766659: PSNR 19.5696 
2025-05-05 16:30:48.782285: Epoch time: 1382.6 s 
2025-05-05 16:30:50.010184:  
2025-05-05 16:30:50.025810: Epoch 874 
2025-05-05 16:30:50.041433: Current learning rate: 8.5e-06 
2025-05-05 16:54:08.751596: train_loss 2.5754 
2025-05-05 16:54:08.782847: val_loss 2.8205 
2025-05-05 16:54:08.782847: PSNR 19.3032 
2025-05-05 16:54:08.798473: Epoch time: 1398.74 s 
2025-05-05 16:54:09.714640:  
2025-05-05 16:54:09.730263: Epoch 875 
2025-05-05 16:54:09.745894: Current learning rate: 8.4e-06 
2025-05-05 17:17:29.802608: train_loss 2.6592 
2025-05-05 17:17:29.818235: val_loss 2.5218 
2025-05-05 17:17:29.833859: PSNR 20.4527 
2025-05-05 17:17:29.865110: Epoch time: 1400.09 s 
2025-05-05 17:17:30.817938:  
2025-05-05 17:17:30.833563: Epoch 876 
2025-05-05 17:17:30.849189: Current learning rate: 8.3e-06 
2025-05-05 17:40:52.254533: train_loss 2.5763 
2025-05-05 17:40:52.285783: val_loss 2.9594 
2025-05-05 17:40:52.301409: PSNR 18.9368 
2025-05-05 17:40:52.301409: Epoch time: 1401.44 s 
2025-05-05 17:40:53.357450:  
2025-05-05 17:40:53.373078: Epoch 877 
2025-05-05 17:40:53.388702: Current learning rate: 8.2e-06 
2025-05-05 18:03:57.937134: train_loss 2.6437 
2025-05-05 18:03:57.952758: val_loss 2.879 
2025-05-05 18:03:57.968380: PSNR 19.0987 
2025-05-05 18:03:57.984009: Epoch time: 1384.58 s 
2025-05-05 18:03:58.950541:  
2025-05-05 18:03:58.966170: Epoch 878 
2025-05-05 18:03:58.981794: Current learning rate: 8e-06 
2025-05-05 18:27:08.522704: train_loss 2.5953 
2025-05-05 18:27:08.553952: val_loss 2.8163 
2025-05-05 18:27:08.569585: PSNR 19.4525 
2025-05-05 18:27:08.585205: Epoch time: 1389.57 s 
2025-05-05 18:27:09.669569:  
2025-05-05 18:27:09.700820: Epoch 879 
2025-05-05 18:27:09.716444: Current learning rate: 7.9e-06 
2025-05-05 18:50:27.957772: train_loss 2.584 
2025-05-05 18:50:27.989023: val_loss 2.9406 
2025-05-05 18:50:28.004647: PSNR 19.0965 
2025-05-05 18:50:28.004647: Epoch time: 1398.3 s 
2025-05-05 18:50:29.020272:  
2025-05-05 18:50:29.051524: Epoch 880 
2025-05-05 18:50:29.082774: Current learning rate: 7.8e-06 
2025-05-05 19:13:58.773302: train_loss 2.644 
2025-05-05 19:13:58.788930: val_loss 2.6625 
2025-05-05 19:13:58.788930: PSNR 19.5685 
2025-05-05 19:13:58.804556: Epoch time: 1409.75 s 
2025-05-05 19:13:59.829198:  
2025-05-05 19:13:59.844827: Epoch 881 
2025-05-05 19:13:59.860452: Current learning rate: 7.7e-06 
2025-05-05 19:37:29.434935: train_loss 2.5371 
2025-05-05 19:37:29.450558: val_loss 2.9416 
2025-05-05 19:37:29.466183: PSNR 19.4663 
2025-05-05 19:37:29.481809: Epoch time: 1409.61 s 
2025-05-05 19:37:30.641724:  
2025-05-05 19:37:30.657351: Epoch 882 
2025-05-05 19:37:30.672976: Current learning rate: 7.5e-06 
2025-05-05 20:00:58.077228: train_loss 2.6251 
2025-05-05 20:00:58.108481: val_loss 2.9103 
2025-05-05 20:00:58.124107: PSNR 19.1728 
2025-05-05 20:00:58.139729: Epoch time: 1407.44 s 
2025-05-05 20:00:59.317436:  
2025-05-05 20:00:59.333063: Epoch 883 
2025-05-05 20:00:59.348686: Current learning rate: 7.4e-06 
2025-05-05 20:24:22.360386: train_loss 2.68 
2025-05-05 20:24:22.391643: val_loss 2.8006 
2025-05-05 20:24:22.407260: PSNR 18.7497 
2025-05-05 20:24:22.422885: Epoch time: 1403.06 s 
2025-05-05 20:24:23.350271:  
2025-05-05 20:24:23.381521: Epoch 884 
2025-05-05 20:24:23.397151: Current learning rate: 7.3e-06 
2025-05-05 20:47:49.980097: train_loss 2.4859 
2025-05-05 20:47:49.995720: val_loss 2.8706 
2025-05-05 20:47:50.026974: PSNR 19.3948 
2025-05-05 20:47:50.042598: Epoch time: 1406.63 s 
2025-05-05 20:47:51.337967:  
2025-05-05 20:47:51.353589: Epoch 885 
2025-05-05 20:47:51.369218: Current learning rate: 7.2e-06 
2025-05-05 21:11:03.221497: train_loss 2.7126 
2025-05-05 21:11:03.237123: val_loss 2.4746 
2025-05-05 21:11:03.252751: PSNR 19.704 
2025-05-05 21:11:03.268373: Epoch time: 1391.88 s 
2025-05-05 21:11:04.278270:  
2025-05-05 21:11:04.278270: Epoch 886 
2025-05-05 21:11:04.309525: Current learning rate: 7.1e-06 
2025-05-05 21:34:30.766229: train_loss 2.5573 
2025-05-05 21:34:30.797475: val_loss 2.982 
2025-05-05 21:34:30.813102: PSNR 19.1084 
2025-05-05 21:34:30.813102: Epoch time: 1406.5 s 
2025-05-05 21:34:32.150816:  
2025-05-05 21:34:32.166441: Epoch 887 
2025-05-05 21:34:32.182067: Current learning rate: 7e-06 
2025-05-05 21:57:56.916453: train_loss 2.5467 
2025-05-05 21:57:56.947704: val_loss 2.9569 
2025-05-05 21:57:56.947704: PSNR 18.7636 
2025-05-05 21:57:56.963333: Epoch time: 1404.77 s 
2025-05-05 21:57:57.885213:  
2025-05-05 21:57:57.900832: Epoch 888 
2025-05-05 21:57:57.916459: Current learning rate: 6.8e-06 
2025-05-05 22:21:13.568600: train_loss 2.5082 
2025-05-05 22:21:13.599851: val_loss 2.7629 
2025-05-05 22:21:13.615475: PSNR 19.9983 
2025-05-05 22:21:13.631109: Epoch time: 1395.68 s 
2025-05-05 22:21:14.687101:  
2025-05-05 22:21:14.702725: Epoch 889 
2025-05-05 22:21:14.702725: Current learning rate: 6.7e-06 
2025-05-05 22:44:43.353909: train_loss 2.6712 
2025-05-05 22:44:43.375190: val_loss 2.4584 
2025-05-05 22:44:43.390826: PSNR 19.9957 
2025-05-05 22:44:43.406451: Epoch time: 1408.67 s 
2025-05-05 22:44:44.321826:  
2025-05-05 22:44:44.337451: Epoch 890 
2025-05-05 22:44:44.353076: Current learning rate: 6.6e-06 
2025-05-05 23:08:10.768828: train_loss 2.6215 
2025-05-05 23:08:10.800073: val_loss 2.6305 
2025-05-05 23:08:10.800073: PSNR 20.5787 
2025-05-05 23:08:10.815702: Epoch time: 1406.46 s 
2025-05-05 23:08:11.912023:  
2025-05-05 23:08:11.912023: Epoch 891 
2025-05-05 23:08:11.943274: Current learning rate: 6.5e-06 
2025-05-05 23:31:22.011861: train_loss 2.6369 
2025-05-05 23:31:22.027488: val_loss 2.9196 
2025-05-05 23:31:22.043114: PSNR 19.0712 
2025-05-05 23:31:22.058738: Epoch time: 1390.12 s 
2025-05-05 23:31:23.312650:  
2025-05-05 23:31:23.328279: Epoch 892 
2025-05-05 23:31:23.343902: Current learning rate: 6.4e-06 
2025-05-05 23:54:50.754252: train_loss 2.6342 
2025-05-05 23:54:50.769876: val_loss 2.8023 
2025-05-05 23:54:50.785501: PSNR 19.6314 
2025-05-05 23:54:50.801130: Epoch time: 1407.44 s 
2025-05-05 23:54:51.778910:  
2025-05-05 23:54:51.810156: Epoch 893 
2025-05-05 23:54:51.810156: Current learning rate: 6.3e-06 
2025-05-06 00:18:17.635397: train_loss 2.5893 
2025-05-06 00:18:17.651020: val_loss 2.6835 
2025-05-06 00:18:17.666646: PSNR 19.9782 
2025-05-06 00:18:17.682985: Epoch time: 1405.87 s 
2025-05-06 00:18:18.895318:  
2025-05-06 00:18:18.926567: Epoch 894 
2025-05-06 00:18:18.926567: Current learning rate: 6.2e-06 
2025-05-06 00:41:30.779406: train_loss 2.6167 
2025-05-06 00:41:30.810654: val_loss 2.8033 
2025-05-06 00:41:30.826280: PSNR 19.7961 
2025-05-06 00:41:30.841905: Epoch time: 1391.88 s 
2025-05-06 00:41:31.835197:  
2025-05-06 00:41:31.850822: Epoch 895 
2025-05-06 00:41:31.866448: Current learning rate: 6.1e-06 
2025-05-06 01:04:54.541852: train_loss 2.6485 
2025-05-06 01:04:54.557479: val_loss 2.6928 
2025-05-06 01:04:54.573106: PSNR 19.7052 
2025-05-06 01:04:54.588735: Epoch time: 1402.71 s 
2025-05-06 01:04:55.765100:  
2025-05-06 01:04:55.780723: Epoch 896 
2025-05-06 01:04:55.796347: Current learning rate: 6e-06 
2025-05-06 01:28:21.652010: train_loss 2.6674 
2025-05-06 01:28:21.667634: val_loss 2.7929 
2025-05-06 01:28:21.683264: PSNR 18.8598 
2025-05-06 01:28:21.698888: Epoch time: 1405.9 s 
2025-05-06 01:28:22.624109:  
2025-05-06 01:28:22.655359: Epoch 897 
2025-05-06 01:28:22.655359: Current learning rate: 5.9e-06 
2025-05-06 01:51:40.438503: train_loss 2.6023 
2025-05-06 01:51:40.469755: val_loss 3.0352 
2025-05-06 01:51:40.485381: PSNR 19.0178 
2025-05-06 01:51:40.501003: Epoch time: 1397.81 s 
2025-05-06 01:51:41.418262:  
2025-05-06 01:51:41.433893: Epoch 898 
2025-05-06 01:51:41.449514: Current learning rate: 5.7e-06 
2025-05-06 02:15:08.986684: train_loss 2.5075 
2025-05-06 02:15:09.002311: val_loss 2.8923 
2025-05-06 02:15:09.033558: PSNR 19.3207 
2025-05-06 02:15:09.049185: Epoch time: 1407.58 s 
2025-05-06 02:15:10.164986:  
2025-05-06 02:15:10.180611: Epoch 899 
2025-05-06 02:15:10.196238: Current learning rate: 5.6e-06 
2025-05-06 02:38:34.711535: train_loss 2.6964 
2025-05-06 02:38:34.727159: val_loss 2.505 
2025-05-06 02:38:34.742785: PSNR 20.3997 
2025-05-06 02:38:34.758410: Epoch time: 1404.55 s 
2025-05-06 02:38:41.357551:  
2025-05-06 02:38:41.388803: Epoch 900 
2025-05-06 02:38:41.388803: Current learning rate: 5.5e-06 
2025-05-06 03:01:54.575571: train_loss 2.6282 
2025-05-06 03:01:54.591198: val_loss 2.6861 
2025-05-06 03:01:54.606821: PSNR 19.7371 
2025-05-06 03:01:54.622446: Epoch time: 1393.22 s 
2025-05-06 03:01:55.678195:  
2025-05-06 03:01:55.705597: Epoch 901 
2025-05-06 03:01:55.722245: Current learning rate: 5.4e-06 
2025-05-06 03:25:13.154304: train_loss 2.5757 
2025-05-06 03:25:13.169927: val_loss 2.5782 
2025-05-06 03:25:13.185556: PSNR 20.0151 
2025-05-06 03:25:13.201180: Epoch time: 1397.48 s 
2025-05-06 03:25:14.107431:  
2025-05-06 03:25:14.123058: Epoch 902 
2025-05-06 03:25:14.138683: Current learning rate: 5.3e-06 
2025-05-06 03:48:42.291914: train_loss 2.5363 
2025-05-06 03:48:42.307538: val_loss 2.7415 
2025-05-06 03:48:42.307538: PSNR 19.8467 
2025-05-06 03:48:42.338787: Epoch time: 1408.2 s 
2025-05-06 03:48:43.289320:  
2025-05-06 03:48:43.315219: Epoch 903 
2025-05-06 03:48:43.330854: Current learning rate: 5.2e-06 
2025-05-06 04:11:53.021752: train_loss 2.5609 
2025-05-06 04:11:53.037378: val_loss 2.7831 
2025-05-06 04:11:53.053003: PSNR 19.7401 
2025-05-06 04:11:53.068629: Epoch time: 1389.73 s 
2025-05-06 04:11:53.921520:  
2025-05-06 04:11:53.937141: Epoch 904 
2025-05-06 04:11:53.952769: Current learning rate: 5.1e-06 
2025-05-06 04:35:21.113695: train_loss 2.5837 
2025-05-06 04:35:21.144941: val_loss 2.8332 
2025-05-06 04:35:21.160568: PSNR 19.8043 
2025-05-06 04:35:21.171103: Epoch time: 1407.19 s 
2025-05-06 04:35:22.305136:  
2025-05-06 04:35:22.336385: Epoch 905 
2025-05-06 04:35:22.367638: Current learning rate: 5e-06 
2025-05-06 04:58:41.617050: train_loss 2.6399 
2025-05-06 04:58:41.648299: val_loss 2.7516 
2025-05-06 04:58:41.679549: PSNR 19.4522 
2025-05-06 04:58:41.695175: Epoch time: 1399.31 s 
2025-05-06 04:58:42.751792:  
2025-05-06 04:58:42.767424: Epoch 906 
2025-05-06 04:58:42.783051: Current learning rate: 4.9e-06 
2025-05-06 05:22:11.850612: train_loss 2.72 
2025-05-06 05:22:11.866239: val_loss 2.5181 
2025-05-06 05:22:11.881866: PSNR 19.6117 
2025-05-06 05:22:11.897489: Epoch time: 1409.12 s 
2025-05-06 05:22:13.137291:  
2025-05-06 05:22:13.152915: Epoch 907 
2025-05-06 05:22:13.168543: Current learning rate: 4.8e-06 
2025-05-06 05:45:32.083225: train_loss 2.4255 
2025-05-06 05:45:32.098847: val_loss 3.0151 
2025-05-06 05:45:32.123507: PSNR 19.2772 
2025-05-06 05:45:32.123507: Epoch time: 1398.95 s 
2025-05-06 05:45:32.998517:  
2025-05-06 05:45:33.029768: Epoch 908 
2025-05-06 05:45:33.045393: Current learning rate: 4.8e-06 
2025-05-06 06:09:01.915756: train_loss 2.4601 
2025-05-06 06:09:01.931382: val_loss 2.8113 
2025-05-06 06:09:01.947008: PSNR 19.726 
2025-05-06 06:09:01.962634: Epoch time: 1408.92 s 
2025-05-06 06:09:03.124387:  
2025-05-06 06:09:03.140008: Epoch 909 
2025-05-06 06:09:03.155636: Current learning rate: 4.7e-06 
2025-05-06 06:32:27.927490: train_loss 2.6747 
2025-05-06 06:32:27.958741: val_loss 2.6758 
2025-05-06 06:32:27.974367: PSNR 19.9079 
2025-05-06 06:32:28.005614: Epoch time: 1404.8 s 
2025-05-06 06:32:28.965092:  
2025-05-06 06:32:28.981734: Epoch 910 
2025-05-06 06:32:28.997362: Current learning rate: 4.6e-06 
2025-05-06 06:55:49.514674: train_loss 2.5933 
2025-05-06 06:55:49.530300: val_loss 2.8818 
2025-05-06 06:55:49.561548: PSNR 19.1765 
2025-05-06 06:55:49.577175: Epoch time: 1400.55 s 
2025-05-06 06:55:50.941370:  
2025-05-06 06:55:50.972621: Epoch 911 
2025-05-06 06:55:50.988247: Current learning rate: 4.5e-06 
2025-05-06 07:19:05.635870: train_loss 2.5884 
2025-05-06 07:19:05.651490: val_loss 2.6562 
2025-05-06 07:19:05.667116: PSNR 20.5859 
2025-05-06 07:19:05.682744: Epoch time: 1394.69 s 
2025-05-06 07:19:06.682744:  
2025-05-06 07:19:06.698368: Epoch 912 
2025-05-06 07:19:06.713994: Current learning rate: 4.4e-06 
2025-05-06 07:42:32.857705: train_loss 2.6709 
2025-05-06 07:42:32.873329: val_loss 2.6756 
2025-05-06 07:42:32.873329: PSNR 19.4219 
2025-05-06 07:42:32.888956: Epoch time: 1406.19 s 
2025-05-06 07:42:33.915222:  
2025-05-06 07:42:33.930845: Epoch 913 
2025-05-06 07:42:33.946471: Current learning rate: 4.3e-06 
2025-05-06 08:05:45.834057: train_loss 2.589 
2025-05-06 08:05:45.849681: val_loss 2.6365 
2025-05-06 08:05:45.865307: PSNR 20.2459 
2025-05-06 08:05:45.880935: Epoch time: 1391.92 s 
2025-05-06 08:05:46.932344:  
2025-05-06 08:05:46.963594: Epoch 914 
2025-05-06 08:05:46.963594: Current learning rate: 4.2e-06 
2025-05-06 08:29:12.080297: train_loss 2.6308 
2025-05-06 08:29:12.095922: val_loss 2.4786 
2025-05-06 08:29:12.111549: PSNR 20.6864 
2025-05-06 08:29:12.127174: Epoch time: 1405.15 s 
2025-05-06 08:29:13.004576:  
2025-05-06 08:29:13.020205: Epoch 915 
2025-05-06 08:29:13.035826: Current learning rate: 4.1e-06 
2025-05-06 08:52:39.942378: train_loss 2.5919 
2025-05-06 08:52:39.973628: val_loss 2.9578 
2025-05-06 08:52:39.999273: PSNR 18.8187 
2025-05-06 08:52:40.014900: Epoch time: 1406.94 s 
2025-05-06 08:52:41.149553:  
2025-05-06 08:52:41.180804: Epoch 916 
2025-05-06 08:52:41.196429: Current learning rate: 4e-06 
2025-05-06 09:15:52.019764: train_loss 2.5647 
2025-05-06 09:15:52.035389: val_loss 2.7751 
2025-05-06 09:15:52.066641: PSNR 19.6797 
2025-05-06 09:15:52.082267: Epoch time: 1390.87 s 
2025-05-06 09:15:52.975474:  
2025-05-06 09:15:53.006718: Epoch 917 
2025-05-06 09:15:53.022343: Current learning rate: 3.9e-06 
2025-05-06 09:39:19.674564: train_loss 2.6462 
2025-05-06 09:39:19.721439: val_loss 2.7407 
2025-05-06 09:39:19.752690: PSNR 19.5858 
2025-05-06 09:39:19.768314: Epoch time: 1406.7 s 
2025-05-06 09:39:20.751201:  
2025-05-06 09:39:20.782449: Epoch 918 
2025-05-06 09:39:20.798077: Current learning rate: 3.8e-06 
2025-05-06 10:02:39.333997: train_loss 2.675 
2025-05-06 10:02:39.349621: val_loss 2.6741 
2025-05-06 10:02:39.365246: PSNR 19.68 
2025-05-06 10:02:39.380872: Epoch time: 1398.58 s 
2025-05-06 10:02:40.305234:  
2025-05-06 10:02:40.320858: Epoch 919 
2025-05-06 10:02:40.320858: Current learning rate: 3.8e-06 
2025-05-06 10:26:08.904844: train_loss 2.565 
2025-05-06 10:26:08.944978: val_loss 2.898 
2025-05-06 10:26:08.944978: PSNR 19.2407 
2025-05-06 10:26:08.961291: Epoch time: 1408.6 s 
2025-05-06 10:26:09.959745:  
2025-05-06 10:26:09.975372: Epoch 920 
2025-05-06 10:26:09.990995: Current learning rate: 3.7e-06 
2025-05-06 10:49:33.455455: train_loss 2.6019 
2025-05-06 10:49:33.486701: val_loss 2.9399 
2025-05-06 10:49:33.502330: PSNR 19.4341 
2025-05-06 10:49:33.517955: Epoch time: 1403.5 s 
2025-05-06 10:49:34.613226:  
2025-05-06 10:49:34.628859: Epoch 921 
2025-05-06 10:49:34.644487: Current learning rate: 3.6e-06 
2025-05-06 11:12:59.154988: train_loss 2.5724 
2025-05-06 11:12:59.170611: val_loss 2.8336 
2025-05-06 11:12:59.186238: PSNR 19.6755 
2025-05-06 11:12:59.201864: Epoch time: 1404.54 s 
2025-05-06 11:13:00.095042:  
2025-05-06 11:13:00.126286: Epoch 922 
2025-05-06 11:13:00.157536: Current learning rate: 3.5e-06 
2025-05-06 11:36:18.090731: train_loss 2.5783 
2025-05-06 11:36:18.121980: val_loss 2.7164 
2025-05-06 11:36:18.137606: PSNR 19.9798 
2025-05-06 11:36:18.153232: Epoch time: 1398.0 s 
2025-05-06 11:36:19.134842:  
2025-05-06 11:36:19.166083: Epoch 923 
2025-05-06 11:36:19.181709: Current learning rate: 3.4e-06 
2025-05-06 11:59:31.026070: train_loss 2.603 
2025-05-06 11:59:31.072945: val_loss 2.8394 
2025-05-06 11:59:31.088573: PSNR 19.8653 
2025-05-06 11:59:31.104197: Epoch time: 1391.89 s 
2025-05-06 11:59:31.988228:  
2025-05-06 11:59:32.003853: Epoch 924 
2025-05-06 11:59:32.035104: Current learning rate: 3.4e-06 
2025-05-06 12:23:00.014993: train_loss 2.6471 
2025-05-06 12:23:00.030618: val_loss 2.7572 
2025-05-06 12:23:00.061868: PSNR 19.5637 
2025-05-06 12:23:00.077495: Epoch time: 1408.03 s 
2025-05-06 12:23:01.063887:  
2025-05-06 12:23:01.079512: Epoch 925 
2025-05-06 12:23:01.079512: Current learning rate: 3.3e-06 
2025-05-06 12:46:17.664726: train_loss 2.6471 
2025-05-06 12:46:17.680350: val_loss 2.7527 
2025-05-06 12:46:17.695978: PSNR 20.1415 
2025-05-06 12:46:17.695978: Epoch time: 1396.62 s 
2025-05-06 12:46:18.548878:  
2025-05-06 12:46:18.584971: Epoch 926 
2025-05-06 12:46:18.604912: Current learning rate: 3.2e-06 
2025-05-06 13:09:36.999502: train_loss 2.5872 
2025-05-06 13:09:37.015130: val_loss 2.7024 
2025-05-06 13:09:37.030756: PSNR 19.752 
2025-05-06 13:09:37.046388: Epoch time: 1398.45 s 
2025-05-06 13:09:37.883479:  
2025-05-06 13:09:37.899103: Epoch 927 
2025-05-06 13:09:37.914730: Current learning rate: 3.1e-06 
2025-05-06 13:32:48.811369: train_loss 2.5349 
2025-05-06 13:32:48.826992: val_loss 2.6791 
2025-05-06 13:32:48.858244: PSNR 19.8914 
2025-05-06 13:32:48.873869: Epoch time: 1390.94 s 
2025-05-06 13:32:50.129924:  
2025-05-06 13:32:50.145550: Epoch 928 
2025-05-06 13:32:50.161178: Current learning rate: 3e-06 
2025-05-06 13:56:07.983062: train_loss 2.5952 
2025-05-06 13:56:07.998687: val_loss 2.7927 
2025-05-06 13:56:08.014318: PSNR 19.3494 
2025-05-06 13:56:08.029938: Epoch time: 1397.85 s 
2025-05-06 13:56:09.129557:  
2025-05-06 13:56:09.145183: Epoch 929 
2025-05-06 13:56:09.160810: Current learning rate: 3e-06 
2025-05-06 14:19:21.559983: train_loss 2.5746 
2025-05-06 14:19:21.575611: val_loss 2.7331 
2025-05-06 14:19:21.591236: PSNR 19.9623 
2025-05-06 14:19:21.622483: Epoch time: 1392.43 s 
2025-05-06 14:19:22.496827:  
2025-05-06 14:19:22.512455: Epoch 930 
2025-05-06 14:19:22.528078: Current learning rate: 2.9e-06 
2025-05-06 14:42:51.368647: train_loss 2.6343 
2025-05-06 14:42:51.409565: val_loss 2.7803 
2025-05-06 14:42:51.425192: PSNR 19.4706 
2025-05-06 14:42:51.440817: Epoch time: 1408.87 s 
2025-05-06 14:42:52.459328:  
2025-05-06 14:42:52.474956: Epoch 931 
2025-05-06 14:42:52.490580: Current learning rate: 2.8e-06 
2025-05-06 15:06:03.900731: train_loss 2.5837 
2025-05-06 15:06:03.904856: val_loss 2.5358 
2025-05-06 15:06:03.936115: PSNR 20.1564 
2025-05-06 15:06:03.951742: Epoch time: 1391.46 s 
2025-05-06 15:06:05.077355:  
2025-05-06 15:06:05.108607: Epoch 932 
2025-05-06 15:06:05.124231: Current learning rate: 2.7e-06 
2025-05-06 15:29:33.111655: train_loss 2.5927 
2025-05-06 15:29:33.142907: val_loss 2.5851 
2025-05-06 15:29:33.158531: PSNR 20.7427 
2025-05-06 15:29:33.189788: Epoch time: 1408.03 s 
2025-05-06 15:29:34.308884:  
2025-05-06 15:29:34.324513: Epoch 933 
2025-05-06 15:29:34.340134: Current learning rate: 2.7e-06 
2025-05-06 15:53:01.585037: train_loss 2.626 
2025-05-06 15:53:01.600668: val_loss 2.9135 
2025-05-06 15:53:01.616300: PSNR 19.0029 
2025-05-06 15:53:01.631915: Epoch time: 1407.28 s 
2025-05-06 15:53:02.996225:  
2025-05-06 15:53:03.027487: Epoch 934 
2025-05-06 15:53:03.043101: Current learning rate: 2.6e-06 
2025-05-06 16:16:15.805371: train_loss 2.5707 
2025-05-06 16:16:15.820999: val_loss 2.5283 
2025-05-06 16:16:15.836619: PSNR 20.581 
2025-05-06 16:16:15.852244: Epoch time: 1392.81 s 
2025-05-06 16:16:16.736623:  
2025-05-06 16:16:16.767875: Epoch 935 
2025-05-06 16:16:16.767875: Current learning rate: 2.5e-06 
2025-05-06 16:39:46.159721: train_loss 2.6793 
2025-05-06 16:39:46.190965: val_loss 2.9349 
2025-05-06 16:39:46.206591: PSNR 18.7087 
2025-05-06 16:39:46.222218: Epoch time: 1409.42 s 
2025-05-06 16:39:47.404639:  
2025-05-06 16:39:47.420264: Epoch 936 
2025-05-06 16:39:47.435894: Current learning rate: 2.5e-06 
2025-05-06 17:03:15.355471: train_loss 2.5243 
2025-05-06 17:03:15.386721: val_loss 2.7514 
2025-05-06 17:03:15.402359: PSNR 19.9962 
2025-05-06 17:03:15.417971: Epoch time: 1407.95 s 
2025-05-06 17:03:16.487294:  
2025-05-06 17:03:16.487294: Epoch 937 
2025-05-06 17:03:16.518552: Current learning rate: 2.4e-06 
2025-05-06 17:26:42.738451: train_loss 2.6071 
2025-05-06 17:26:42.754075: val_loss 2.5707 
2025-05-06 17:26:42.769703: PSNR 20.9261 
2025-05-06 17:26:42.785328: Epoch time: 1406.25 s 
2025-05-06 17:26:43.814079:  
2025-05-06 17:26:43.845330: Epoch 938 
2025-05-06 17:26:43.845330: Current learning rate: 2.3e-06 
2025-05-06 17:50:08.309368: train_loss 2.5371 
2025-05-06 17:50:08.340621: val_loss 2.7607 
2025-05-06 17:50:08.340621: PSNR 19.6273 
2025-05-06 17:50:08.356244: Epoch time: 1404.5 s 
2025-05-06 17:50:09.361258:  
2025-05-06 17:50:09.376884: Epoch 939 
2025-05-06 17:50:09.392510: Current learning rate: 2.3e-06 
2025-05-06 18:13:36.802135: train_loss 2.6363 
2025-05-06 18:13:36.817759: val_loss 2.9051 
2025-05-06 18:13:36.833385: PSNR 19.0228 
2025-05-06 18:13:36.849010: Epoch time: 1407.44 s 
2025-05-06 18:13:37.750064:  
2025-05-06 18:13:37.765687: Epoch 940 
2025-05-06 18:13:37.781315: Current learning rate: 2.2e-06 
2025-05-06 18:36:50.897860: train_loss 2.5634 
2025-05-06 18:36:50.913488: val_loss 2.6226 
2025-05-06 18:36:50.929113: PSNR 20.2217 
2025-05-06 18:36:50.944741: Epoch time: 1393.15 s 
2025-05-06 18:36:52.407061:  
2025-05-06 18:36:52.422685: Epoch 941 
2025-05-06 18:36:52.453936: Current learning rate: 2.1e-06 
2025-05-06 19:00:02.911795: train_loss 2.6519 
2025-05-06 19:00:02.943040: val_loss 2.9647 
2025-05-06 19:00:02.958671: PSNR 18.8463 
2025-05-06 19:00:02.958671: Epoch time: 1390.52 s 
2025-05-06 19:00:03.928468:  
2025-05-06 19:00:03.944094: Epoch 942 
2025-05-06 19:00:03.959728: Current learning rate: 2.1e-06 
2025-05-06 19:23:30.960609: train_loss 2.5424 
2025-05-06 19:23:30.976234: val_loss 2.9758 
2025-05-06 19:23:30.991859: PSNR 19.1614 
2025-05-06 19:23:31.007484: Epoch time: 1407.03 s 
2025-05-06 19:23:32.142371:  
2025-05-06 19:23:32.157997: Epoch 943 
2025-05-06 19:23:32.189252: Current learning rate: 2e-06 
2025-05-06 19:46:49.686105: train_loss 2.582 
2025-05-06 19:46:49.686105: val_loss 2.8974 
2025-05-06 19:46:49.717354: PSNR 20.0089 
2025-05-06 19:46:49.732983: Epoch time: 1397.54 s 
2025-05-06 19:46:50.628134:  
2025-05-06 19:46:50.643756: Epoch 944 
2025-05-06 19:46:50.659381: Current learning rate: 1.9e-06 
2025-05-06 20:10:20.130058: train_loss 2.6175 
2025-05-06 20:10:20.145686: val_loss 2.8282 
2025-05-06 20:10:20.161309: PSNR 19.3434 
2025-05-06 20:10:20.176934: Epoch time: 1409.5 s 
2025-05-06 20:10:21.358092:  
2025-05-06 20:10:21.373718: Epoch 945 
2025-05-06 20:10:21.389346: Current learning rate: 1.9e-06 
2025-05-06 20:33:33.486733: train_loss 2.5411 
2025-05-06 20:33:33.517984: val_loss 2.6684 
2025-05-06 20:33:33.543350: PSNR 20.187 
2025-05-06 20:33:33.558982: Epoch time: 1392.14 s 
2025-05-06 20:33:34.669839:  
2025-05-06 20:33:34.685464: Epoch 946 
2025-05-06 20:33:34.701097: Current learning rate: 1.8e-06 
2025-05-06 20:56:54.196444: train_loss 2.5652 
2025-05-06 20:56:54.227695: val_loss 2.6716 
2025-05-06 20:56:54.258944: PSNR 20.0838 
2025-05-06 20:56:54.258944: Epoch time: 1399.54 s 
2025-05-06 20:56:55.453287:  
2025-05-06 20:56:55.468912: Epoch 947 
2025-05-06 20:56:55.468912: Current learning rate: 1.8e-06 
2025-05-06 21:20:08.333123: train_loss 2.6153 
2025-05-06 21:20:08.364372: val_loss 2.78 
2025-05-06 21:20:08.380001: PSNR 19.7902 
2025-05-06 21:20:08.395625: Epoch time: 1392.88 s 
2025-05-06 21:20:09.270626:  
2025-05-06 21:20:09.270626: Epoch 948 
2025-05-06 21:20:09.286252: Current learning rate: 1.7e-06 
2025-05-06 21:43:35.455381: train_loss 2.5997 
2025-05-06 21:43:35.486634: val_loss 2.5432 
2025-05-06 21:43:35.502258: PSNR 20.6587 
2025-05-06 21:43:35.517885: Epoch time: 1406.2 s 
2025-05-06 21:43:36.344956:  
2025-05-06 21:43:36.344956: Epoch 949 
2025-05-06 21:43:36.376215: Current learning rate: 1.7e-06 
2025-05-06 22:06:53.923012: train_loss 2.6019 
2025-05-06 22:06:53.938640: val_loss 2.6056 
2025-05-06 22:06:53.938640: PSNR 20.2734 
2025-05-06 22:06:53.969889: Epoch time: 1397.58 s 
2025-05-06 22:07:01.088826:  
2025-05-06 22:07:01.104455: Epoch 950 
2025-05-06 22:07:01.120079: Current learning rate: 1.6e-06 
2025-05-06 22:30:23.669002: train_loss 2.6189 
2025-05-06 22:30:23.700252: val_loss 2.9401 
2025-05-06 22:30:23.715884: PSNR 18.7999 
2025-05-06 22:30:23.715884: Epoch time: 1402.58 s 
2025-05-06 22:30:24.782339:  
2025-05-06 22:30:24.797968: Epoch 951 
2025-05-06 22:30:24.813593: Current learning rate: 1.5e-06 
2025-05-06 22:53:47.809594: train_loss 2.5982 
2025-05-06 22:53:47.840840: val_loss 2.952 
2025-05-06 22:53:47.856468: PSNR 18.8463 
2025-05-06 22:53:47.872096: Epoch time: 1403.03 s 
2025-05-06 22:53:49.016460:  
2025-05-06 22:53:49.032085: Epoch 952 
2025-05-06 22:53:49.047709: Current learning rate: 1.5e-06 
2025-05-06 23:16:59.511408: train_loss 2.6457 
2025-05-06 23:16:59.527033: val_loss 2.4764 
2025-05-06 23:16:59.542659: PSNR 20.9507 
2025-05-06 23:16:59.558285: Epoch time: 1390.49 s 
2025-05-06 23:17:00.692928:  
2025-05-06 23:17:00.724178: Epoch 953 
2025-05-06 23:17:00.724178: Current learning rate: 1.4e-06 
2025-05-06 23:40:15.616811: train_loss 2.4788 
2025-05-06 23:40:15.648058: val_loss 2.8767 
2025-05-06 23:40:15.663685: PSNR 19.4642 
2025-05-06 23:40:15.679313: Epoch time: 1394.92 s 
2025-05-06 23:40:16.813137:  
2025-05-06 23:40:16.828760: Epoch 954 
2025-05-06 23:40:16.844385: Current learning rate: 1.4e-06 
2025-05-07 00:03:32.496539: train_loss 2.5402 
2025-05-07 00:03:32.512167: val_loss 2.903 
2025-05-07 00:03:32.543423: PSNR 19.4153 
2025-05-07 00:03:32.559042: Epoch time: 1395.68 s 
2025-05-07 00:03:33.724132:  
2025-05-07 00:03:33.755382: Epoch 955 
2025-05-07 00:03:33.771009: Current learning rate: 1.3e-06 
2025-05-07 00:26:55.645457: train_loss 2.6721 
2025-05-07 00:26:55.673932: val_loss 2.7077 
2025-05-07 00:26:55.689559: PSNR 19.5404 
2025-05-07 00:26:55.705184: Epoch time: 1401.92 s 
2025-05-07 00:26:56.979873:  
2025-05-07 00:26:57.011129: Epoch 956 
2025-05-07 00:26:57.026751: Current learning rate: 1.3e-06 
2025-05-07 00:50:11.112449: train_loss 2.6164 
2025-05-07 00:50:11.128071: val_loss 2.5713 
2025-05-07 00:50:11.143695: PSNR 19.9782 
2025-05-07 00:50:11.159320: Epoch time: 1394.13 s 
2025-05-07 00:50:12.031708:  
2025-05-07 00:50:12.062963: Epoch 957 
2025-05-07 00:50:12.062963: Current learning rate: 1.2e-06 
2025-05-07 01:13:39.525837: train_loss 2.556 
2025-05-07 01:13:39.557090: val_loss 2.7143 
2025-05-07 01:13:39.557090: PSNR 20.3814 
2025-05-07 01:13:39.572717: Epoch time: 1407.49 s 
2025-05-07 01:13:40.557761:  
2025-05-07 01:13:40.589008: Epoch 958 
2025-05-07 01:13:40.604635: Current learning rate: 1.2e-06 
2025-05-07 01:36:58.207712: train_loss 2.605 
2025-05-07 01:36:58.223339: val_loss 2.7288 
2025-05-07 01:36:58.237484: PSNR 20.0125 
2025-05-07 01:36:58.253116: Epoch time: 1397.65 s 
2025-05-07 01:36:59.485596:  
2025-05-07 01:36:59.501222: Epoch 959 
2025-05-07 01:36:59.516851: Current learning rate: 1.2e-06 
2025-05-07 02:00:08.487571: train_loss 2.6071 
2025-05-07 02:00:08.503192: val_loss 2.9535 
2025-05-07 02:00:08.518819: PSNR 18.8678 
2025-05-07 02:00:08.534453: Epoch time: 1389.0 s 
2025-05-07 02:00:09.528526:  
2025-05-07 02:00:09.544151: Epoch 960 
2025-05-07 02:00:09.559782: Current learning rate: 1.1e-06 
2025-05-07 02:23:32.516560: train_loss 2.5234 
2025-05-07 02:23:32.556415: val_loss 2.9248 
2025-05-07 02:23:32.572043: PSNR 19.1618 
2025-05-07 02:23:32.587670: Epoch time: 1402.99 s 
2025-05-07 02:23:33.721623:  
2025-05-07 02:23:33.760271: Epoch 961 
2025-05-07 02:23:33.760271: Current learning rate: 1.1e-06 
2025-05-07 02:46:57.697494: train_loss 2.6021 
2025-05-07 02:46:57.738784: val_loss 2.7011 
2025-05-07 02:46:57.754411: PSNR 20.0281 
2025-05-07 02:46:57.770035: Epoch time: 1403.98 s 
2025-05-07 02:46:58.629415:  
2025-05-07 02:46:58.645040: Epoch 962 
2025-05-07 02:46:58.660663: Current learning rate: 1e-06 
2025-05-07 03:10:15.521933: train_loss 2.5837 
2025-05-07 03:10:15.537561: val_loss 2.7087 
2025-05-07 03:10:15.553186: PSNR 20.1825 
2025-05-07 03:10:15.584436: Epoch time: 1396.89 s 
2025-05-07 03:10:16.845138:  
2025-05-07 03:10:16.860762: Epoch 963 
2025-05-07 03:10:16.892012: Current learning rate: 1e-06 
2025-05-07 03:33:41.486262: train_loss 2.6003 
2025-05-07 03:33:41.501890: val_loss 2.6603 
2025-05-07 03:33:41.517515: PSNR 19.9839 
2025-05-07 03:33:41.533139: Epoch time: 1404.64 s 
2025-05-07 03:33:42.719344:  
2025-05-07 03:33:42.734967: Epoch 964 
2025-05-07 03:33:42.750593: Current learning rate: 9e-07 
2025-05-07 03:56:59.703687: train_loss 2.5707 
2025-05-07 03:56:59.719312: val_loss 2.6916 
2025-05-07 03:56:59.734941: PSNR 20.4099 
2025-05-07 03:56:59.750567: Epoch time: 1396.98 s 
2025-05-07 03:57:01.127672:  
2025-05-07 03:57:01.143295: Epoch 965 
2025-05-07 03:57:01.158922: Current learning rate: 9e-07 
2025-05-07 04:20:27.467458: train_loss 2.6121 
2025-05-07 04:20:27.483083: val_loss 2.5566 
2025-05-07 04:20:27.498706: PSNR 20.8113 
2025-05-07 04:20:27.529959: Epoch time: 1406.34 s 
2025-05-07 04:20:28.837185:  
2025-05-07 04:20:28.852806: Epoch 966 
2025-05-07 04:20:28.884059: Current learning rate: 9e-07 
2025-05-07 04:43:55.218936: train_loss 2.5959 
2025-05-07 04:43:55.234564: val_loss 2.6776 
2025-05-07 04:43:55.250189: PSNR 19.8192 
2025-05-07 04:43:55.281435: Epoch time: 1406.38 s 
2025-05-07 04:43:56.472433:  
2025-05-07 04:43:56.488061: Epoch 967 
2025-05-07 04:43:56.503685: Current learning rate: 8e-07 
2025-05-07 05:07:05.691891: train_loss 2.5914 
2025-05-07 05:07:05.707516: val_loss 3.1209 
2025-05-07 05:07:05.723140: PSNR 18.5296 
2025-05-07 05:07:05.738772: Epoch time: 1389.24 s 
2025-05-07 05:07:06.613768:  
2025-05-07 05:07:06.613768: Epoch 968 
2025-05-07 05:07:06.645017: Current learning rate: 8e-07 
2025-05-07 05:30:32.074326: train_loss 2.5374 
2025-05-07 05:30:32.100129: val_loss 2.8305 
2025-05-07 05:30:32.115764: PSNR 19.3741 
2025-05-07 05:30:32.131386: Epoch time: 1405.48 s 
2025-05-07 05:30:33.078787:  
2025-05-07 05:30:33.094415: Epoch 969 
2025-05-07 05:30:33.110036: Current learning rate: 7e-07 
2025-05-07 05:53:56.270980: train_loss 2.4614 
2025-05-07 05:53:56.286605: val_loss 2.794 
2025-05-07 05:53:56.317851: PSNR 19.9823 
2025-05-07 05:53:56.333477: Epoch time: 1403.21 s 
2025-05-07 05:53:57.370751:  
2025-05-07 05:53:57.402002: Epoch 970 
2025-05-07 05:53:57.417624: Current learning rate: 7e-07 
2025-05-07 06:17:07.165268: train_loss 2.6298 
2025-05-07 06:17:07.187844: val_loss 3.0758 
2025-05-07 06:17:07.205528: PSNR 18.7692 
2025-05-07 06:17:07.221155: Epoch time: 1389.79 s 
2025-05-07 06:17:08.075627:  
2025-05-07 06:17:08.075627: Epoch 971 
2025-05-07 06:17:08.091250: Current learning rate: 7e-07 
2025-05-07 06:40:19.163883: train_loss 2.5518 
2025-05-07 06:40:19.179507: val_loss 2.8349 
2025-05-07 06:40:19.204031: PSNR 19.3162 
2025-05-07 06:40:19.219665: Epoch time: 1391.1 s 
2025-05-07 06:40:20.229807:  
2025-05-07 06:40:20.245432: Epoch 972 
2025-05-07 06:40:20.261060: Current learning rate: 6e-07 
2025-05-07 07:03:32.689979: train_loss 2.6711 
2025-05-07 07:03:32.705608: val_loss 2.7094 
2025-05-07 07:03:32.721237: PSNR 19.7526 
2025-05-07 07:03:32.736859: Epoch time: 1392.48 s 
2025-05-07 07:03:33.643693:  
2025-05-07 07:03:33.659318: Epoch 973 
2025-05-07 07:03:33.674942: Current learning rate: 6e-07 
2025-05-07 07:26:59.844279: train_loss 2.5511 
2025-05-07 07:26:59.859903: val_loss 2.4099 
2025-05-07 07:26:59.875529: PSNR 20.9401 
2025-05-07 07:26:59.906777: Epoch time: 1406.2 s 
2025-05-07 07:27:00.963876:  
2025-05-07 07:27:00.995141: Epoch 974 
2025-05-07 07:27:01.010760: Current learning rate: 6e-07 
2025-05-07 07:50:17.317361: train_loss 2.6482 
2025-05-07 07:50:17.332984: val_loss 2.6941 
2025-05-07 07:50:17.348613: PSNR 19.7007 
2025-05-07 07:50:17.364237: Epoch time: 1396.35 s 
2025-05-07 07:50:18.508812:  
2025-05-07 07:50:18.524439: Epoch 975 
2025-05-07 07:50:18.540066: Current learning rate: 5e-07 
2025-05-07 08:13:27.869910: train_loss 2.5504 
2025-05-07 08:13:27.885517: val_loss 2.8062 
2025-05-07 08:13:27.901141: PSNR 19.589 
2025-05-07 08:13:27.916771: Epoch time: 1389.36 s 
2025-05-07 08:13:29.066317:  
2025-05-07 08:13:29.081942: Epoch 976 
2025-05-07 08:13:29.097578: Current learning rate: 5e-07 
2025-05-07 08:36:38.461758: train_loss 2.6193 
2025-05-07 08:36:38.502027: val_loss 2.8208 
2025-05-07 08:36:38.517656: PSNR 20.2197 
2025-05-07 08:36:38.533281: Epoch time: 1389.41 s 
2025-05-07 08:36:39.657855:  
2025-05-07 08:36:39.664067: Epoch 977 
2025-05-07 08:36:39.679701: Current learning rate: 5e-07 
2025-05-07 08:59:54.363415: train_loss 2.5127 
2025-05-07 08:59:54.394661: val_loss 3.0297 
2025-05-07 08:59:54.410290: PSNR 18.8599 
2025-05-07 08:59:54.425915: Epoch time: 1394.71 s 
2025-05-07 08:59:55.741537:  
2025-05-07 08:59:55.757165: Epoch 978 
2025-05-07 08:59:55.788415: Current learning rate: 5e-07 
2025-05-07 09:23:11.847577: train_loss 2.5251 
2025-05-07 09:23:11.863199: val_loss 2.7575 
2025-05-07 09:23:11.863199: PSNR 20.5993 
2025-05-07 09:23:11.878827: Epoch time: 1396.11 s 
2025-05-07 09:23:12.793344:  
2025-05-07 09:23:12.808969: Epoch 979 
2025-05-07 09:23:12.808969: Current learning rate: 4e-07 
2025-05-07 09:46:30.323944: train_loss 2.6011 
2025-05-07 09:46:30.355196: val_loss 2.9041 
2025-05-07 09:46:30.386447: PSNR 19.7628 
2025-05-07 09:46:30.386447: Epoch time: 1397.53 s 
2025-05-07 09:46:31.261447:  
2025-05-07 09:46:31.292694: Epoch 980 
2025-05-07 09:46:31.308323: Current learning rate: 4e-07 
2025-05-07 10:09:59.904576: train_loss 2.5524 
2025-05-07 10:09:59.920210: val_loss 2.8353 
2025-05-07 10:09:59.935837: PSNR 19.5935 
2025-05-07 10:09:59.935837: Epoch time: 1408.64 s 
2025-05-07 10:10:01.201459:  
2025-05-07 10:10:01.201459: Epoch 981 
2025-05-07 10:10:01.232713: Current learning rate: 4e-07 
2025-05-07 10:33:18.737803: train_loss 2.5519 
2025-05-07 10:33:18.753429: val_loss 2.89 
2025-05-07 10:33:18.769055: PSNR 19.4172 
2025-05-07 10:33:18.769055: Epoch time: 1397.55 s 
2025-05-07 10:33:19.726992:  
2025-05-07 10:33:19.742619: Epoch 982 
2025-05-07 10:33:19.773870: Current learning rate: 4e-07 
2025-05-07 10:56:31.729043: train_loss 2.5432 
2025-05-07 10:56:31.744666: val_loss 2.7046 
2025-05-07 10:56:31.760292: PSNR 20.1781 
2025-05-07 10:56:31.791543: Epoch time: 1392.0 s 
2025-05-07 10:56:33.004360:  
2025-05-07 10:56:33.035614: Epoch 983 
2025-05-07 10:56:33.051233: Current learning rate: 3e-07 
2025-05-07 11:19:54.589186: train_loss 2.6173 
2025-05-07 11:19:54.620438: val_loss 2.6799 
2025-05-07 11:19:54.636062: PSNR 19.7987 
2025-05-07 11:19:54.651689: Epoch time: 1401.58 s 
2025-05-07 11:19:55.536350:  
2025-05-07 11:19:55.567599: Epoch 984 
2025-05-07 11:19:55.567599: Current learning rate: 3e-07 
2025-05-07 11:43:21.599318: train_loss 2.578 
2025-05-07 11:43:21.630568: val_loss 2.6558 
2025-05-07 11:43:21.646195: PSNR 20.2262 
2025-05-07 11:43:21.661820: Epoch time: 1406.06 s 
2025-05-07 11:43:22.849161:  
2025-05-07 11:43:22.864785: Epoch 985 
2025-05-07 11:43:22.880413: Current learning rate: 3e-07 
2025-05-07 12:06:47.729621: train_loss 2.5545 
2025-05-07 12:06:47.760869: val_loss 2.7619 
2025-05-07 12:06:47.776495: PSNR 19.5111 
2025-05-07 12:06:47.776495: Epoch time: 1404.88 s 
2025-05-07 12:06:49.019987:  
2025-05-07 12:06:49.035612: Epoch 986 
2025-05-07 12:06:49.051238: Current learning rate: 3e-07 
2025-05-07 12:30:12.279912: train_loss 2.4858 
2025-05-07 12:30:12.326787: val_loss 2.7584 
2025-05-07 12:30:12.342412: PSNR 19.7249 
2025-05-07 12:30:12.373663: Epoch time: 1403.28 s 
2025-05-07 12:30:13.367478:  
2025-05-07 12:30:13.383105: Epoch 987 
2025-05-07 12:30:13.398729: Current learning rate: 3e-07 
2025-05-07 12:53:23.243325: train_loss 2.6704 
2025-05-07 12:53:23.258949: val_loss 2.7171 
2025-05-07 12:53:23.274576: PSNR 19.5614 
2025-05-07 12:53:23.290200: Epoch time: 1389.88 s 
2025-05-07 12:53:24.176595:  
2025-05-07 12:53:24.192221: Epoch 988 
2025-05-07 12:53:24.207842: Current learning rate: 2e-07 
2025-05-07 13:16:46.149852: train_loss 2.5744 
2025-05-07 13:16:46.181114: val_loss 2.8259 
2025-05-07 13:16:46.196728: PSNR 19.5824 
2025-05-07 13:16:46.196728: Epoch time: 1401.97 s 
2025-05-07 13:16:47.286423:  
2025-05-07 13:16:47.302057: Epoch 989 
2025-05-07 13:16:47.317684: Current learning rate: 2e-07 
2025-05-07 13:40:04.663777: train_loss 2.6249 
2025-05-07 13:40:04.679401: val_loss 2.6812 
2025-05-07 13:40:04.695026: PSNR 19.8904 
2025-05-07 13:40:04.710655: Epoch time: 1397.38 s 
2025-05-07 13:40:05.767186:  
2025-05-07 13:40:05.782808: Epoch 990 
2025-05-07 13:40:05.798437: Current learning rate: 2e-07 
2025-05-07 14:03:28.233828: train_loss 2.5497 
2025-05-07 14:03:28.249448: val_loss 2.9539 
2025-05-07 14:03:28.265076: PSNR 19.1401 
2025-05-07 14:03:28.280700: Epoch time: 1402.48 s 
2025-05-07 14:03:29.117652:  
2025-05-07 14:03:29.133279: Epoch 991 
2025-05-07 14:03:29.148905: Current learning rate: 2e-07 
2025-05-07 14:26:52.399492: train_loss 2.5293 
2025-05-07 14:26:52.430741: val_loss 2.7074 
2025-05-07 14:26:52.446369: PSNR 19.7734 
2025-05-07 14:26:52.461991: Epoch time: 1403.28 s 
2025-05-07 14:26:53.674856:  
2025-05-07 14:26:53.706110: Epoch 992 
2025-05-07 14:26:53.706110: Current learning rate: 2e-07 
2025-05-07 14:50:03.339702: train_loss 2.5776 
2025-05-07 14:50:03.355324: val_loss 3.1461 
2025-05-07 14:50:03.370953: PSNR 18.7742 
2025-05-07 14:50:03.370953: Epoch time: 1389.66 s 
2025-05-07 14:50:04.301754:  
2025-05-07 14:50:04.333004: Epoch 993 
2025-05-07 14:50:04.348627: Current learning rate: 2e-07 
2025-05-07 15:13:13.456806: train_loss 2.5464 
2025-05-07 15:13:13.488053: val_loss 2.7051 
2025-05-07 15:13:13.503682: PSNR 19.5748 
2025-05-07 15:13:13.519312: Epoch time: 1389.16 s 
2025-05-07 15:13:14.893951:  
2025-05-07 15:13:14.925201: Epoch 994 
2025-05-07 15:13:14.940835: Current learning rate: 2e-07 
2025-05-07 15:36:38.447171: train_loss 2.5585 
2025-05-07 15:36:38.462796: val_loss 2.8869 
2025-05-07 15:36:38.478425: PSNR 19.2197 
2025-05-07 15:36:38.494061: Epoch time: 1403.57 s 
2025-05-07 15:36:39.341691:  
2025-05-07 15:36:39.357315: Epoch 995 
2025-05-07 15:36:39.357315: Current learning rate: 1e-07 
2025-05-07 16:00:04.949632: train_loss 2.5801 
2025-05-07 16:00:04.965910: val_loss 2.9111 
2025-05-07 16:00:04.981546: PSNR 19.0333 
2025-05-07 16:00:04.997171: Epoch time: 1405.62 s 
2025-05-07 16:00:06.100624:  
2025-05-07 16:00:06.116249: Epoch 996 
2025-05-07 16:00:06.147501: Current learning rate: 1e-07 
2025-05-07 16:23:15.302438: train_loss 2.5719 
2025-05-07 16:23:15.327529: val_loss 2.9048 
2025-05-07 16:23:15.343172: PSNR 19.2743 
2025-05-07 16:23:15.358793: Epoch time: 1389.2 s 
2025-05-07 16:23:16.243249:  
2025-05-07 16:23:16.258872: Epoch 997 
2025-05-07 16:23:16.274498: Current learning rate: 1e-07 
2025-05-07 16:46:34.170899: train_loss 2.5116 
2025-05-07 16:46:34.202152: val_loss 2.8074 
2025-05-07 16:46:34.217779: PSNR 19.5156 
2025-05-07 16:46:34.217779: Epoch time: 1397.93 s 
2025-05-07 16:46:35.451746:  
2025-05-07 16:46:35.451746: Epoch 998 
2025-05-07 16:46:35.467373: Current learning rate: 1e-07 
2025-05-07 17:09:59.997817: train_loss 2.6318 
2025-05-07 17:10:00.029070: val_loss 2.8579 
2025-05-07 17:10:00.044693: PSNR 19.3504 
2025-05-07 17:10:00.060321: Epoch time: 1404.56 s 
2025-05-07 17:10:01.256785:  
2025-05-07 17:10:01.272410: Epoch 999 
2025-05-07 17:10:01.272410: Current learning rate: 1e-07 
2025-05-07 17:33:10.964242: train_loss 2.5807 
2025-05-07 17:33:10.979868: val_loss 2.7128 
2025-05-07 17:33:10.995495: PSNR 19.3276 
2025-05-07 17:33:10.995495: Epoch time: 1389.71 s 
2025-05-07 17:33:16.581553:  
2025-05-07 17:33:16.597184: Epoch 1000 
2025-05-07 17:33:16.612806: Current learning rate: 1e-07 
2025-05-07 17:56:40.116083: train_loss 2.642 
2025-05-07 17:56:40.131709: val_loss 2.5234 
2025-05-07 17:56:40.162961: PSNR 20.5142 
2025-05-07 17:56:40.178584: Epoch time: 1403.53 s 
2025-05-07 17:56:41.217925:  
2025-05-07 17:56:41.233552: Epoch 1001 
2025-05-07 17:56:41.249178: Current learning rate: 1e-07 
2025-05-07 18:21:54.033797: train_loss 4.6999 
2025-05-07 18:21:54.049424: val_loss 2.812 
2025-05-07 18:21:54.065047: PSNR 19.2743 
2025-05-07 18:21:54.080673: Epoch time: 1512.82 s 
2025-05-07 18:21:54.987808:  
2025-05-07 18:21:55.003431: Epoch 1002 
2025-05-07 18:21:55.019059: Current learning rate: 1e-07 
2025-05-07 18:47:28.912619: train_loss 5.0064 
2025-05-07 18:47:28.943866: val_loss 2.8117 
2025-05-07 18:47:28.975119: PSNR 19.6129 
2025-05-07 18:47:28.990743: Epoch time: 1533.94 s 
2025-05-07 18:47:30.046515:  
2025-05-07 18:47:30.062143: Epoch 1003 
2025-05-07 18:47:30.077767: Current learning rate: 1e-07 
2025-05-07 19:12:31.933116: train_loss 4.9098 
2025-05-07 19:12:31.948741: val_loss 2.7123 
2025-05-07 19:12:31.948741: PSNR 19.8376 
2025-05-07 19:12:31.964364: Epoch time: 1501.9 s 
2025-05-07 19:12:32.942341:  
2025-05-07 19:12:32.957967: Epoch 1004 
2025-05-07 19:12:32.973593: Current learning rate: 1e-07 
2025-05-07 19:37:36.448425: train_loss 5.1599 
2025-05-07 19:37:36.464055: val_loss 2.6816 
2025-05-07 19:37:36.495300: PSNR 19.7524 
2025-05-07 19:37:36.495300: Epoch time: 1503.51 s 
2025-05-07 19:37:37.364969:  
2025-05-07 19:37:37.380594: Epoch 1005 
2025-05-07 19:37:37.396220: Current learning rate: 0.0002 
2025-05-07 20:02:35.454121: train_loss 4.8668 
2025-05-07 20:02:35.485370: val_loss 2.5737 
2025-05-07 20:02:35.500997: PSNR 19.5804 
2025-05-07 20:02:35.532246: Epoch time: 1498.09 s 
2025-05-07 20:02:36.794829:  
2025-05-07 20:02:36.810458: Epoch 1006 
2025-05-07 20:02:36.826084: Current learning rate: 0.0002 
2025-05-07 20:27:56.418654: train_loss 5.5971 
2025-05-07 20:27:56.450057: val_loss 2.4807 
2025-05-07 20:27:56.465686: PSNR 19.9779 
2025-05-07 20:27:56.481311: Epoch time: 1519.62 s 
2025-05-07 20:27:57.740337:  
2025-05-07 20:27:57.755963: Epoch 1007 
2025-05-07 20:27:57.771589: Current learning rate: 0.0002 
2025-05-07 20:53:36.414392: train_loss 5.5385 
2025-05-07 20:53:36.430017: val_loss 2.4791 
2025-05-07 20:53:36.445643: PSNR 19.9599 
2025-05-07 20:53:36.445643: Epoch time: 1538.67 s 
2025-05-07 20:53:37.385220:  
2025-05-07 20:53:37.400852: Epoch 1008 
2025-05-07 20:53:37.416474: Current learning rate: 0.0002 
2025-05-07 21:19:21.522465: train_loss 4.7631 
2025-05-07 21:19:21.553714: val_loss 2.4237 
2025-05-07 21:19:21.553714: PSNR 19.7849 
2025-05-07 21:19:21.584967: Epoch time: 1544.14 s 
2025-05-07 21:19:22.879185:  
2025-05-07 21:19:22.894811: Epoch 1009 
2025-05-07 21:19:22.910438: Current learning rate: 0.0002 
2025-05-07 21:45:26.719438: train_loss 4.8759 
2025-05-07 21:45:26.735065: val_loss 2.521 
2025-05-07 21:45:26.735065: PSNR 19.7667 
2025-05-07 21:45:26.750693: Epoch time: 1563.86 s 
2025-05-07 21:45:27.702466:  
2025-05-07 21:45:27.718089: Epoch 1010 
2025-05-07 21:45:27.733717: Current learning rate: 0.0002 
2025-05-07 22:10:48.997640: train_loss 4.4986 
2025-05-07 22:10:49.013265: val_loss 2.8213 
2025-05-07 22:10:49.044518: PSNR 18.8901 
2025-05-07 22:10:49.044518: Epoch time: 1521.3 s 
2025-05-07 22:10:50.078288:  
2025-05-07 22:10:50.109545: Epoch 1011 
2025-05-07 22:10:50.125162: Current learning rate: 0.0002 
2025-05-07 22:36:15.377482: train_loss 4.8762 
2025-05-07 22:36:15.408732: val_loss 2.4258 
2025-05-07 22:36:15.424358: PSNR 19.8787 
2025-05-07 22:36:15.424358: Epoch time: 1525.3 s 
2025-05-07 22:36:16.527062:  
2025-05-07 22:36:16.558308: Epoch 1012 
2025-05-07 22:36:16.573938: Current learning rate: 0.0002 
2025-05-07 23:02:07.505152: train_loss 4.4687 
2025-05-07 23:02:07.523642: val_loss 2.6959 
2025-05-07 23:02:07.554899: PSNR 19.3254 
2025-05-07 23:02:07.570526: Epoch time: 1550.98 s 
2025-05-07 23:02:08.582548:  
2025-05-07 23:02:08.598175: Epoch 1013 
2025-05-07 23:02:08.613797: Current learning rate: 0.0002 
