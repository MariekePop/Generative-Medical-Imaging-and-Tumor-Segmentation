
#######################################################################
Please cite the following paper when using nnSeq2Seq:
[1] Han L, Tan T, Zhang T, et al. Synthesis-based imaging-differentiation representation learning for multi-sequence 3D/4D MRI[J]. Medical Image Analysis, 2024, 92: 103044.
[2] Han L, Zhang T, Huang Y, et al. An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis[C]. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023: 45-55.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnSeq2SeqPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [256, 320], 'median_image_size_in_voxels': [515.0, 560.0], 'spacing': [0.4296875, 0.4296875], 'normalization_schemes': ['Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization', 'Rescale0_995to01Normalization'], 'use_mask_for_norm': [False, False, False, False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'nnseq2seq.networks.seq2seq.seq2seq.Seq2Seq2d', 'arch_kwargs': {'image_encoder': {'in_channels': 1, 'conv_channels': [32, 64, 128, 256, 256], 'conv_kernel': [3, 2, 2, 2, 2], 'conv_stride': [1, 2, 2, 2, 2], 'resblock_n': [2, 2, 4, 4, 4], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'vq_n_embed': 8192, 'vq_beta': 0.25}, 'image_decoder': {'out_channels': 1, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'hyper_conv_dim': 16, 'latent_space_dim': 3, 'style_dim': 6, 'deep_supervision': True, 'focal_mode': 'focal_mix'}, 'segmentor': {'in_channels': 1, 'num_classes': 2, 'conv_channels': [256, 256, 128, 64, 32], 'conv_kernel': [3, 3, 3, 3, 3], 'conv_stride': [2, 2, 2, 2, 1], 'resblock_n': [4, 4, 4, 2, 2], 'resblock_kernel': [3, 3, 3, 3, 3], 'resblock_padding': [1, 1, 1, 1, 1], 'layer_scale_init_value': 1e-06, 'latent_space_dim': 3, 'deep_supervision': True}, 'discriminator': {'in_channels': 1, 'ndf': 32, 'hyper_conv_dim': 16, 'style_dim': 6, 'layer_scale_init_value': 1e-06, 'n_layers': 3, 'kw': 4, 'padw': 1}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset520_NeckTumour', 'plans_name': 'nnSeq2SeqPlans', 'original_median_spacing_after_transp': [4.400000095367432, 0.4296875, 0.4296875], 'original_median_shape_after_transp': [29, 515, 560], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 267.0, 'mean': 105.27149200439453, 'median': 105.0, 'min': 27.0, 'percentile_00_5': 61.0, 'percentile_99_5': 158.0, 'std': 18.445335388183594}, '1': {'max': 622.0, 'mean': 308.62164306640625, 'median': 313.0, 'min': 14.0, 'percentile_00_5': 129.0, 'percentile_99_5': 481.0, 'std': 61.370479583740234}, '2': {'max': 292.2086181640625, 'mean': 94.79603576660156, 'median': 90.01033782958984, 'min': 3.5035572052001953, 'percentile_00_5': 24.245834350585938, 'percentile_99_5': 184.52996826171875, 'std': 30.297283172607422}, '3': {'max': 607.82421875, 'mean': 256.9958801269531, 'median': 256.0714111328125, 'min': 21.118223190307617, 'percentile_00_5': 48.9947395324707, 'percentile_99_5': 449.9946594238281, 'std': 68.48628234863281}, '4': {'max': 782.535400390625, 'mean': 81.82330322265625, 'median': 33.67308044433594, 'min': 8.940948486328125, 'percentile_00_5': 10.96875, 'percentile_99_5': 506.26910400390625, 'std': 113.0}, '5': {'max': 4095.0, 'mean': 1518.48828125, 'median': 1545.581787109375, 'min': 0.0, 'percentile_00_5': 4.1741180419921875, 'percentile_99_5': 3541.29931640625, 'std': 716.7125854492188}}} 
 
2025-04-14 17:02:09.909376: unpacking dataset... 
2025-04-14 17:02:21.549833: unpacking done... 
2025-04-14 17:02:21.549833: do_dummy_2d_data_aug: False 
2025-04-14 17:02:21.565457: Using splits from existing split file: C:\Users\P095789\Downloads\seq2seqOG\nnSeq2Seq_preprocessed\Dataset520_NeckTumour\splits_final.json 
2025-04-14 17:02:21.581080: The split file contains 5 splits. 
2025-04-14 17:02:21.596708: Desired fold for training: 0 
2025-04-14 17:02:21.612330: This split has 4 training and 1 validation cases. 
2025-04-14 17:02:22.331088: Unable to plot network architecture: 
2025-04-14 17:02:22.346710: No module named 'hiddenlayer' 
2025-04-14 17:02:22.409214:  
2025-04-14 17:02:22.409214: Epoch 100 
2025-04-14 17:02:22.424838: Current learning rate: 0.000196 
2025-04-14 17:09:54.351205: train_loss 3.6031 
2025-04-14 17:09:54.351205: val_loss 2.3127 
2025-04-14 17:09:54.366857: PSNR 18.4234 
2025-04-14 17:09:54.394025: Epoch time: 451.93 s 
2025-04-14 17:09:56.032650:  
2025-04-14 17:09:56.048297: Epoch 101 
2025-04-14 17:09:56.063904: Current learning rate: 0.0001959 
2025-04-14 17:24:15.437729: train_loss 4.4931 
2025-04-14 17:24:15.468988: val_loss 2.5298 
2025-04-14 17:24:15.468988: PSNR 18.8091 
2025-04-14 17:24:15.500232: Epoch time: 859.42 s 
2025-04-14 17:24:16.274969:  
2025-04-14 17:24:16.306220: Epoch 102 
2025-04-14 17:24:16.321846: Current learning rate: 0.0001958 
2025-04-14 17:38:42.376599: train_loss 4.7768 
2025-04-14 17:38:42.392231: val_loss 2.7103 
2025-04-14 17:38:42.407858: PSNR 18.4117 
2025-04-14 17:38:42.423477: Epoch time: 866.1 s 
2025-04-14 17:38:43.710744:  
2025-04-14 17:38:43.726373: Epoch 103 
2025-04-14 17:38:43.741996: Current learning rate: 0.0001957 
2025-04-14 17:52:54.512603: train_loss 4.962 
2025-04-14 17:52:54.543854: val_loss 2.4907 
2025-04-14 17:52:54.559478: PSNR 19.1279 
2025-04-14 17:52:54.575109: Epoch time: 850.82 s 
2025-04-14 17:52:55.809360:  
2025-04-14 17:52:55.824981: Epoch 104 
2025-04-14 17:52:55.840612: Current learning rate: 0.0001956 
2025-04-14 18:07:33.492189: train_loss 4.903 
2025-04-14 18:07:33.523439: val_loss 2.4889 
2025-04-14 18:07:33.554693: PSNR 19.3681 
2025-04-14 18:07:33.570328: Epoch time: 877.7 s 
2025-04-14 18:07:34.820294:  
2025-04-14 18:07:34.851543: Epoch 105 
2025-04-14 18:07:34.867168: Current learning rate: 0.0001955 
2025-04-14 18:30:49.479731: train_loss 4.9514 
2025-04-14 18:30:49.495358: val_loss 2.3907 
2025-04-14 18:30:49.510988: PSNR 19.9123 
2025-04-14 18:30:49.526618: Epoch time: 1394.66 s 
2025-04-14 18:30:50.421399:  
2025-04-14 18:30:50.437025: Epoch 106 
2025-04-14 18:30:50.452657: Current learning rate: 0.0001954 
2025-04-14 18:54:08.028796: train_loss 4.9433 
2025-04-14 18:54:08.060051: val_loss 2.5118 
2025-04-14 18:54:08.075686: PSNR 19.6981 
2025-04-14 18:54:08.075686: Epoch time: 1397.61 s 
2025-04-14 18:54:08.993850:  
2025-04-14 18:54:09.025101: Epoch 107 
2025-04-14 18:54:09.025101: Current learning rate: 0.0001953 
2025-04-14 19:17:17.750386: train_loss 4.971 
2025-04-14 19:17:17.781636: val_loss 2.4333 
2025-04-14 19:17:17.797263: PSNR 19.6044 
2025-04-14 19:17:17.828515: Epoch time: 1388.76 s 
2025-04-14 19:17:17.844147: Yayy! New best EMA pseudo PSNR: 19.1661 
2025-04-14 19:17:23.910676:  
2025-04-14 19:17:23.941928: Epoch 108 
2025-04-14 19:17:23.957566: Current learning rate: 0.0001953 
2025-04-14 19:40:30.526939: train_loss 4.8235 
2025-04-14 19:40:30.542565: val_loss 2.4975 
2025-04-14 19:40:30.558191: PSNR 19.4453 
2025-04-14 19:40:30.573821: Epoch time: 1386.62 s 
2025-04-14 19:40:30.605064: Yayy! New best EMA pseudo PSNR: 19.194 
2025-04-14 19:40:37.343803:  
2025-04-14 19:40:37.375069: Epoch 109 
2025-04-14 19:40:37.375069: Current learning rate: 0.0001952 
2025-04-14 20:03:52.828928: train_loss 5.0156 
2025-04-14 20:03:52.844556: val_loss 2.6097 
2025-04-14 20:03:52.875806: PSNR 18.6411 
2025-04-14 20:03:52.891436: Epoch time: 1395.49 s 
2025-04-14 20:03:53.723786:  
2025-04-14 20:03:53.739420: Epoch 110 
2025-04-14 20:03:53.755051: Current learning rate: 0.0001951 
2025-04-14 20:26:59.616673: train_loss 4.8536 
2025-04-14 20:26:59.647926: val_loss 2.702 
2025-04-14 20:26:59.663552: PSNR 19.196 
2025-04-14 20:26:59.679178: Epoch time: 1385.89 s 
2025-04-14 20:27:00.834612:  
2025-04-14 20:27:00.850228: Epoch 111 
2025-04-14 20:27:00.865856: Current learning rate: 0.000195 
2025-04-14 20:50:17.122321: train_loss 4.8827 
2025-04-14 20:50:17.137950: val_loss 2.6853 
2025-04-14 20:50:17.153574: PSNR 18.3846 
2025-04-14 20:50:17.179501: Epoch time: 1396.29 s 
2025-04-14 20:50:18.226561:  
2025-04-14 20:50:18.257806: Epoch 112 
2025-04-14 20:50:18.273434: Current learning rate: 0.0001949 
2025-04-14 21:13:24.562567: train_loss 4.8945 
2025-04-14 21:13:24.592860: val_loss 2.5463 
2025-04-14 21:13:24.594054: PSNR 18.9932 
2025-04-14 21:13:24.609695: Epoch time: 1386.35 s 
2025-04-14 21:13:25.654916:  
2025-04-14 21:13:25.670541: Epoch 113 
2025-04-14 21:13:25.686170: Current learning rate: 0.0001948 
2025-04-14 21:36:36.309949: train_loss 4.8219 
2025-04-14 21:36:36.341204: val_loss 2.5528 
2025-04-14 21:36:36.356833: PSNR 19.3502 
2025-04-14 21:36:36.372456: Epoch time: 1390.67 s 
2025-04-14 21:36:37.274522:  
2025-04-14 21:36:37.305780: Epoch 114 
2025-04-14 21:36:37.305780: Current learning rate: 0.0001947 
2025-04-14 21:59:43.203199: train_loss 4.8682 
2025-04-14 21:59:43.233907: val_loss 2.676 
2025-04-14 21:59:43.249538: PSNR 19.2304 
2025-04-14 21:59:43.249538: Epoch time: 1385.93 s 
2025-04-14 21:59:44.527523:  
2025-04-14 21:59:44.546240: Epoch 115 
2025-04-14 21:59:44.577492: Current learning rate: 0.0001946 
2025-04-14 22:22:57.062846: train_loss 4.8019 
2025-04-14 22:22:57.078484: val_loss 2.6142 
2025-04-14 22:22:57.094104: PSNR 18.8926 
2025-04-14 22:22:57.109730: Epoch time: 1392.56 s 
2025-04-14 22:22:57.962370:  
2025-04-14 22:22:57.993634: Epoch 116 
2025-04-14 22:22:58.009248: Current learning rate: 0.0001945 
2025-04-14 22:46:13.062599: train_loss 4.6921 
2025-04-14 22:46:13.093863: val_loss 2.6273 
2025-04-14 22:46:13.125098: PSNR 19.2439 
2025-04-14 22:46:13.140724: Epoch time: 1395.1 s 
2025-04-14 22:46:14.055994:  
2025-04-14 22:46:14.087257: Epoch 117 
2025-04-14 22:46:14.102873: Current learning rate: 0.0001944 
2025-04-14 23:09:27.621077: train_loss 4.8998 
2025-04-14 23:09:27.636703: val_loss 2.4572 
2025-04-14 23:09:27.667970: PSNR 19.5195 
2025-04-14 23:09:27.683582: Epoch time: 1393.57 s 
2025-04-14 23:09:28.597460:  
2025-04-14 23:09:28.613107: Epoch 118 
2025-04-14 23:09:28.628721: Current learning rate: 0.0001942 
2025-04-14 23:32:42.042274: train_loss 4.8779 
2025-04-14 23:32:42.073525: val_loss 2.5832 
2025-04-14 23:32:42.089151: PSNR 18.8938 
2025-04-14 23:32:42.104776: Epoch time: 1393.44 s 
2025-04-14 23:32:43.430517:  
2025-04-14 23:32:43.446159: Epoch 119 
2025-04-14 23:32:43.461767: Current learning rate: 0.0001941 
2025-04-14 23:55:56.998667: train_loss 4.7752 
2025-04-14 23:55:57.029923: val_loss 2.7208 
2025-04-14 23:55:57.045563: PSNR 18.4748 
2025-04-14 23:55:57.061173: Epoch time: 1393.58 s 
2025-04-14 23:55:58.378846:  
2025-04-14 23:55:58.394461: Epoch 120 
2025-04-14 23:55:58.410090: Current learning rate: 0.000194 
2025-04-15 00:19:15.226509: train_loss 4.7498 
2025-04-15 00:19:15.242134: val_loss 2.6034 
2025-04-15 00:19:15.257761: PSNR 19.1365 
2025-04-15 00:19:15.273394: Epoch time: 1396.86 s 
2025-04-15 00:19:16.773067:  
2025-04-15 00:19:16.804315: Epoch 121 
2025-04-15 00:19:16.819941: Current learning rate: 0.0001939 
2025-04-15 00:42:30.384846: train_loss 4.8245 
2025-04-15 00:42:30.416101: val_loss 2.6464 
2025-04-15 00:42:30.447351: PSNR 18.506 
2025-04-15 00:42:30.462976: Epoch time: 1393.61 s 
2025-04-15 00:42:32.010970:  
2025-04-15 00:42:32.042217: Epoch 122 
2025-04-15 00:42:32.057840: Current learning rate: 0.0001938 
2025-04-15 01:05:49.697529: train_loss 4.7632 
2025-04-15 01:05:49.713156: val_loss 2.4857 
2025-04-15 01:05:49.728785: PSNR 19.6698 
2025-04-15 01:05:49.744407: Epoch time: 1397.7 s 
2025-04-15 01:05:51.126357:  
2025-04-15 01:05:51.141984: Epoch 123 
2025-04-15 01:05:51.173235: Current learning rate: 0.0001937 
2025-04-15 01:29:04.089460: train_loss 4.7954 
2025-04-15 01:29:04.105090: val_loss 2.5999 
2025-04-15 01:29:04.120714: PSNR 18.6065 
2025-04-15 01:29:04.136340: Epoch time: 1392.96 s 
2025-04-15 01:29:05.483179:  
2025-04-15 01:29:05.514425: Epoch 124 
2025-04-15 01:29:05.530054: Current learning rate: 0.0001936 
2025-04-15 01:52:16.534274: train_loss 4.8315 
2025-04-15 01:52:16.575020: val_loss 2.4237 
2025-04-15 01:52:16.590649: PSNR 19.7767 
2025-04-15 01:52:16.606271: Epoch time: 1391.05 s 
2025-04-15 01:52:17.499968:  
2025-04-15 01:52:17.515594: Epoch 125 
2025-04-15 01:52:17.531221: Current learning rate: 0.0001935 
2025-04-15 02:15:32.434915: train_loss 4.8252 
2025-04-15 02:15:32.450541: val_loss 2.7348 
2025-04-15 02:15:32.481796: PSNR 18.7296 
2025-04-15 02:15:32.481796: Epoch time: 1394.93 s 
2025-04-15 02:15:33.356792:  
2025-04-15 02:15:33.388047: Epoch 126 
2025-04-15 02:15:33.388047: Current learning rate: 0.0001934 
2025-04-15 02:38:43.293612: train_loss 4.8144 
2025-04-15 02:38:43.324862: val_loss 2.9384 
2025-04-15 02:38:43.340490: PSNR 18.1277 
2025-04-15 02:38:43.356119: Epoch time: 1389.94 s 
2025-04-15 02:38:44.868067:  
2025-04-15 02:38:44.892778: Epoch 127 
2025-04-15 02:38:44.892778: Current learning rate: 0.0001933 
2025-04-15 03:01:59.738006: train_loss 4.7093 
2025-04-15 03:01:59.753631: val_loss 2.5884 
2025-04-15 03:01:59.769258: PSNR 19.0529 
2025-04-15 03:01:59.800505: Epoch time: 1394.89 s 
2025-04-15 03:02:01.345554:  
2025-04-15 03:02:01.361185: Epoch 128 
2025-04-15 03:02:01.392432: Current learning rate: 0.0001931 
2025-04-15 03:25:13.671038: train_loss 4.708 
2025-04-15 03:25:13.702287: val_loss 2.7016 
2025-04-15 03:25:13.727660: PSNR 19.1484 
2025-04-15 03:25:13.743305: Epoch time: 1392.33 s 
2025-04-15 03:25:15.153828:  
2025-04-15 03:25:15.185079: Epoch 129 
2025-04-15 03:25:15.200705: Current learning rate: 0.000193 
2025-04-15 03:48:23.209310: train_loss 4.7597 
2025-04-15 03:48:23.240573: val_loss 2.6826 
2025-04-15 03:48:23.256201: PSNR 18.8405 
2025-04-15 03:48:23.256201: Epoch time: 1388.06 s 
2025-04-15 03:48:24.844099:  
2025-04-15 03:48:24.875349: Epoch 130 
2025-04-15 03:48:24.890976: Current learning rate: 0.0001929 
2025-04-15 04:11:32.644970: train_loss 4.6992 
2025-04-15 04:11:32.676220: val_loss 2.7689 
2025-04-15 04:11:32.691845: PSNR 18.6273 
2025-04-15 04:11:32.707470: Epoch time: 1387.8 s 
2025-04-15 04:11:33.687263:  
2025-04-15 04:11:33.702885: Epoch 131 
2025-04-15 04:11:33.718510: Current learning rate: 0.0001928 
2025-04-15 04:34:40.334756: train_loss 4.7541 
2025-04-15 04:34:40.350391: val_loss 2.6824 
2025-04-15 04:34:40.366014: PSNR 18.4679 
2025-04-15 04:34:40.366014: Epoch time: 1386.66 s 
2025-04-15 04:34:41.302929:  
2025-04-15 04:34:41.334194: Epoch 132 
2025-04-15 04:34:41.349810: Current learning rate: 0.0001927 
2025-04-15 04:57:48.165740: train_loss 4.8126 
2025-04-15 04:57:48.197001: val_loss 2.4548 
2025-04-15 04:57:48.197001: PSNR 19.1964 
2025-04-15 04:57:48.212628: Epoch time: 1386.86 s 
2025-04-15 04:57:49.524674:  
2025-04-15 04:57:49.546527: Epoch 133 
2025-04-15 04:57:49.562148: Current learning rate: 0.0001926 
2025-04-15 05:20:57.120636: train_loss 4.6343 
2025-04-15 05:20:57.151885: val_loss 2.6356 
2025-04-15 05:20:57.167513: PSNR 19.0952 
2025-04-15 05:20:57.183141: Epoch time: 1387.61 s 
2025-04-15 05:20:58.202197:  
2025-04-15 05:20:58.233451: Epoch 134 
2025-04-15 05:20:58.233451: Current learning rate: 0.0001924 
2025-04-15 05:44:08.513724: train_loss 4.8254 
2025-04-15 05:44:08.529351: val_loss 2.7937 
2025-04-15 05:44:08.544978: PSNR 18.793 
2025-04-15 05:44:08.560601: Epoch time: 1390.31 s 
2025-04-15 05:44:09.554085:  
2025-04-15 05:44:09.569735: Epoch 135 
2025-04-15 05:44:09.585341: Current learning rate: 0.0001923 
2025-04-15 06:07:23.708310: train_loss 4.6311 
2025-04-15 06:07:23.739561: val_loss 2.5681 
2025-04-15 06:07:23.755190: PSNR 19.0087 
2025-04-15 06:07:23.770814: Epoch time: 1394.15 s 
2025-04-15 06:07:24.937757:  
2025-04-15 06:07:24.969010: Epoch 136 
2025-04-15 06:07:24.969010: Current learning rate: 0.0001922 
2025-04-15 06:30:40.942982: train_loss 4.7826 
2025-04-15 06:30:40.974235: val_loss 2.7022 
2025-04-15 06:30:40.989860: PSNR 18.4164 
2025-04-15 06:30:41.005481: Epoch time: 1396.01 s 
2025-04-15 06:30:42.467902:  
2025-04-15 06:30:42.483525: Epoch 137 
2025-04-15 06:30:42.499152: Current learning rate: 0.0001921 
2025-04-15 06:53:49.879589: train_loss 4.7506 
2025-04-15 06:53:49.895215: val_loss 2.4594 
2025-04-15 06:53:49.926469: PSNR 19.7508 
2025-04-15 06:53:49.942101: Epoch time: 1387.43 s 
2025-04-15 06:53:51.307204:  
2025-04-15 06:53:51.338453: Epoch 138 
2025-04-15 06:53:51.338453: Current learning rate: 0.0001919 
2025-04-15 07:17:06.954978: train_loss 4.7693 
2025-04-15 07:17:06.970605: val_loss 2.6841 
2025-04-15 07:17:07.001855: PSNR 19.1161 
2025-04-15 07:17:07.017480: Epoch time: 1395.65 s 
2025-04-15 07:17:08.142937:  
2025-04-15 07:17:08.156209: Epoch 139 
2025-04-15 07:17:08.166258: Current learning rate: 0.0001918 
2025-04-15 07:40:16.970441: train_loss 4.845 
2025-04-15 07:40:16.986069: val_loss 2.5573 
2025-04-15 07:40:17.001696: PSNR 18.7842 
2025-04-15 07:40:17.032945: Epoch time: 1388.83 s 
2025-04-15 07:40:18.245162:  
2025-04-15 07:40:18.286471: Epoch 140 
2025-04-15 07:40:18.302102: Current learning rate: 0.0001917 
2025-04-15 08:03:32.182469: train_loss 4.7608 
2025-04-15 08:03:32.213717: val_loss 2.6304 
2025-04-15 08:03:32.229345: PSNR 18.9181 
2025-04-15 08:03:32.244971: Epoch time: 1393.94 s 
2025-04-15 08:03:33.582209:  
2025-04-15 08:03:33.597832: Epoch 141 
2025-04-15 08:03:33.613458: Current learning rate: 0.0001916 
2025-04-15 08:26:44.336345: train_loss 4.7449 
2025-04-15 08:26:44.367598: val_loss 2.686 
2025-04-15 08:26:44.383229: PSNR 18.6159 
2025-04-15 08:26:44.398856: Epoch time: 1390.75 s 
2025-04-15 08:26:45.839206:  
2025-04-15 08:26:45.870451: Epoch 142 
2025-04-15 08:26:45.886081: Current learning rate: 0.0001914 
2025-04-15 08:49:55.133970: train_loss 4.6144 
2025-04-15 08:49:55.149597: val_loss 2.6286 
2025-04-15 08:49:55.165235: PSNR 19.0361 
2025-04-15 08:49:55.196472: Epoch time: 1389.29 s 
2025-04-15 08:49:56.224115:  
2025-04-15 08:49:56.255365: Epoch 143 
2025-04-15 08:49:56.255365: Current learning rate: 0.0001913 
2025-04-15 09:13:14.153461: train_loss 4.7198 
2025-04-15 09:13:14.169088: val_loss 2.4846 
2025-04-15 09:13:14.200340: PSNR 19.3398 
2025-04-15 09:13:14.200340: Epoch time: 1397.93 s 
2025-04-15 09:13:15.319828:  
2025-04-15 09:13:15.335457: Epoch 144 
2025-04-15 09:13:15.351084: Current learning rate: 0.0001912 
2025-04-15 09:36:30.700001: train_loss 4.7073 
2025-04-15 09:36:30.715630: val_loss 2.4991 
2025-04-15 09:36:30.731256: PSNR 19.568 
2025-04-15 09:36:30.731256: Epoch time: 1395.38 s 
2025-04-15 09:36:31.720834:  
2025-04-15 09:36:31.736465: Epoch 145 
2025-04-15 09:36:31.752084: Current learning rate: 0.0001911 
2025-04-15 09:59:44.203980: train_loss 4.6842 
2025-04-15 09:59:44.219609: val_loss 2.6446 
2025-04-15 09:59:44.235232: PSNR 19.2319 
2025-04-15 09:59:44.250859: Epoch time: 1392.48 s 
2025-04-15 09:59:45.597351:  
2025-04-15 09:59:45.612985: Epoch 146 
2025-04-15 09:59:45.628599: Current learning rate: 0.0001909 
2025-04-15 10:23:00.642508: train_loss 4.7046 
2025-04-15 10:23:00.673761: val_loss 2.6171 
2025-04-15 10:23:00.689391: PSNR 18.8859 
2025-04-15 10:23:00.705011: Epoch time: 1395.06 s 
2025-04-15 10:23:01.974644:  
2025-04-15 10:23:01.990279: Epoch 147 
2025-04-15 10:23:02.005896: Current learning rate: 0.0001908 
2025-04-15 10:46:14.867229: train_loss 4.6629 
2025-04-15 10:46:14.882845: val_loss 2.6202 
2025-04-15 10:46:14.898471: PSNR 19.2267 
2025-04-15 10:46:14.914095: Epoch time: 1392.91 s 
2025-04-15 10:46:15.954197:  
2025-04-15 10:46:15.969822: Epoch 148 
2025-04-15 10:46:15.985447: Current learning rate: 0.0001907 
2025-04-15 11:10:05.340434: train_loss 4.8256 
2025-04-15 11:10:05.359437: val_loss 2.4942 
2025-04-15 11:10:05.374431: PSNR 19.6843 
2025-04-15 11:10:05.385432: Epoch time: 1429.39 s 
2025-04-15 11:10:06.704483:  
2025-04-15 11:10:06.721488: Epoch 149 
2025-04-15 11:10:06.737488: Current learning rate: 0.0001905 
2025-04-15 11:34:44.167547: train_loss 4.7873 
2025-04-15 11:34:44.181548: val_loss 2.4817 
2025-04-15 11:34:44.197554: PSNR 18.9179 
2025-04-15 11:34:44.208550: Epoch time: 1477.47 s 
2025-04-15 11:34:51.357257:  
2025-04-15 11:34:51.383259: Epoch 150 
2025-04-15 11:34:51.398258: Current learning rate: 0.0001904 
2025-04-15 11:59:10.937862: train_loss 4.6414 
2025-04-15 11:59:10.969114: val_loss 2.5587 
2025-04-15 11:59:10.984740: PSNR 19.177 
2025-04-15 11:59:11.000363: Epoch time: 1459.59 s 
2025-04-15 11:59:12.090456:  
2025-04-15 11:59:12.106084: Epoch 151 
2025-04-15 11:59:12.121705: Current learning rate: 0.0001903 
2025-04-15 12:22:17.715797: train_loss 4.7379 
2025-04-15 12:22:17.731425: val_loss 2.6437 
2025-04-15 12:22:17.747052: PSNR 18.7478 
2025-04-15 12:22:17.762677: Epoch time: 1385.64 s 
2025-04-15 12:22:18.636404:  
2025-04-15 12:22:18.667652: Epoch 152 
2025-04-15 12:22:18.683278: Current learning rate: 0.0001901 
